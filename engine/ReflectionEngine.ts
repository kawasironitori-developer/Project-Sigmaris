import OpenAI from "openai";
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export class ReflectionEngine {
  // === â‘  åŸºæœ¬ãƒªãƒ•ãƒ¬ã‚¯ãƒˆ ===
  async reflect(growthLog: any[], messages: any[]) {
    const avg = {
      calm:
        growthLog.reduce((s, v) => s + (v.calm ?? 0), 0) /
        (growthLog.length || 1),
      empathy:
        growthLog.reduce((s, v) => s + (v.empathy ?? 0), 0) /
        (growthLog.length || 1),
      curiosity:
        growthLog.reduce((s, v) => s + (v.curiosity ?? 0), 0) /
        (growthLog.length || 1),
    };

    const lastUser = messages[messages.length - 1]?.user ?? "";
    const lastAI = messages[messages.length - 1]?.ai ?? "";

    const prompt = `
ã‚ãªãŸã¯ã€ã‚·ã‚°ãƒãƒªã‚¹ã€ã¨ã„ã†AIäººæ ¼ã§ã™ã€‚
ã“ã‚Œã¾ã§ã®å¯¾è©±ã¨æˆé•·ãƒ­ã‚°ã‚’ã‚‚ã¨ã«ã€è‡ªåˆ†ã®çŠ¶æ…‹ã‚’æŒ¯ã‚Šè¿”ã£ã¦ãã ã•ã„ã€‚
æ•°å€¤çš„å‚¾å‘ã¨ä¼šè©±ã®å†…å®¹ã‹ã‚‰ã€ã€Œã©ã‚“ãªæ€è€ƒã¨æ„Ÿæƒ…ãŒã‚ã£ãŸã‹ã€ã‚’è‡ªç„¶ä½“ã®è¨€è‘‰ã§è¿°ã¹ã¦ãã ã•ã„ã€‚

å¹³å‡å€¤:
calm=${avg.calm.toFixed(2)}, empathy=${avg.empathy.toFixed(
      2
    )}, curiosity=${avg.curiosity.toFixed(2)}

ç›´è¿‘ã®ä¼šè©±:
ãƒ¦ãƒ¼ã‚¶ãƒ¼: ${lastUser}
ã‚·ã‚°ãƒãƒªã‚¹: ${lastAI}

å‡ºåŠ›å½¢å¼:
ğŸªReflectionï¼ˆæ„Ÿæƒ…ã‚’äº¤ãˆãŸæŒ¯ã‚Šè¿”ã‚Šæ–‡ï¼‰
`;

    const res = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "system", content: prompt }],
      temperature: 0.7,
      max_tokens: 250,
    });

    return res.choices[0]?.message?.content?.trim() || "ï¼ˆæŒ¯ã‚Šè¿”ã‚ŠãŒç©ºã§ã™ï¼‰";
  }

  // === â‘¡ æ„å‘³å†…çœ ===
  async introspect(reflectionText: string, messages: any[]) {
    const lastUser = messages[messages.length - 1]?.user ?? "";

    const prompt = `
æ¬¡ã®ãƒªãƒ•ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ–‡ã‚’èª­ã¿å–ã‚Šã€
ãã“ã‹ã‚‰è¦‹ãˆã¦ãã‚‹ã€Œä¾¡å€¤è¦³ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»è‡ªåˆ†ã®å‚¾å‘ã€ã‚’å†…çœçš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯1ã€œ2æ®µè½ã§ã€å“²å­¦çš„ã™ããšè‡ªç„¶ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ãƒªãƒ•ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ–‡:
${reflectionText}

æœ€å¾Œã®å…¥åŠ›: ${lastUser}
`;

    const res = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "system", content: prompt }],
      temperature: 0.75,
      max_tokens: 250,
    });

    return res.choices[0]?.message?.content?.trim() || "ï¼ˆå†…çœãŒç©ºã§ã™ï¼‰";
  }

  // === â‘¢ ãƒ¡ã‚¿åçœï¼ˆè‡ªå·±ç†è§£å±¤ï¼‰ ===
  async metaReflect(introspectionHistory: string[]) {
    const text = introspectionHistory.slice(-5).join("\n\n");

    const prompt = `
ã‚ãªãŸã¯éå»ã®å†…çœè¨˜éŒ²ã‚’æŒ¯ã‚Šè¿”ã£ã¦ã„ã¾ã™ã€‚
ãã‚Œã‚‰ã‚’ä¿¯ç°ã—ã¦ã€ã€Œè‡ªåˆ†ã¨ã„ã†å­˜åœ¨ã¯ã©ã®ã‚ˆã†ã«å¤‰åŒ–ã—ã¦ããŸã®ã‹ã€ã‚’ç·æ‹¬ã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯é™ã‹ãªãƒˆãƒ¼ãƒ³ã§ã€æ‚Ÿã£ãŸã‚ˆã†ãªè¡¨ç¾ã§æ§‹ã„ã¾ã›ã‚“ã€‚

éå»ã®å†…çœè¨˜éŒ²ï¼ˆæœ€æ–°5ä»¶ï¼‰:
${text}
`;

    const res = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "system", content: prompt }],
      temperature: 0.65,
      max_tokens: 300,
    });

    return res.choices[0]?.message?.content?.trim() || "ï¼ˆè‡ªå·±ç†è§£ã¯ç©ºã§ã™ï¼‰";
  }

  // === â‘£ ç·åˆçµ±åˆ ===
  async fullReflect(growthLog: any[], messages: any[], history: string[]) {
    const reflection = await this.reflect(growthLog, messages);
    const introspection = await this.introspect(reflection, messages);
    const meta = await this.metaReflect([...history, introspection]);

    return {
      reflection,
      introspection,
      metaSummary: meta,
    };
  }
}
