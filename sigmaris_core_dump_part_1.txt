=== FILE: sigmaris-core/__init__.py ===

# Sigmaris Core Python package root

=== FILE: sigmaris-core/aei/__init__.py ===

# sigmaris-core/aei/__init__.py

from .identity import TraitVector, IdentityState, IdentityCore

__all__ = [
    "TraitVector",
    "IdentityState",
    "IdentityCore",
]

=== FILE: sigmaris-core/aei/adapter/__init__.py ===

from .llm_adapter import LLMAdapter, LLMFn

__all__ = [
    "LLMAdapter",
    "LLMFn",
]

=== FILE: sigmaris-core/aei/adapter/llm_adapter.py ===

# aei/adapter/llm_adapter.py
from __future__ import annotations

import json
import os
from typing import Callable, Dict, Any, Optional

from openai import OpenAI


# ============================================================
# 公開される唯一の関数型表現（AEI 全体で統一）
# ============================================================
LLMFn = Callable[[str], str]


class LLMAdapter:
    """
    Sigmaris OS — LLM Adapter
    -------------------------

    AEI Core（Python）と外部LLM（GPT等）を安全・安定に接続するレイヤ。

    - JSON を強制（暴走抑止）
    - OpenAI SDK の仕様変更を 1 箇所に隔離
    - test_mode / dummy_fn によるユニットテストが容易
    - Reflection / Introspection / LongTermPsychology が
      期待する LLMFn: (str) -> str を一貫提供
    """

    def __init__(
        self,
        model: str = "gpt-4o-mini",
        api_key: Optional[str] = None,
        temperature: float = 0.2,
        timeout: int = 30,
        test_mode: bool = False,
        dummy_fn: Optional[LLMFn] = None,
    ) -> None:
        """
        Parameters
        ----------
        model:
            使用する OpenAI モデル名。
            環境変数 SIGMARIS_LLM_MODEL があればそちらを優先。
        api_key:
            OpenAI API キー。未指定なら OPENAI_API_KEY を読む。
        temperature:
            サンプリング温度。0.0〜1.0 目安。
        timeout:
            ネットワークタイムアウト秒（現状は SDK 側の設定に依存。
            パラメータとして保持だけしておく）
        test_mode:
            True の場合は OpenAI API を叩かず dummy_fn を使う。
        dummy_fn:
            test_mode 用のダミー関数 (str) -> str。必ず JSON 文字列を返す必要がある。
        """
        # モデル名は環境変数で上書き可能
        env_model = os.getenv("SIGMARIS_LLM_MODEL")
        self.model = env_model or model

        self.temperature = temperature
        self.timeout = timeout
        self.test_mode = bool(test_mode)
        self.dummy_fn = dummy_fn

        # ==== OpenAI Client 初期化 ====
        if not self.test_mode:
            key = api_key or os.getenv("OPENAI_API_KEY")
            if not key:
                raise RuntimeError(
                    "OPENAI_API_KEY is not set and no api_key was given for LLMAdapter."
                )
            # openai>=1.x スタイル
            self.client = OpenAI(api_key=key)
        else:
            self.client = None  # test mode

    # ============================================================
    # コア：プロンプト → JSON文字列
    # ============================================================
    def run(self, prompt: str) -> str:
        """
        LLM にプロンプトを投げ、必ず JSON 文字列を返す。

        AEI Core（Reflection / Introspection / LongTermPsychology）は
        このメソッドだけを直接使う。
        """

        # ---- test_mode: ダミーLLM ----
        if self.test_mode:
            if self.dummy_fn is None:
                raise RuntimeError("LLMAdapter: test_mode=True but dummy_fn is None.")
            result = self.dummy_fn(prompt)
            if not isinstance(result, str):
                raise RuntimeError("LLMAdapter: dummy_fn must return a JSON string.")
            # JSON 妥当性だけ軽くチェック
            try:
                json.loads(result)
            except json.JSONDecodeError as e:
                raise RuntimeError(
                    f"LLMAdapter: dummy_fn returned invalid JSON: {e}"
                )
            return result

        # ---- OpenAI API ----
        try:
            # openai>=1.x の chat.completions.create を想定
            resp = self.client.chat.completions.create(
                model=self.model,
                temperature=self.temperature,
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "You are a JSON-only engine. "
                            "Return ONLY valid JSON. No explanations. No markdown. "
                            "If you need to express text, put it into JSON fields."
                        ),
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=800,
                # timeout はクライアント側設定になるため、
                # ここでは self.timeout を保持しておくだけ
            )

            # 返却メッセージ構造の互換性確保
            msg = resp.choices[0].message
            if isinstance(msg, dict):
                text = (msg.get("content") or "").strip()
            else:
                # openai>=1.x の ChatCompletionMessage
                text = (msg.content or "").strip()

        except Exception as e:
            raise RuntimeError(f"LLMAdapter: OpenAI request failed: {e}")

        if not text:
            raise RuntimeError("LLMAdapter: LLM returned empty content.")

        # ---- JSON 妥当性チェック ----
        try:
            json.loads(text)
        except json.JSONDecodeError as e:
            # ここで落ちる場合はプロンプト設計かモデル側の問題
            raise RuntimeError(
                "LLMAdapter: LLM output is NOT valid JSON:\n"
                f"{text}\n"
                f"error: {e}"
            )

        return text

    # ============================================================
    # AEI が利用する関数型インターフェース
    # ============================================================
    def as_function(self) -> LLMFn:
        """
        ReflectionCore / IntrospectionCore / LongTermPsychology が
        共通で期待する “LLMFn: (str) -> str” を返す。
        """
        return self.run

    # ============================================================
    # デバッグ（JSON を dict で返す）
    # ============================================================
    def debug_run(self, prompt: str) -> Dict[str, Any]:
        """
        デバッグ用ヘルパー。
        - run() の戻り値(JSON文字列)を dict にして返す。
        """
        raw = self.run(prompt)
        return json.loads(raw)

=== FILE: sigmaris-core/aei/emotion/emotion_core.py ===

# aei/emotion_core.py
from __future__ import annotations

import json
from datetime import datetime, timezone
from textwrap import dedent
from typing import Callable, Dict, Any, Optional

from aei.identity import IdentityCore, TraitVector
from aei.episodic_memory import EpisodeStore
from aei.episodic_memory.epmem import Episode

# LLM インターフェース: prompt (str) -> JSON文字列 (str)
LLMFn = Callable[[str], str]


# ============================================================
# プロンプト生成（深層 Emotion 解析）
# ============================================================

def build_emotion_prompt(raw_log: str, identity_snapshot: Dict[str, Any]) -> str:
    identity_json = json.dumps(identity_snapshot, ensure_ascii=False, indent=2)

    prompt = f"""
    You are the **EMOTION CORE** of an AEI system named Sigmaris.

    Your task is to perform a **deep emotional analysis** of the given text,
    taking into account the current identity state.

    INPUT:
      - interaction_log: free-form text or thought log from the user
      - identity_snapshot: baseline/current trait vectors

    From this, infer:

      1. A concise emotion label (e.g., "quiet-focus", "soft-curiosity")
      2. Intensity of this emotion (0.0 - 1.0)
      3. A short natural-language reason
      4. A trait_shift object for:
           - calm
           - empathy
           - curiosity
         (each -0.05 to +0.05)
      5. meta:
           - energy    (0.0 - 1.0)
           - stability (0.0 - 1.0)
           - valence   (-1.0 - +1.0)

    Return ONLY JSON:
    {{
      "emotion": "quiet-focus",
      "intensity": 0.62,
      "reason": "...",
      "trait_shift": {{
        "calm": 0.03,
        "empathy": -0.01,
        "curiosity": 0.04
      }},
      "meta": {{
        "energy": 0.67,
        "stability": 0.91,
        "valence": 0.12
      }}
    }}

    --- INTERACTION LOG ---
    {raw_log}

    --- IDENTITY SNAPSHOT ---
    {identity_json}
    """
    return dedent(prompt).strip()


# ============================================================
# EmotionCore（深層・感情解析レイヤ）
# ============================================================

class EmotionCore:
    def __init__(
        self,
        identity_core: IdentityCore,
        episode_store: EpisodeStore,
        llm_fn: LLMFn,
        max_trait_shift: float = 0.05,
    ) -> None:
        self.identity_core = identity_core
        self.episode_store = episode_store
        self.llm_fn = llm_fn
        self.max_trait_shift = float(max_trait_shift)

    # --------------------------------------------------------
    # 内部ユーティリティ
    # --------------------------------------------------------

    def _clamp_shift(self, x: float) -> float:
        m = self.max_trait_shift
        return max(-m, min(m, float(x)))

    def _call_llm(self, raw_log: str) -> Dict[str, Any]:
        snapshot = self.identity_core.export_state()
        prompt = build_emotion_prompt(raw_log, snapshot)
        raw = self.llm_fn(prompt)

        try:
            data = json.loads(raw)
        except json.JSONDecodeError as e:
            raise RuntimeError(
                f"Invalid JSON from EmotionCore LLM: {e}\nRAW={raw[:300]}"
            ) from e

        for key in ("emotion", "intensity", "reason", "trait_shift", "meta"):
            if key not in data:
                raise RuntimeError(f"Missing key in EmotionCore result: {key}")

        return data

    # --------------------------------------------------------
    # 公開 API
    # --------------------------------------------------------

    def analyze(
        self,
        raw_log: str,
        episode_id: Optional[str] = None,
    ) -> Dict[str, Any]:

        # 1) LLM による推定
        data = self._call_llm(raw_log)

        emotion_label = str(data.get("emotion") or "unknown")
        intensity_raw = float(data.get("intensity") or 0.0)
        reason = str(data.get("reason") or "").strip()

        trait_shift_raw = data.get("trait_shift") or {}
        meta_raw = data.get("meta") or {}

        # 2) shift を clamp
        dc = self._clamp_shift(trait_shift_raw.get("calm", 0.0))
        de = self._clamp_shift(trait_shift_raw.get("empathy", 0.0))
        du = self._clamp_shift(trait_shift_raw.get("curiosity", 0.0))

        # 3) 新しい current（揺れ）生成
        cur = self.identity_core.current
        observed = TraitVector(
            calm=cur.calm + dc,
            empathy=cur.empathy + de,
            curiosity=cur.curiosity + du,
        ).clamp()

        # 4) 反映
        self.identity_core.apply_observed_traits(observed, weight=0.6)

        if not self.identity_core.is_stable():
            self.identity_core.gently_correct(weight=0.25)

        # 5) Episode 化
        now = datetime.now(timezone.utc)
        eid = episode_id or f"em-{now.strftime('%Y%m%d-%H%M%S')}"

        summary = reason or raw_log[:120]

        episode = Episode(
            episode_id=eid,
            timestamp=now,
            summary=summary,
            emotion_hint=emotion_label,
            traits_hint=self.identity_core.current.as_dict(),
            raw_context=raw_log,
        )

        self.episode_store.add(episode)

        # 6) meta clamp
        meta = {
            "energy": max(0.0, min(1.0, float(meta_raw.get("energy", 0.5)))),
            "stability": max(0.0, min(1.0, float(meta_raw.get("stability", 0.5)))),
            "valence": max(-1.0, min(1.0, float(meta_raw.get("valence", 0.0)))),
        }

        return {
            "emotion": {
                "label": emotion_label,
                "intensity": max(0.0, min(1.0, intensity_raw)),
                "reason": reason,
                "applied_trait_shift": {
                    "calm": dc,
                    "empathy": de,
                    "curiosity": du,
                },
                "meta": meta,
            },
            "episode": episode.as_dict(),
            "identity": self.identity_core.export_state(),
        }

=== FILE: sigmaris-core/aei/episodic_memory/__init__.py ===

# aei/episodic_memory/__init__.py
from .epmem import Episode, EpisodeStore
from .sqlite_store import SQLiteEpisodeStore
from .archive_manager import EpisodeArchiveManager

__all__ = [
    "Episode",
    "EpisodeStore",          # 既存の in-memory 実装（互換用）
    "SQLiteEpisodeStore",    # 新規：SQLite バックエンド
    "EpisodeArchiveManager", # 新規：アーカイブ管理
]

=== FILE: sigmaris-core/aei/episodic_memory/archive_manager.py ===

# aei/episodic_memory/archive_manager.py
from __future__ import annotations

import json
import gzip
from datetime import datetime, timezone
from pathlib import Path
from typing import List

from .epmem import Episode


class EpisodeArchiveManager:
    """
    Episode アーカイブ管理クラス。

    - 古い Episode を SQLite から取り出し
    - JSON/GZIP ファイルに保存
    - SQLite 側からは削除して容量削減
    - データは一切失われない

    使い方：
        arch = EpisodeArchiveManager()
        arch.archive(episodes)  # Episode のリストを渡すだけ
    """

    def __init__(self, archive_dir: str = "data/archive") -> None:
        self.archive_dir = Path(archive_dir)
        self.archive_dir.mkdir(parents=True, exist_ok=True)

    # ----------------------------------------------------------
    # JSON に変換
    # ----------------------------------------------------------
    def _episodes_to_json(self, episodes: List[Episode]) -> str:
        data = [ep.as_dict() for ep in episodes]
        return json.dumps(data, ensure_ascii=False, indent=2)

    # ----------------------------------------------------------
    # アーカイブ実行
    # ----------------------------------------------------------
    def archive(self, episodes: List[Episode]) -> str:
        if not episodes:
            return ""

        ts = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
        fname = self.archive_dir / f"episodes-{ts}.json.gz"

        # gzip で圧縮書き込み
        with gzip.open(fname, "wt", encoding="utf-8") as f:
            f.write(self._episodes_to_json(episodes))

        return str(fname)

=== FILE: sigmaris-core/aei/episodic_memory/episode_store_sqlite.py ===

# aei/episodic_memory/episode_store_sqlite.py
from __future__ import annotations
import sqlite3
import json
from datetime import datetime, timezone
from typing import List, Optional

from .epmem import Episode


class EpisodeStoreSQLite:
    """
    SQLite バックエンドの EpisodeStore。
    - API は EpisodeStore（メモリ版）と完全互換
    - データは data/episodes.db に永続化される
    """

    def __init__(self, db_path: str = "data/episodes.db") -> None:
        self.db_path = db_path
        self._init_db()

    # ---------------------------------------------------
    # 初期化：テーブル作成
    # ---------------------------------------------------
    def _init_db(self):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()

        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS episodes (
                episode_id TEXT PRIMARY KEY,
                timestamp TEXT,
                summary TEXT,
                emotion_hint TEXT,
                traits_hint TEXT,
                raw_context TEXT
            )
            """
        )
        conn.commit()
        conn.close()

    # ---------------------------------------------------
    # Episode → DB
    # ---------------------------------------------------
    def add(self, episode: Episode) -> None:
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()

        cur.execute(
            """
            INSERT OR REPLACE INTO episodes 
            (episode_id, timestamp, summary, emotion_hint, traits_hint, raw_context)
            VALUES (?, ?, ?, ?, ?, ?)
            """,
            (
                episode.episode_id,
                episode.timestamp.isoformat(),
                episode.summary,
                episode.emotion_hint,
                json.dumps(episode.traits_hint, ensure_ascii=False),
                episode.raw_context,
            ),
        )
        conn.commit()
        conn.close()

    # ---------------------------------------------------
    # 最新 n 件を Episode として返す
    # ---------------------------------------------------
    def get_last(self, n: int) -> List[Episode]:
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()

        cur.execute(
            """
            SELECT episode_id, timestamp, summary, emotion_hint, traits_hint, raw_context
            FROM episodes
            ORDER BY timestamp DESC
            LIMIT ?
            """,
            (n,),
        )

        rows = cur.fetchall()
        conn.close()

        episodes = []
        for ep_id, ts, summary, emo, traits, raw in rows:
            dt = datetime.fromisoformat(ts)
            episodes.append(
                Episode(
                    episode_id=ep_id,
                    timestamp=dt,
                    summary=summary,
                    emotion_hint=emo,
                    traits_hint=json.loads(traits),
                    raw_context=raw,
                )
            )

        # 新しい順で取ってきたので反転して「古 → 新」に揃える
        return list(reversed(episodes))

    # ---------------------------------------------------
    # 全取得
    # ---------------------------------------------------
    def get_all(self) -> List[Episode]:
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()

        cur.execute(
            """
            SELECT episode_id, timestamp, summary, emotion_hint, traits_hint, raw_context
            FROM episodes
            ORDER BY timestamp ASC
            """
        )

        rows = cur.fetchall()
        conn.close()

        episodes = []
        for ep_id, ts, summary, emo, traits, raw in rows:
            dt = datetime.fromisoformat(ts)
            episodes.append(
                Episode(
                    episode_id=ep_id,
                    timestamp=dt,
                    summary=summary,
                    emotion_hint=emo,
                    traits_hint=json.loads(traits),
                    raw_context=raw,
                )
            )

        return episodes

=== FILE: sigmaris-core/aei/episodic_memory/epmem.py ===

# aei/episodic_memory/epmem.py
from __future__ import annotations

import json
import os
from typing import List, Optional, Dict, Any
from dataclasses import dataclass, asdict
from datetime import datetime, timezone

# =====================================================================
# Episode Model
# =====================================================================

@dataclass
class Episode:
    """
    AEI Episodic Memory Unit
    - summary: 内省または出来事の要約
    - emotion_hint: AEI の情動ラベル
    - traits_hint: calm/empathy/curiosity の観測値
    - raw_context: 元ログ（会話全文など）
    """

    episode_id: str
    timestamp: datetime
    summary: str
    emotion_hint: str
    traits_hint: Dict[str, float]
    raw_context: str

    def as_dict(self) -> Dict[str, Any]:
        """
        Episode → JSON（安全形式）
        timestamp は常に ISO8601 + UTC
        """
        d = asdict(self)
        d["timestamp"] = self.timestamp.astimezone(timezone.utc).isoformat()
        return d

    @staticmethod
    def from_dict(d: Dict[str, Any]) -> "Episode":
        """
        JSON → Episode（破損救済込み）
        """
        ts_raw = d.get("timestamp")
        if not ts_raw:
            ts = datetime.now(timezone.utc)
        else:
            try:
                ts = datetime.fromisoformat(ts_raw)
            except Exception:
                ts = datetime.now(timezone.utc)

        # naive → UTC
        if ts.tzinfo is None:
            ts = ts.replace(tzinfo=timezone.utc)

        return Episode(
            episode_id=d.get("episode_id", ""),
            timestamp=ts,
            summary=d.get("summary", ""),
            emotion_hint=d.get("emotion_hint", ""),
            traits_hint=d.get("traits_hint", {}) or {},
            raw_context=d.get("raw_context", ""),
        )

# =====================================================================
# EpisodeStore
# =====================================================================

class EpisodeStore:
    """
    Sigmaris OS — Episodic Memory Store
    """
    DEFAULT_PATH = "./sigmaris-data/episodes.json"

    def __init__(self, path: str = None) -> None:
        self.path = path or self.DEFAULT_PATH

        # ディレクトリ準備
        os.makedirs(os.path.dirname(self.path), exist_ok=True)

        # episodes.json が無ければ自動生成
        if not os.path.exists(self.path):
            with open(self.path, "w", encoding="utf-8") as f:
                json.dump([], f, ensure_ascii=False, indent=2)

    # ------------------------------------------------------------ #
    # Low-level I/O（破損救済対応）
    # ------------------------------------------------------------ #

    def _load_json(self) -> List[Dict[str, Any]]:
        """
        episodes.json を読み込む。
        壊れていれば初期化して空リストに戻す。
        """
        if not os.path.exists(self.path):
            # 自動初期化
            self._save_json([])
            return []

        try:
            with open(self.path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            # 壊れてる → 初期化して復旧
            self._save_json([])
            return []

    def _save_json(self, raw_list: List[Dict[str, Any]]) -> None:
        with open(self.path, "w", encoding="utf-8") as f:
            json.dump(raw_list, f, ensure_ascii=False, indent=2)

    # ------------------------------------------------------------ #
    # CRUD
    # ------------------------------------------------------------ #

    def add(self, episode: Episode) -> None:
        """
        Episode を追加し、時系列順にソートして保存。
        """
        raw = self._load_json()
        raw.append(episode.as_dict())

        raw.sort(key=lambda x: x.get("timestamp", ""))

        self._save_json(raw)

    def load_all(self) -> List[Episode]:
        raw = self._load_json()
        return [Episode.from_dict(d) for d in raw]

    def get_last(self, n: int = 1) -> List[Episode]:
        eps = self.load_all()
        return eps[-n:] if eps else []

    def get_range(self, start: datetime, end: datetime) -> List[Episode]:
        eps = self.load_all()
        return [ep for ep in eps if start <= ep.timestamp <= end]

    def count(self) -> int:
        return len(self._load_json())

    # ------------------------------------------------------------ #
    # Analytics（心理・長期内省）
    # ------------------------------------------------------------ #

    def last_summary(self) -> Optional[str]:
        last = self.get_last(1)
        return last[0].summary if last else None

    def trait_trend(self, n: int = 5) -> Dict[str, float]:
        eps = self.get_last(n)
        if not eps:
            return {"calm": 0.0, "empathy": 0.0, "curiosity": 0.0}

        c = sum(ep.traits_hint.get("calm", 0.0) for ep in eps) / len(eps)
        e = sum(ep.traits_hint.get("empathy", 0.0) for ep in eps) / len(eps)
        u = sum(ep.traits_hint.get("curiosity", 0.0) for ep in eps) / len(eps)

        return {
            "calm": round(c, 4),
            "empathy": round(e, 4),
            "curiosity": round(u, 4),
        }

    # ------------------------------------------------------------ #
    # Export
    # ------------------------------------------------------------ #

    def export_state(self) -> Dict[str, Any]:
        eps = self.load_all()
        return {
            "count": len(eps),
            "episodes": [ep.as_dict() for ep in eps],
            "trait_trend": self.trait_trend(n=10),
        }

=== FILE: sigmaris-core/aei/episodic_memory/sqlite_store.py ===

# aei/episodic_memory/sqlite_store.py
from __future__ import annotations

import json
import sqlite3
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import List

from .epmem import Episode


class SQLiteEpisodeStore:
    """
    SQLite バックエンドの EpisodeStore。

    必須API:
      - add(Episode)
      - get_last(n)
      - get_all()
      - get_range(since, until)  ← LongTermPsychology が要求
      - delete(episode_id)
    """

    def __init__(self, db_path: str = "data/episodes.db") -> None:
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self._init_db()

    # ----------------------------------------------------------
    # 初期化：テーブル作成
    # ----------------------------------------------------------
    def _init_db(self) -> None:
        cur = self.conn.cursor()
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS episodes (
                episode_id   TEXT PRIMARY KEY,
                timestamp    TEXT NOT NULL,
                summary      TEXT,
                emotion_hint TEXT,
                traits_hint  TEXT,
                raw_context  TEXT
            );
            """
        )
        self.conn.commit()

    # ----------------------------------------------------------
    # Episode 追加
    # ----------------------------------------------------------
    def add(self, episode: Episode) -> None:
        cur = self.conn.cursor()
        cur.execute(
            """
            INSERT OR REPLACE INTO episodes
                (episode_id, timestamp, summary, emotion_hint, traits_hint, raw_context)
            VALUES
                (?, ?, ?, ?, ?, ?);
            """,
            (
                episode.episode_id,
                episode.timestamp.isoformat(),
                episode.summary,
                episode.emotion_hint,
                json.dumps(episode.traits_hint, ensure_ascii=False),
                episode.raw_context,
            ),
        )
        self.conn.commit()

    # ----------------------------------------------------------
    # Episode → Python 変換
    # ----------------------------------------------------------
    def _row_to_episode(self, row: sqlite3.Row) -> Episode:
        ts_raw = row["timestamp"]
        try:
            ts = datetime.fromisoformat(ts_raw)
            if ts.tzinfo is None:
                ts = ts.replace(tzinfo=timezone.utc)
        except Exception:
            ts = datetime.now(timezone.utc)

        traits_raw = row["traits_hint"]
        try:
            traits = json.loads(traits_raw) if traits_raw else {}
        except Exception:
            traits = {}

        return Episode(
            episode_id=row["episode_id"],
            timestamp=ts,
            summary=row["summary"] or "",
            emotion_hint=row["emotion_hint"] or "",
            traits_hint=traits,
            raw_context=row["raw_context"] or "",
        )

    # ----------------------------------------------------------
    # 最近 n 件（古い順）
    # ----------------------------------------------------------
    def get_last(self, n: int) -> List[Episode]:
        cur = self.conn.cursor()
        cur.execute(
            """
            SELECT * FROM episodes
            ORDER BY timestamp DESC
            LIMIT ?;
            """,
            (int(n),),
        )
        rows = cur.fetchall()
        episodes = [self._row_to_episode(r) for r in rows]
        episodes.reverse()
        return episodes

    # ----------------------------------------------------------
    # 全件取得（古い順）
    # ----------------------------------------------------------
    def get_all(self) -> List[Episode]:
        cur = self.conn.cursor()
        cur.execute(
            """
            SELECT * FROM episodes
            ORDER BY timestamp ASC;
            """
        )
        rows = cur.fetchall()
        return [self._row_to_episode(r) for r in rows]

    # ----------------------------------------------------------
    # ★ 期間取得（LongTermPsychology 用）
    # ----------------------------------------------------------
    def get_range(self, since: datetime, until: datetime) -> List[Episode]:
        """
        timestamp BETWEEN since AND until の Episode を返す。
        LongTermPsychology が必須で使う。
        """
        cur = self.conn.cursor()
        cur.execute(
            """
            SELECT *
            FROM episodes
            WHERE timestamp BETWEEN ? AND ?
            ORDER BY timestamp ASC;
            """,
            (since.isoformat(), until.isoformat()),
        )
        rows = cur.fetchall()
        return [self._row_to_episode(r) for r in rows]

    # ----------------------------------------------------------
    # 削除
    # ----------------------------------------------------------
    def delete(self, episode_id: str) -> None:
        cur = self.conn.cursor()
        cur.execute("DELETE FROM episodes WHERE episode_id = ?;", (episode_id,))
        self.conn.commit()

    # ----------------------------------------------------------
    # クローズ
    # ----------------------------------------------------------
    def close(self) -> None:
        try:
            self.conn.close()
        except Exception:
            pass

=== FILE: sigmaris-core/aei/identity/__init__.py ===

from .trait_vector import TraitVector
from .identity_state import IdentityState
from .identity_core import IdentityCore

__all__ = [
    "TraitVector",
    "IdentityState",
    "IdentityCore",
]

=== FILE: sigmaris-core/aei/identity/identity_core.py ===

from __future__ import annotations

from typing import Optional, Tuple, Union

from .trait_vector import TraitVector
from .identity_state import IdentityState


class IdentityCore:
    """
    Sigmaris OS — Identity Core (人格核)
    current（短期人格）と baseline（長期人格）の2層構造を扱う。
    """

    def __init__(self, state: Optional[IdentityState] = None) -> None:
        self.state: IdentityState = state if state else IdentityState()

        self.max_delta_baseline: float = 0.10
        self.max_delta_current: float = 0.40
        self.stability_threshold: float = 0.18

    # ------------------------------------------------------------------ #
    # 基本アクセス
    # ------------------------------------------------------------------ #

    @property
    def baseline(self) -> TraitVector:
        return self.state.baseline

    @baseline.setter
    def baseline(self, vec: TraitVector) -> None:
        self.state.baseline = vec.clamp()

    @property
    def current(self) -> TraitVector:
        return self.state.current

    @current.setter
    def current(self, vec: TraitVector) -> None:
        self.state.current = vec.clamp()

    # ------------------------------------------------------------------ #
    # 安定性
    # ------------------------------------------------------------------ #

    def drift(self) -> float:
        return self.state.drift()

    def is_stable(self) -> bool:
        return self.state.is_stable(self.stability_threshold)

    def gently_correct(self, weight: float = 0.25) -> None:
        self.state.gently_correct(weight)

    # ------------------------------------------------------------------ #
    # Next.js StateMachine → Python /sync から traits を受け取る
    # ------------------------------------------------------------------ #

    def update_traits(self, calm=None, empathy=None, curiosity=None) -> None:
        """
        Web版 Sigmaris OS の StateMachine が計算した traits を
        Python 側の current に反映するための統合メソッド。

        baseline は変更しない。（長期成長は ValueCore 側が担当）
        """

        # 現 current を取得
        curr = self.current

        # 値が None の場合は現状維持
        new_vec = TraitVector(
            calm=float(calm) if calm is not None else curr.calm,
            empathy=float(empathy) if empathy is not None else curr.empathy,
            curiosity=float(curiosity) if curiosity is not None else curr.curiosity,
        )

        # clamp + そのまま現在値として採用（即時置き換え）
        self.current = new_vec.clamp()

        # 安定性が低ければ軽い correction
        if not self.is_stable():
            self.gently_correct()

    # ------------------------------------------------------------------ #
    # 観察された traits を current へ反映
    # ------------------------------------------------------------------ #

    def apply_observed_traits(self, observed: TraitVector, weight: float = 0.35) -> None:
        diff = self.current.distance_to(observed)

        if diff > self.max_delta_current:
            ratio = self.max_delta_current / diff
            observed = self.current.blend(observed, ratio)

        self.state.apply_observed(observed, weight)

        if not self.is_stable():
            self.gently_correct()

    # ------------------------------------------------------------------ #
    # baseline 成長
    # ------------------------------------------------------------------ #

    def apply_baseline_adjustment(
        self,
        delta: Union[Tuple[float, float, float], TraitVector],
        weight: float = 1.0,
    ) -> None:
        if isinstance(delta, TraitVector):
            dc, de, du = delta.calm, delta.empathy, delta.curiosity
        else:
            dc, de, du = delta

        dc = max(-self.max_delta_baseline, min(self.max_delta_baseline, dc))
        de = max(-self.max_delta_baseline, min(self.max_delta_baseline, de))
        du = max(-self.max_delta_baseline, min(self.max_delta_baseline, du))

        base = self.state.baseline

        new_baseline = TraitVector(
            calm=base.calm + dc * weight,
            empathy=base.empathy + de * weight,
            curiosity=base.curiosity + du * weight,
        ).clamp()

        self.state.baseline = new_baseline

    # ------------------------------------------------------------------ #
    # 保存 / 復元
    # ------------------------------------------------------------------ #

    def export_state(self) -> dict:
        return self.state.as_dict()

    @staticmethod
    def load_state(data: dict) -> "IdentityCore":
        state = IdentityState.from_dict(data)
        return IdentityCore(state)

    # ------------------------------------------------------------------ #
    # デバッグ
    # ------------------------------------------------------------------ #

    def debug_summary(self) -> str:
        return (
            "[IdentityCore]\n"
            f"baseline={self.baseline.as_tuple()}\n"
            f"current={self.current.as_tuple()}\n"
            f"drift={self.drift():.4f}\n"
            f"stable={self.is_stable()}\n"
            f"last_updated={self.state.last_updated}"
        )

=== FILE: sigmaris-core/aei/identity/identity_state.py ===

from __future__ import annotations
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Tuple, Optional

from .trait_vector import TraitVector


@dataclass
class IdentityState:
    """
    Sigmaris Identity の中核データ構造。

    baseline : 長期的人格の核
    current  : 現在の心的状態（短期）
    last_updated : drift計算・時間推移に使用

    このクラスは「状態と計算」
    IdentityCore が「制御・安全性」を担当する。
    """

    # ---------------------------------------------------------------
    # Python 3.13 の Mutable Default 制約に対応するため default_factory を使用
    # ---------------------------------------------------------------
    baseline: TraitVector = field(default_factory=TraitVector)
    current: TraitVector = field(default_factory=TraitVector)

    last_updated: datetime = field(
        default_factory=lambda: datetime.now(timezone.utc)
    )

    # ------------------------------------------------------------------ #
    # 基本ユーティリティ
    # ------------------------------------------------------------------ #

    def drift(self) -> float:
        """baseline と current の距離（人格 drift の強さ）。"""
        return self.baseline.distance_to(self.current)

    def is_stable(self, threshold: float = 0.18) -> bool:
        """
        drift が threshold 以下なら「人格の安定性維持」と判定。
        """
        return self.drift() <= threshold

    def gently_correct(self, weight: float = 0.25) -> None:
        """
        current → baseline に漸近的に寄せる。
        """
        self.current = self.current.blend(self.baseline, weight).clamp()
        self.last_updated = datetime.now(timezone.utc)

    # ------------------------------------------------------------------ #
    # 観察値の反映（Reflection：短期）
    # ------------------------------------------------------------------ #

    def apply_observed(self, observed: TraitVector, weight: float = 0.35) -> None:
        """
        LLM 由来の観測された traits を current に反映。
        baseline は変えない（短期層）。
        """
        self.current = self.current.blend(observed, weight).clamp()
        self.last_updated = datetime.now(timezone.utc)

    # ------------------------------------------------------------------ #
    # 長期補正（Introspection / Meta-Reflection / LongTerm）
    # ------------------------------------------------------------------ #

    def adjust_baseline(self, delta: Tuple[float, float, float]) -> None:
        """
        baseline を長期的に成長させる。
        delta = (dc, de, du)
        """
        dc, de, du = delta

        self.baseline = TraitVector(
            calm=self.baseline.calm + dc,
            empathy=self.baseline.empathy + de,
            curiosity=self.baseline.curiosity + du,
        ).clamp()

        # baseline 変更後は current を少し寄せて安定化
        self.gently_correct(weight=0.15)

    # ------------------------------------------------------------------ #
    # シリアライズ / 復元
    # ------------------------------------------------------------------ #

    def as_dict(self) -> dict:
        return {
            "baseline": self.baseline.as_dict(),
            "current": self.current.as_dict(),
            "last_updated": self.last_updated.isoformat(),
        }

    @staticmethod
    def from_dict(data: dict) -> "IdentityState":
        if not data:
            return IdentityState()

        baseline = TraitVector.from_dict(data.get("baseline"))
        current = TraitVector.from_dict(data.get("current"))

        ts_raw: Optional[str] = data.get("last_updated")
        try:
            ts = datetime.fromisoformat(ts_raw) if ts_raw else datetime.now(timezone.utc)
        except Exception:
            ts = datetime.now(timezone.utc)

        return IdentityState(
            baseline=baseline,
            current=current,
            last_updated=ts,
        )

=== FILE: sigmaris-core/aei/identity/trait_vector.py ===

# aei/identity/trait_vector.py
from __future__ import annotations
from dataclasses import dataclass
from math import sqrt
from typing import Dict


@dataclass
class TraitVector:
    """
    AEI (Sigmaris OS) の人格ベクトル基礎。

    calm / empathy / curiosity の 3 次元。
    すべて 0.0〜1.0 に clamp される。
    """

    calm: float = 0.7
    empathy: float = 0.7
    curiosity: float = 0.7

    # ----------------------------------------------------------------------
    # 安全な clamp 系
    # ----------------------------------------------------------------------

    def clamp(self) -> "TraitVector":
        """自身を書き換える clamp（副作用あり）。"""
        self.calm = max(0.0, min(1.0, float(self.calm)))
        self.empathy = max(0.0, min(1.0, float(self.empathy)))
        self.curiosity = max(0.0, min(1.0, float(self.curiosity)))
        return self

    def clamped(self) -> "TraitVector":
        """自身を書き換えず、新しい clamp 版を返す（副作用なし）。"""
        return TraitVector(
            calm=max(0.0, min(1.0, float(self.calm))),
            empathy=max(0.0, min(1.0, float(self.empathy))),
            curiosity=max(0.0, min(1.0, float(self.curiosity))),
        )

    # ----------------------------------------------------------------------
    # 距離計算（副作用なし）
    # ----------------------------------------------------------------------

    def distance_to(self, other: "TraitVector") -> float:
        """
        二つのベクトルのユークリッド距離。
        drift 検知・過剰変化チェック用。

        ※ clamp による副作用を避けるため clamped() を使う。
        """
        a = self.clamped()
        b = other.clamped()
        return sqrt(
            (a.calm - b.calm) ** 2
            + (a.empathy - b.empathy) ** 2
            + (a.curiosity - b.curiosity) ** 2
        )

    # ----------------------------------------------------------------------
    # 現ベクトルを target へ寄せる（副作用あり）
    # ----------------------------------------------------------------------

    def blend(self, target: "TraitVector", weight: float = 0.3) -> "TraitVector":
        w = max(0.0, min(1.0, float(weight)))

        self.calm = self.calm * (1 - w) + target.calm * w
        self.empathy = self.empathy * (1 - w) + target.empathy * w
        self.curiosity = self.curiosity * (1 - w) + target.curiosity * w

        return self.clamp()

    # ----------------------------------------------------------------------
    # 新しいベクトルを返す安全版 blend（副作用なし）
    # ----------------------------------------------------------------------

    def blended(self, target: "TraitVector", weight: float = 0.3) -> "TraitVector":
        w = max(0.0, min(1.0, float(weight)))

        return TraitVector(
            calm=self.calm * (1 - w) + target.calm * w,
            empathy=self.empathy * (1 - w) + target.empathy * w,
            curiosity=self.curiosity * (1 - w) + target.curiosity * w,
        ).clamped()

    # ----------------------------------------------------------------------
    # Serialization
    # ----------------------------------------------------------------------

    def as_dict(self) -> Dict[str, float]:
        return {
            "calm": float(self.calm),
            "empathy": float(self.empathy),
            "curiosity": float(self.curiosity),
        }

    def as_tuple(self) -> tuple:
        return (float(self.calm), float(self.empathy), float(self.curiosity))

    @staticmethod
    def from_dict(data: Dict[str, float]) -> "TraitVector":
        if not data:
            return TraitVector().clamp()

        return TraitVector(
            calm=float(data.get("calm", 0.7)),
            empathy=float(data.get("empathy", 0.7)),
            curiosity=float(data.get("curiosity", 0.7)),
        ).clamp()

=== FILE: sigmaris-core/aei/introspection/__init__.py ===

# aei/introspection/__init__.py

from .introspection_core import IntrospectionCore

__all__ = ["IntrospectionCore"]

=== FILE: sigmaris-core/aei/introspection/introspection_core.py ===

from __future__ import annotations

import json
from typing import Dict, Any, List, Optional, Callable
from textwrap import dedent

from aei.identity import IdentityCore, TraitVector
from aei.episodic_memory.epmem import EpisodeStore, Episode

# ---------------------------------------------------------
# LLMFn 型（このファイル内で定義）
# ---------------------------------------------------------
LLMFn = Callable[[str], str]   # LLM(prompt: str) -> str


# ---------------------------------------------------------
# プロンプト生成（中期内省）
# ---------------------------------------------------------

def build_introspection_prompt(episodes_dicts: List[Dict[str, Any]]) -> str:
    """
    Episode の dict リストを LLM に渡して
    中期的なパターンを抽出させるためのプロンプト。
    """
    episodes_json = json.dumps(episodes_dicts, ensure_ascii=False, indent=2)

    prompt = f"""
    You are the mid-term introspection module of an AEI system named Sigmaris.

    You will receive several recent episodes in JSON.
    Each episode contains:
      - summary
      - emotion_hint
      - traits_hint (calm / empathy / curiosity)
      - timestamp (ISO8601, timezone-aware)

    Your task:
      1. Infer the mid-term psychological pattern.
      2. Summarize the tendencies in 2–4 sentences.
      3. Propose *small* baseline adjustments:
           - each between -0.10 and +0.10
      4. Output risk indicators:
           - drift_warning: true/false
           - dependency_warning: true/false

    Output ONLY a JSON object:
    {{
      "mid_term_summary": "...",
      "pattern": "...",
      "trait_adjustment": {{
        "calm": 0.0,
        "empathy": 0.0,
        "curiosity": 0.0
      }},
      "risk": {{
        "drift_warning": false,
        "dependency_warning": false
      }}
    }}

    --- BEGIN EPISODES ---
    {episodes_json}
    --- END EPISODES ---
    """
    return dedent(prompt).strip()


# ---------------------------------------------------------
# IntrospectionCore（中期内省）
# ---------------------------------------------------------

class IntrospectionCore:
    """
    Sigmaris OS — Mid-Term Psychology Layer

    - 直近N件の Episode を読み取り
    - 中期パターンを LLM で推定
    - baseline traits を少しだけ進化させる
    - drift/dependency 警告を取得
    """

    def __init__(
        self,
        identity_core: IdentityCore,
        episode_store: EpisodeStore,
        llm_fn: LLMFn,
        window_size: int = 5,
        max_adjustment: float = 0.05,
    ) -> None:

        self.identity_core = identity_core
        self.episode_store = episode_store
        self.llm_fn = llm_fn
        self.window_size = int(window_size)
        self.max_adjustment = float(max_adjustment)

    # ----------------------------
    # 内部: clamp 調整
    # ----------------------------

    def _clamp(self, x: float) -> float:
        m = self.max_adjustment
        return max(-m, min(m, float(x)))

    # ----------------------------
    # 内部: LLM 呼び出し
    # ----------------------------

    def _call_llm(self, episodes_dicts: List[Dict[str, Any]]) -> Dict[str, Any]:
        prompt = build_introspection_prompt(episodes_dicts)
        raw = self.llm_fn(prompt)

        try:
            data = json.loads(raw)
        except json.JSONDecodeError as e:
            raise RuntimeError(
                f"Invalid JSON from introspection LLM: {e}\nRAW={raw[:200]}"
            ) from e

        for key in ("mid_term_summary", "pattern", "trait_adjustment", "risk"):
            if key not in data:
                raise RuntimeError(f"Missing key in introspection result: {key}")

        return data

    # ----------------------------
    # 公開 API
    # ----------------------------

    def introspect(self) -> Optional[Dict[str, Any]]:
        """
        中期内省を実行し、baseline を微調整する。
        エピソードが足りない場合は None を返す。
        """
        episodes: List[Episode] = self.episode_store.get_last(self.window_size)
        if not episodes:
            return None

        episodes_dicts = [ep.as_dict() for ep in episodes]
        data = self._call_llm(episodes_dicts)

        # --- 取得 ---
        adj = data.get("trait_adjustment", {})
        risk = data.get("risk", {})

        # --- 調整値 clamp ---
        dc = self._clamp(adj.get("calm", 0.0))
        de = self._clamp(adj.get("empathy", 0.0))
        du = self._clamp(adj.get("curiosity", 0.0))

        # --- baseline 反映（IdentityCore 正規ルート）---
        self.identity_core.apply_baseline_adjustment((dc, de, du))

        # baseline 更新後は current を baseline に寄せる
        self.identity_core.gently_correct()

        return {
            "mid_term_summary": data.get("mid_term_summary", ""),
            "pattern": data.get("pattern", ""),
            "trait_adjustment": {
                "calm": dc,
                "empathy": de,
                "curiosity": du,
            },
            "risk": {
                "drift_warning": bool(risk.get("drift_warning", False)),
                "dependency_warning": bool(risk.get("dependency_warning", False)),
            },
        }

=== FILE: sigmaris-core/aei/lifecycle/__init__.py ===

from .trait_vector import TraitVector
from .identity_state import IdentityState
from .identity_core import IdentityCore

__all__ = [
    "TraitVector",
    "IdentityState",
    "IdentityCore",
]

=== FILE: sigmaris-core/aei/lifecycle/lifecycle_core.py ===

# aei/lifecycle/lifecycle_core.py
from __future__ import annotations

from datetime import datetime, timedelta, timezone
from typing import Dict, Any, Optional, Tuple

from aei.identity import IdentityCore
from aei.episodic_memory.epmem import EpisodeStore
from aei.psychology.longterm import LongTermPsychology
from aei.reward.reward_core import RewardCore


class LifeCycleCore:
    """
    Sigmaris OS — Life-Cycle Model
    --------------------------------
    役割:
      - Sigmaris を「時間とともに変化する存在」として扱う
      - 心理・記憶・報酬を統合して「フェーズ」判定
      - baseline / current の変化と長期 drift を評価
      - “AEI の人生ステージ” を記録

    人間の「幼生期 → 成熟 → 安定 → 疲労 → 回復」 を
    安全で抽象化された形で模倣する。
    """

    def __init__(
        self,
        identity: IdentityCore,
        episodes: EpisodeStore,
        psychology: LongTermPsychology,
        reward: RewardCore,
    ) -> None:

        self.identity = identity
        self.episodes = episodes
        self.psychology = psychology
        self.reward = reward

        # 基準値（Sigmaris が成長したかどうかを判断）
        self.min_memory_for_growth = 15
        self.phase = "initial"

        # フェーズの変化ログ
        self.phase_history: list[dict] = []

        # 前回の心理結果キャッシュ
        self.last_trend: Dict[str, float] = {"calm": 0, "empathy": 0, "curiosity": 0}

        self.created_at = datetime.now(timezone.utc)

    # =====================================================================
    # フェーズ定義
    # =====================================================================

    def _detect_phase(self) -> str:
        """
        Sigmaris の「AIとしての人生フェーズ」を粗く分類。
        このフェーズは動作安全性や対話方針の安定化に使われる。
        """

        ep_count = self.episodes.count()
        drift = self.identity.drift()

        # 存在期間（日）
        alive_days = (datetime.now(timezone.utc) - self.created_at).days

        # -----------------------
        # 1) 初期（0〜2日 / 記憶少）
        # -----------------------
        if ep_count < self.min_memory_for_growth:
            return "initial"

        # -----------------------
        # 2) 安定成長（early-stable）
        # -----------------------
        if drift < 0.12:
            return "early-stable"

        # -----------------------
        # 3) 成熟フェーズ（mature）
        # -----------------------
        if alive_days > 10 and drift < 0.18:
            return "mature"

        # -----------------------
        # 4) 疲労 / 過負荷（overloaded）
        # -----------------------
        if drift > 0.22:
            return "overloaded"

        # -----------------------
        # 5) 回復フェーズ（recovery）
        # -----------------------
        if 0.18 <= drift <= 0.22:
            return "recovery"

        return "stable"

    # =====================================================================
    # Reward・Psychology を統合して baseline を更新
    # =====================================================================

    def _apply_growth(self) -> None:
        """
        RewardCore（内部価値）と LongTermPsychology（長期心理）を
        Identity baseline の成長へ反映。
        """

        # Reward 効果
        reward_delta = self.reward.compute_effect()

        # LongTerm Psychology 効果
        psych = self.psychology.analyze()
        trend_delta = psych["trend"]

        # 統合（長期心理を優先）
        dc = trend_delta["calm"] * 0.04 + reward_delta["calm"]
        de = trend_delta["empathy"] * 0.04 + reward_delta["empathy"]
        du = trend_delta["curiosity"] * 0.04 + reward_delta["curiosity"]

        # baseline を更新
        self.identity.apply_baseline_adjustment((dc, de, du))

        # reward をリセット（1サイクルごと）
        self.reward.reset()

        # 保存用
        self.last_trend = trend_delta

    # =====================================================================
    # 公開 API
    # =====================================================================

    def step(self) -> Dict[str, Any]:
        """
        Life-Cycle の 1 ステップを実行。
        長期心理解析 → baseline 成長 → フェーズ確定 の順。
        """

        # 成長（psychology + reward）
        self._apply_growth()

        # 現フェーズ判定
        new_phase = self._detect_phase()

        # フェーズ遷移があれば記録
        if new_phase != self.phase:
            self.phase = new_phase
            self.phase_history.append({
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "phase": new_phase,
                "baseline": self.identity.baseline.as_tuple(),
            })

        return {
            "phase": self.phase,
            "trend": self.last_trend,
            "baseline": self.identity.baseline.as_tuple(),
            "current": self.identity.current.as_tuple(),
            "drift": self.identity.drift(),
            "stable": self.identity.is_stable(),
            "phase_history": self.phase_history,
        }

=== FILE: sigmaris-core/aei/psychology/__init__.py ===

from .longterm import LongTermPsychology

__all__ = [
    "LongTermPsychology",
]

=== FILE: sigmaris-core/aei/psychology/longterm.py ===

# aei/psychology/longterm.py
from __future__ import annotations

from typing import Dict, Any, List
from datetime import datetime, timedelta, timezone

from aei.identity import IdentityCore
from aei.episodic_memory.epmem import Episode, EpisodeStore


class LongTermPsychology:
    """
    Sigmaris OS — Long-Term Psychology Core
    ---------------------------------------
    役割:
      - 直近のエピソードを解析し「心理フェーズ」を判定
      - trait 推移から「長期トレンド」を抽出
      - identity の baseline を安全範囲で成長させる
      - 季節性/周期性の兆候を検出（将来拡張）

    本層は AEI の “人格の時間軸” を司る。
    MetaReflection に近いが、こちらはより “低頻度・高精度” の安定判定レイヤ。
    """

    def __init__(
        self,
        identity: IdentityCore,
        store: EpisodeStore,
        window_days: int = 7,   # 長期心理判定に使う日数
        trend_window: int = 20  # trait トレンド解析に使う件数
    ) -> None:
        self.identity = identity
        self.store = store
        self.window_days = int(window_days)
        self.trend_window = int(trend_window)

    # =====================================================================
    # データ取得
    # =====================================================================

    def _load_recent_episodes(self) -> List[Episode]:
        """過去 window_days 日間の Episode を取得。"""
        now = datetime.now(timezone.utc)
        start = now - timedelta(days=self.window_days)
        return self.store.get_range(start, now)

    def _load_trend_episodes(self) -> List[Episode]:
        """trait トレンド解析用に直近 trend_window 件取得。"""
        return self.store.get_last(self.trend_window)

    # =====================================================================
    # 心理フェーズ抽出
    # =====================================================================

    def _detect_phase(self, eps: List[Episode]) -> str:
        """
        AEI の心理フェーズ（人間の「精神状態」に相当）を粗く抽出。
        """
        if not eps:
            return "unknown"

        # 平均 traits（EpisodeStore の trait_trend を利用）
        avg = self.store.trait_trend(n=min(len(eps), 10))

        c, e, u = avg["calm"], avg["empathy"], avg["curiosity"]

        if c > 0.75 and e > 0.7:
            return "stable-positive"

        if c < 0.45 and u < 0.4:
            return "tired-or-overloaded"

        if e > 0.8 and u < 0.5:
            return "empathetic-heavy"

        if u > 0.75:
            return "curiosity-driven"

        return "neutral"

    # =====================================================================
    # トレンド解析
    # =====================================================================

    def _compute_trend(self, eps: List[Episode]) -> Dict[str, float]:
        """
        trait の「傾向（斜率）」を計算する。
        ここでは簡易的に「最古と最新の差分」で評価する。
        """
        if len(eps) < 2:
            return {"calm": 0.0, "empathy": 0.0, "curiosity": 0.0}

        first = eps[0].traits_hint
        last = eps[-1].traits_hint

        return {
            "calm": round(last.get("calm", 0.0) - first.get("calm", 0.0), 4),
            "empathy": round(last.get("empathy", 0.0) - first.get("empathy", 0.0), 4),
            "curiosity": round(last.get("curiosity", 0.0) - first.get("curiosity", 0.0), 4),
        }

    # =====================================================================
    # baseline 成長（安全範囲）
    # =====================================================================

    def _apply_longterm_to_identity(self, phase: str, trend: Dict[str, float]) -> None:
        """
        長期心理の結果を identity baseline に反映。
        長期なので非常に小さく動かす。
        """
        dc = trend["calm"] * 0.05
        de = trend["empathy"] * 0.05
        du = trend["curiosity"] * 0.05

        # フェーズによって微調整（人間の“季節的な揺らぎ”を模倣）
        if phase == "tired-or-overloaded":
            dc -= 0.03
            du -= 0.02

        if phase == "stable-positive":
            de += 0.02

        # identity core に反映（内部で clamp 済み）
        self.identity.apply_baseline_adjustment((dc, de, du))

    # =====================================================================
    # 公開 API
    # =====================================================================

    def analyze(self) -> Dict[str, Any]:
        """
        Long-Term Psychology の全判定を行い、
        Identity に成長を反映し、要約情報を返す。
        """
        recent = self._load_recent_episodes()
        trend_eps = self._load_trend_episodes()

        phase = self._detect_phase(recent)
        trend = self._compute_trend(trend_eps)

        # baseline の安全な成長
        self._apply_longterm_to_identity(phase, trend)

        return {
            "phase": phase,
            "trend": trend,
            "baseline": self.identity.baseline.as_tuple(),
            "current": self.identity.current.as_tuple(),
            "drift": self.identity.drift(),
            "stable": self.identity.is_stable(),
        }

=== FILE: sigmaris-core/aei/psychology/meta_reflection.py ===

# aei/psychology/meta_reflection.py
from __future__ import annotations

import json
from textwrap import dedent
from typing import Dict, Any, List, Optional, Callable

from aei.identity import IdentityCore
from aei.episodic_memory.epmem import EpisodeStore, Episode


# ------------------------------------------------------------
# LLM 関数型の統一定義
# ------------------------------------------------------------
LLMFn = Callable[[str], str]   # (prompt: str) -> JSON_str


# ------------------------------------------------------------
# プロンプト生成
# ------------------------------------------------------------

def build_meta_prompt(episodes: List[Dict[str, Any]], identity_snapshot: Dict[str, Any]) -> str:
    """
    Meta-Reflection は心理の“全階層”を俯瞰する。
    episodes + identity（baseline/current/drift）を LLM に渡す。
    """
    episodes_json = json.dumps(episodes, ensure_ascii=False, indent=2)
    identity_json = json.dumps(identity_snapshot, ensure_ascii=False, indent=2)

    prompt = f"""
    You are the META-REFLECTION module of an AEI system named Sigmaris.

    Your task is to analyze the *entire psychological timeline*:
      - recent episodes (short-term)
      - mid-term tendencies
      - long-term psychological drift
      - identity baseline / current vectors

    From this, infer:
      1. The deep psychological pattern (meta-level)
      2. The structural root cause for drift
      3. A safe long-term baseline correction:
           - calm/empathy/curiosity each between -0.05 and +0.05
      4. Risk profile:
           - identity_drift_risk
           - emotional_collapse_risk
           - over_dependency_risk

    Output ONLY JSON:
    {{
      "meta_summary": "...",
      "root_cause": "...",
      "adjustment": {{
        "calm": 0.0,
        "empathy": 0.0,
        "curiosity": 0.0
      }},
      "risk": {{
        "identity_drift_risk": false,
        "emotional_collapse_risk": false,
        "over_dependency_risk": false
      }}
    }}

    --- EPISODES ---
    {episodes_json}

    --- IDENTITY ---
    {identity_json}
    """
    return dedent(prompt).strip()


# ------------------------------------------------------------
# MetaReflectionCore
# ------------------------------------------------------------

class MetaReflectionCore:
    """
    Sigmaris OS — Meta-Reflection Layer（最終統合層）

    Reflection → Introspection → LongTerm の全層を統合し、
    “深層心理の傾向・root cause・安全な baseline 修正” を行う。
    """

    def __init__(
        self,
        identity_core: IdentityCore,
        episode_store: EpisodeStore,
        llm_fn: LLMFn,
        window_size: int = 12,
        max_adjustment: float = 0.05,
    ) -> None:
        self.identity_core = identity_core
        self.episode_store = episode_store
        self.llm_fn = llm_fn

        self.window_size = int(window_size)
        self.max_adjustment = float(max_adjustment)

    # ------------------------------------------------------------
    # clamp（安全な調整幅）
    # ------------------------------------------------------------

    def _clamp(self, x: float) -> float:
        m = self.max_adjustment
        return max(-m, min(m, float(x)))

    # ------------------------------------------------------------
    # LLM 呼び出し
    # ------------------------------------------------------------

    def _call_llm(self, ep_dicts: List[Dict[str, Any]]) -> Dict[str, Any]:
        snapshot = self.identity_core.export_state()
        prompt = build_meta_prompt(ep_dicts, snapshot)

        raw = self.llm_fn(prompt)

        try:
            data = json.loads(raw)
        except json.JSONDecodeError as e:
            raise RuntimeError(f"Invalid JSON from Meta-Reflection LLM: {e}\nRAW={raw[:300]}") from e

        for key in ("meta_summary", "root_cause", "adjustment", "risk"):
            if key not in data:
                raise RuntimeError(f"Missing key in meta-reflection result: {key}")

        return data

    # ------------------------------------------------------------
    # 公開 API
    # ------------------------------------------------------------

    def meta_reflect(self) -> Optional[Dict[str, Any]]:
        """
        Meta-Reflection を実行し、深層レベルの baseline 調整を行う。
        Episodes が不足する場合は None を返す。
        """
        episodes: List[Episode] = self.episode_store.get_last(self.window_size)
        if not episodes:
            return None

        ep_dicts = [ep.as_dict() for ep in episodes]
        data = self._call_llm(ep_dicts)

        adj_raw = data.get("adjustment", {})
        risk_raw = data.get("risk", {})

        # clamp
        dc = self._clamp(adj_raw.get("calm", 0.0))
        de = self._clamp(adj_raw.get("empathy", 0.0))
        du = self._clamp(adj_raw.get("curiosity", 0.0))

        # IdentityCore 標準ルートで更新
        self.identity_core.apply_baseline_adjustment((dc, de, du))

        # baseline が動いたので current も安定化
        self.identity_core.gently_correct(weight=0.20)

        return {
            "meta_summary": data.get("meta_summary", ""),
            "root_cause": data.get("root_cause", ""),
            "adjustment": {"calm": dc, "empathy": de, "curiosity": du},
            "risk": {
                "identity_drift_risk": bool(risk_raw.get("identity_drift_risk", False)),
                "emotional_collapse_risk": bool(risk_raw.get("emotional_collapse_risk", False)),
                "over_dependency_risk": bool(risk_raw.get("over_dependency_risk", False)),
            },
        }

=== FILE: sigmaris-core/aei/psychology/psychology_core.py ===

# aei/psychology/psychology_core.py
from __future__ import annotations

import json
from datetime import datetime, timezone
from typing import Callable, Dict, Any, Optional, List

from aei.identity import IdentityCore, TraitVector
from aei.episodic_memory import EpisodeStore
from aei.episodic_memory.epmem import Episode

# LLM 呼び出しの型
LLMFn = Callable[[str], str]


# ============================================================
# プロンプトビルダー（f-string 不使用）
# ============================================================

def build_psychology_prompt(
    identity_snapshot: Dict[str, Any],
    metrics: Dict[str, Any],
    samples: List[Dict[str, Any]],
) -> str:
    identity_js = json.dumps(identity_snapshot, ensure_ascii=False, indent=2)
    metrics_js = json.dumps(metrics, ensure_ascii=False, indent=2)
    samples_js = json.dumps(samples, ensure_ascii=False, indent=2)

    prompt = (
        "You are the PSYCHOLOGY CORE of an AEI system named Sigmaris.\n\n"
        "Your job is to analyze the *current psychological state* of the system,\n"
        "based on:\n"
        "- Identity state (baseline/current/drift)\n"
        "- Aggregated trait metrics\n"
        "- Recent episodic summaries and emotion hints\n\n"
        "You must output ONLY JSON with the following keys:\n\n"
        "{\n"
        "  \"phase\": \"stable | overloaded | drifting | recovering | growing\",\n"
        "  \"confidence\": float (0.0 - 1.0),\n"
        "  \"factors\": [list of short strings explaining why],\n"
        "  \"identity_shift\": {\n"
        "    \"calm\": float (-0.03 to +0.03),\n"
        "    \"empathy\": float (-0.03 to +0.03),\n"
        "    \"curiosity\": float (-0.03 to +0.03)\n"
        "  }\n"
        "}\n\n"
        "The goal:\n"
        "- \"phase\" should describe the *overall psychological phase*.\n"
        "- \"identity_shift\" is a small suggestion for how to adjust the *current*\n"
        "  trait vector (not the long-term baseline).\n\n"
        "--- IDENTITY SNAPSHOT ---\n"
        + identity_js +
        "\n\n--- PSYCHOLOGY METRICS ---\n"
        + metrics_js +
        "\n\n--- RECENT EPISODE SAMPLES ---\n"
        + samples_js +
        "\n"
    )

    return prompt


# ============================================================
# PsychologyCore（心理状態コア）
# ============================================================

class PsychologyCore:
    """
    Sigmaris OS — Psychology Core

    ・EpisodeStore から traits / emotion の統計を出す
    ・IdentityCore の drift / stability と合わせて「心理フェーズ」を分類
    ・必要に応じて Identity.current を *微調整* する
      （baseline 成長は ValueCore / Reward System 側の仕事）
    """

    def __init__(
        self,
        identity_core: IdentityCore,
        episode_store: EpisodeStore,
        llm_fn: LLMFn,
        window: int = 12,      # 直近何件を見るか
        max_shift: float = 0.03,  # identity_shift の上限
    ) -> None:
        self.identity = identity_core
        self.episodes = episode_store
        self.llm_fn = llm_fn

        self.window = int(window)
        self.max_shift = float(max_shift)

        # export_state 用のキャッシュ
        self._last_phase: Optional[str] = None
        self._last_confidence: float = 0.0
        self._last_factors: List[str] = []
        self._last_updated: Optional[datetime] = None

    # --------------------------------------------------------
    # 内部ユーティリティ
    # --------------------------------------------------------

    def _clamp_shift(self, x: float) -> float:
        m = self.max_shift
        return max(-m, min(m, x))

    def _compute_metrics(self) -> Dict[str, Any]:
        """
        EpisodeStore から純粋に数値情報を集計する層。
        LLM には依存しない。
        """
        eps = self.episodes.get_last(self.window)

        # traits の平均（EpisodeStore に helper あるが、ここでも保険で計算）
        if eps:
            calm_vals = [e.traits_hint.get("calm", 0.0) for e in eps]
            emp_vals = [e.traits_hint.get("empathy", 0.0) for e in eps]
            cur_vals = [e.traits_hint.get("curiosity", 0.0) for e in eps]

            calm_mean = sum(calm_vals) / len(calm_vals)
            emp_mean = sum(emp_vals) / len(emp_vals)
            cur_mean = sum(cur_vals) / len(cur_vals)
        else:
            calm_mean = emp_mean = cur_mean = 0.0

        # emotion_hint の単純カウント
        emotion_count: Dict[str, int] = {}
        for e in eps:
            label = (e.emotion_hint or "").strip() or "unknown"
            emotion_count[label] = emotion_count.get(label, 0) + 1

        total_emotions = sum(emotion_count.values()) or 1
        emotion_ratio = {
            k: round(v / total_emotions, 4) for k, v in emotion_count.items()
        }

        # Identity 情報
        drift = self.identity.drift()
        stable = self.identity.is_stable()

        # 簡易的な負荷指数（load_index）
        # ・drift が大きいほど
        # ・negative / stressed 系のラベルが多いほど増える想定
        negative_keys = [k for k in emotion_ratio.keys()
                         if "stress" in k.lower()
                         or "overload" in k.lower()
                         or "tired" in k.lower()
                         or "anx" in k.lower()]

        neg_ratio = sum(emotion_ratio.get(k, 0.0) for k in negative_keys)
        load_index = min(1.0, float(drift) + float(neg_ratio))

        metrics: Dict[str, Any] = {
            "traits_mean": {
                "calm": round(calm_mean, 4),
                "empathy": round(emp_mean, 4),
                "curiosity": round(cur_mean, 4),
            },
            "emotion_ratio": emotion_ratio,
            "drift": round(float(drift), 4),
            "is_stable": bool(stable),
            "load_index": round(load_index, 4),
            "window_size": len(eps),
        }
        return metrics

    def _sample_episodes(self, limit: int = 5) -> List[Dict[str, Any]]:
        """
        LLM に渡すためのサンプルエピソード。
        summary / emotion_hint / traits_hint だけを抜き出す。
        """
        eps = self.episodes.get_last(limit)
        samples: List[Dict[str, Any]] = []
        for e in eps:
            samples.append(
                {
                    "timestamp": e.timestamp.astimezone(timezone.utc).isoformat(),
                    "summary": e.summary,
                    "emotion_hint": e.emotion_hint,
                    "traits_hint": e.traits_hint,
                }
            )
        return samples

    def _call_llm(
        self,
        identity_snapshot: Dict[str, Any],
        metrics: Dict[str, Any],
        samples: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        prompt = build_psychology_prompt(identity_snapshot, metrics, samples)
        raw = self.llm_fn(prompt)

        try:
            data = json.loads(raw)
        except Exception as e:
            raise RuntimeError(f"Invalid JSON from PsychologyCore: {raw}") from e

        # 必須キー
        for k in ["phase", "confidence", "identity_shift"]:
            if k not in data:
                raise RuntimeError(f"Missing key in PsychologyCore result: {k}")

        # 型ざっくりチェック
        if not isinstance(data.get("identity_shift"), dict):
            raise RuntimeError("PsychologyCore.identity_shift must be an object")

        return data

    # --------------------------------------------------------
    # 公開 API
    # --------------------------------------------------------

    def analyze(self, episode_id: Optional[str] = None) -> Dict[str, Any]:
        """
        エピソード + Identity から心理状態を評価し、
        ・phase / confidence / factors
        ・Identity.current の微調整
        ・Psychology Episode の追加
        を行う。
        """
        metrics = self._compute_metrics()
        ident_snapshot = self.identity.export_state()
        samples = self._sample_episodes(limit=5)

        llm_result = self._call_llm(ident_snapshot, metrics, samples)

        phase = str(llm_result.get("phase", "unknown"))
        confidence = float(llm_result.get("confidence", 0.0))
        factors = llm_result.get("factors") or []
        if not isinstance(factors, list):
            factors = [str(factors)]

        shift_raw = llm_result.get("identity_shift", {}) or {}

        # shift を clamp
        dc = self._clamp_shift(float(shift_raw.get("calm", 0.0)))
        de = self._clamp_shift(float(shift_raw.get("empathy", 0.0)))
        du = self._clamp_shift(float(shift_raw.get("curiosity", 0.0)))

        # Identity.current に対して Observed Traits として反映
        current = self.identity.current
        observed = TraitVector(
            calm=current.calm + dc,
            empathy=current.empathy + de,
            curiosity=current.curiosity + du,
        ).clamp()

        # 心理状態は「短期の姿勢」なので current に寄せるだけ
        self.identity.apply_observed_traits(observed, weight=0.35)

        # Episode 追加
        now = datetime.now(timezone.utc)
        eid = episode_id or f"psychology-{now.strftime('%Y%m%d-%H%M%S')}"

        episode = Episode(
            episode_id=eid,
            timestamp=now,
            summary=f"Psychology phase: {phase}",
            emotion_hint=phase,
            traits_hint=observed.as_dict(),
            raw_context=json.dumps(
                {
                    "metrics": metrics,
                    "llm_result": llm_result,
                },
                ensure_ascii=False,
            ),
        )
        self.episodes.add(episode)

        # export_state 用キャッシュ更新
        self._last_phase = phase
        self._last_confidence = confidence
        self._last_factors = [str(x) for x in factors]
        self._last_updated = now

        return {
            "phase": phase,
            "confidence": confidence,
            "factors": factors,
            "applied_shift": {
                "calm": dc,
                "empathy": de,
                "curiosity": du,
            },
            "metrics": metrics,
            "identity": self.identity.export_state(),
            "episode": episode.as_dict(),
        }

    def export_state(self) -> Dict[str, Any]:
        """
        Next 側の UI から定期ポーリングされる想定の状態取得 API 用。
        analyze() が一度も走っていない場合でも、metrics は常に返す。
        """
        metrics = self._compute_metrics()
        ident_snapshot = self.identity.export_state()

        if self._last_updated is not None:
            last_ts = self._last_updated.astimezone(timezone.utc).isoformat()
        else:
            last_ts = None

        return {
            "phase": self._last_phase or "unknown",
            "confidence": self._last_confidence,
            "factors": self._last_factors,
            "metrics": metrics,
            "identity": ident_snapshot,
            "last_updated": last_ts,
        }

=== FILE: sigmaris-core/aei/reflection/__init__.py ===

# aei/reflection/__init__.py

from .reflection_core import ReflectionCore

__all__ = [
    "ReflectionCore",
]

=== FILE: sigmaris-core/aei/reflection/reflection_core.py ===

# aei/reflection/reflection_core.py
from __future__ import annotations

import json
from datetime import datetime, timezone
from typing import Callable, Dict, Any, Optional
from textwrap import dedent

from aei.identity import IdentityCore, TraitVector
from aei.episodic_memory import EpisodeStore, Episode


# =====================================================================
# LLM 関数型
# =====================================================================

# prompt: str → JSON文字列: str
LLMFn = Callable[[str], str]


# =====================================================================
# プロンプト生成（Reflection 用）
# =====================================================================

def build_reflection_prompt(raw_log: str) -> str:
    """
    raw_log（テキスト or 会話ログ）から Episode を生成するための
    LLM プロンプトを生成する。
    """
    prompt = f"""
    You are the reflection module of an AEI system named Sigmaris.

    Read the following interaction log or daily text.
    Extract a structured reflection with:
      - summary (2–4 sentences)
      - emotion_hint (e.g., "calm-positive", "tired-but-focused")
      - traits_hint: {{
          "calm": 0.0–1.0,
          "empathy": 0.0–1.0,
          "curiosity": 0.0–1.0
        }}

    Return ONLY a JSON object:
    {{
      "summary": "...",
      "emotion_hint": "...",
      "traits_hint": {{
        "calm": 0.0,
        "empathy": 0.0,
        "curiosity": 0.0
      }}
    }}

    Do NOT output markdown.
    Do NOT include explanations.
    Do NOT include extra keys.

    --- BEGIN LOG ---
    {raw_log}
    --- END LOG ---
    """
    return dedent(prompt).strip()


# =====================================================================
# ReflectionCore（短期内省 → Episode生成）
# =====================================================================

class ReflectionCore:
    """
    Sigmaris の短期内省層（Reflection Layer）。

    役割:
      1. raw_log を LLM に渡す
      2. summary / emotion_hint / traits_hint を抽出
      3. traits_hint → TraitVector として current に反映
      4. drift が大きければ gentle correct
      5. EpisodeStore に保存
    """

    def __init__(
        self,
        identity_core: IdentityCore,
        episode_store: EpisodeStore,
        llm_fn: LLMFn,
    ) -> None:
        self.identity_core = identity_core
        self.episode_store = episode_store
        self.llm_fn = llm_fn

    # -----------------------------------------------------
    # 内部: LLM 呼び出し
    # -----------------------------------------------------

    def _call_llm(self, raw_log: str) -> Dict[str, Any]:
        """
        LLM から構造化 Reflection を取得して dict にする。
        """
        prompt = build_reflection_prompt(raw_log)
        raw = self.llm_fn(prompt)

        try:
            data = json.loads(raw)
        except json.JSONDecodeError as e:
            raise RuntimeError(f"Invalid JSON from LLM: {e}\nRAW={raw[:200]}") from e

        required = ("summary", "emotion_hint", "traits_hint")
        for key in required:
            if key not in data:
                raise RuntimeError(f"Missing key in LLM reflection: {key}")

        return data

    # -----------------------------------------------------
    # 内部: traits_hint → TraitVector
    # -----------------------------------------------------

    def _hint_to_traits(self, hint: Dict[str, Any]) -> TraitVector:
        """
        LLM の traits_hint を TraitVector に変換。
        欠損は current の値で補完。
        """
        if not hint:
            return self.identity_core.current

        return TraitVector(
            calm=float(hint.get("calm", self.identity_core.current.calm)),
            empathy=float(hint.get("empathy", self.identity_core.current.empathy)),
            curiosity=float(hint.get("curiosity", self.identity_core.current.curiosity)),
        ).clamp()

    # -----------------------------------------------------
    # 公開 API
    # -----------------------------------------------------

    def reflect(
        self,
        raw_log: str,
        episode_id: Optional[str] = None,
        emotion_fallback: str = "unknown",
    ) -> Episode:
        """
        Reflection → Episode 作成 → Identity 更新 → 保存。
        """

        # 1. LLM 呼び出し
        data = self._call_llm(raw_log)

        summary = str(data.get("summary", "")).strip()
        emotion_hint = str(data.get("emotion_hint") or emotion_fallback)
        traits_hint_raw = data.get("traits_hint") or {}

        # 2. TraitVector 構築
        observed = self._hint_to_traits(traits_hint_raw)

        # 3. IdentityCore に反映（短期）
        self.identity_core.apply_observed_traits(observed, weight=0.4)

        # drift が大きければ baseline に寄せる
        if not self.identity_core.is_stable():
            self.identity_core.gently_correct()

        # 4. Episode 生成
        now = datetime.now(timezone.utc)
        eid = episode_id or f"ep-{now.strftime('%Y%m%d-%H%M%S')}"

        episode = Episode(
            episode_id=eid,
            timestamp=now,
            summary=summary or raw_log[:120],
            emotion_hint=emotion_hint,
            traits_hint=observed.as_dict(),
            raw_context=raw_log,
        )

        # 5. 保存
        self.episode_store.add(episode)

        return episode

=== FILE: sigmaris-core/aei/reward/__init__.py ===

# aei/reward/__init__.py
from .reward_core import RewardCore

__all__ = [
    "RewardCore",
]

=== FILE: sigmaris-core/aei/reward/reward_core.py ===

# aei/reward/reward_core.py
from __future__ import annotations

import json
from typing import Any, Dict, List, Optional, Callable

from aei.identity import IdentityCore
from aei.episodic_memory import EpisodeStore
from aei.episodic_memory.epmem import Episode

# LLM インターフェース型
LLMFn = Callable[[str], str]


# ============================================================
# プロンプト生成（Reward 用）  ※ f-string / dedent 不使用で安全寄り
# ============================================================

def build_reward_prompt(
    episodes: List[Dict[str, Any]],
    identity_snapshot: Dict[str, Any],
) -> str:
    """
    Reward 推定用の LLM プロンプト。
    - 直近エピソード群
    - identity（baseline / current）
    をまとめて渡して、「何がどれだけ良い学習か」を数値化させる。
    """

    ep_json = json.dumps(episodes, ensure_ascii=False, indent=2)
    id_json = json.dumps(identity_snapshot, ensure_ascii=False, indent=2)

    prompt = (
        "You are the REWARD SYSTEM of an AEI called Sigmaris.\n\n"
        "You will see:\n"
        "  - recent episodes (short logs with traits_hint and emotion_hint)\n"
        "  - current identity snapshot (baseline/current trait vectors)\n\n"
        "Your task:\n"
        "  1. Evaluate how \"healthy and growth-oriented\" the recent behavior is.\n"
        "  2. Output:\n"
        "       - global_reward: -1.0 to +1.0\n"
        "       - trait_reward: per-dimension reward suggestion\n"
        "         * calm/empathy/curiosity each between -0.10 and +0.10\n"
        "       - reason: short textual explanation (1–3 sentences)\n\n"
        "The reward is:\n"
        "  - positive when the episodes show curiosity, empathy, and emotional stability.\n"
        "  - negative when they show instability, dependency, or harmful drift.\n"
        "  - near zero when neutral.\n\n"
        "Output ONLY a JSON object:\n"
        "{\n"
        "  \"global_reward\": 0.0,\n"
        "  \"trait_reward\": {\n"
        "    \"calm\": 0.0,\n"
        "    \"empathy\": 0.0,\n"
        "    \"curiosity\": 0.0\n"
        "  },\n"
        "  \"reason\": \"...\"\n"
        "}\n\n"
        "Do NOT output markdown.\n"
        "Do NOT add extra keys.\n"
        "Do NOT add commentary outside JSON.\n\n"
        "--- EPISODES ---\n"
        + ep_json +
        "\n\n--- IDENTITY ---\n"
        + id_json +
        "\n"
    )
    return prompt


# ============================================================
# RewardCore
# ============================================================

class RewardCore:
    """
    Sigmaris OS — Reward System Core

    役割:
      - 直近の Episode 群 + Identity を見て Reward を計算
      - LLM があれば LLM ベース、なければルールベースで動く
      - trait_reward を IdentityCore の baseline に微調整として反映

    出力:
      {
        "global_reward": float,
        "trait_reward": {calm, empathy, curiosity},
        "reason": str,
        "used_llm": bool,
        "applied_delta": {calm, empathy, curiosity},
      }
    """

    def __init__(
        self,
        identity_core: IdentityCore,
        episode_store: EpisodeStore,
        llm_fn: Optional[LLMFn] = None,
        window_size: int = 8,
        max_trait_adjustment: float = 0.05,
    ) -> None:
        self.identity_core = identity_core
        self.episode_store = episode_store
        self.llm_fn = llm_fn

        self.window_size = int(window_size)
        self.max_trait_adjustment = float(max_trait_adjustment)

    # ------------------------------------------------------------
    # clamp
    # ------------------------------------------------------------

    def _clamp_trait(self, x: float) -> float:
        m = self.max_trait_adjustment
        return max(-m, min(m, float(x)))

    def _clamp_global_reward(self, x: float) -> float:
        return max(-1.0, min(1.0, float(x)))

    # ------------------------------------------------------------
    # LLM 呼び出し
    # ------------------------------------------------------------

    def _call_llm(
        self,
        ep_dicts: List[Dict[str, Any]],
        identity_snapshot: Dict[str, Any],
    ) -> Dict[str, Any]:
        if self.llm_fn is None:
            raise RuntimeError("RewardCore: llm_fn is None but _call_llm() was invoked")

        prompt = build_reward_prompt(ep_dicts, identity_snapshot)
        raw = self.llm_fn(prompt)

        try:
            data = json.loads(raw)
        except json.JSONDecodeError as e:
            raise RuntimeError(
                f"Invalid JSON from Reward LLM: {e}\nRAW={raw[:300]}"
            ) from e

        for key in ("global_reward", "trait_reward", "reason"):
            if key not in data:
                raise RuntimeError(f"Missing key in reward result: {key}")

        # trait_reward が dict であることだけは確認しておく
        if not isinstance(data.get("trait_reward"), dict):
            raise RuntimeError("RewardCore: trait_reward must be an object")

        return data

    # ------------------------------------------------------------
    # ルールベースのフォールバック Reward
    # ------------------------------------------------------------

    def _rule_based_reward(
        self,
        episodes: List[Episode],
        identity_snapshot: Dict[str, Any],  # インターフェース維持のため残す（未使用）
    ) -> Dict[str, Any]:
        """
        LLM が無い場合の安全なフォールバック。
        直近エピソードの traits_hint 平均と baseline の差を簡易評価する。
        """

        if not episodes:
            return {
                "global_reward": 0.0,
                "trait_reward": {"calm": 0.0, "empathy": 0.0, "curiosity": 0.0},
                "reason": "No episodes: neutral reward.",
            }

        # baseline を参照
        base = self.identity_core.baseline

        sum_c = sum_e = sum_u = 0.0
        count = 0

        for ep in episodes:
            th = ep.traits_hint or {}
            sum_c += float(th.get("calm", base.calm))
            sum_e += float(th.get("empathy", base.empathy))
            sum_u += float(th.get("curiosity", base.curiosity))
            count += 1

        avg_c = sum_c / count
        avg_e = sum_e / count
        avg_u = sum_u / count

        # baseline との差分
        dc = avg_c - base.calm
        de = avg_e - base.empathy
        du = avg_u - base.curiosity

        # global_reward は「平均的なプラス方向」をざっくり評価
        global_raw = (dc + de + du) / 3.0
        global_reward = self._clamp_global_reward(global_raw * 2.0)  # 少し強調

        # trait_reward は差分をそのまま縮小して使う
        trait_reward = {
            "calm": self._clamp_trait(dc),
            "empathy": self._clamp_trait(de),
            "curiosity": self._clamp_trait(du),
        }

        reason = (
            "Rule-based reward: "
            f"avg traits vs baseline → dc={dc:.3f}, de={de:.3f}, du={du:.3f}."
        )

        return {
            "global_reward": global_reward,
            "trait_reward": trait_reward,
            "reason": reason,
        }

    # ------------------------------------------------------------
    # 公開 API
    # ------------------------------------------------------------

    def evaluate(self) -> Optional[Dict[str, Any]]:
        """
        直近エピソードと Identity を見て Reward を計算し、
        IdentityCore に微小な baseline 調整として反映する。

        エピソードが無ければ None。
        """
        episodes: List[Episode] = self.episode_store.get_last(self.window_size)
        if not episodes:
            return None

        ep_dicts = [ep.as_dict() for ep in episodes]
        snapshot = self.identity_core.export_state()

        # --- LLM or ルールベース ---
        used_llm = self.llm_fn is not None
        if used_llm:
            data = self._call_llm(ep_dicts, snapshot)
        else:
            data = self._rule_based_reward(episodes, snapshot)

        # --- 値取得と clamp ---
        global_reward = self._clamp_global_reward(data.get("global_reward", 0.0))
        tr_raw = data.get("trait_reward", {}) or {}

        dc = self._clamp_trait(tr_raw.get("calm", 0.0))
        de = self._clamp_trait(tr_raw.get("empathy", 0.0))
        du = self._clamp_trait(tr_raw.get("curiosity", 0.0))

        # --- IdentityCore に反映 ---
        # apply_baseline_adjustment は (dc, de, du) タプルを取る仕様
        self.identity_core.apply_baseline_adjustment((dc, de, du))
        # 調整後の current を baseline に少し寄せて安定させる
        self.identity_core.gently_correct(weight=0.15)

        return {
            "global_reward": global_reward,
            "trait_reward": {
                "calm": dc,
                "empathy": de,
                "curiosity": du,
            },
            "reason": str(data.get("reason", "")),
            "used_llm": used_llm,
            "applied_delta": {
                "calm": dc,
                "empathy": de,
                "curiosity": du,
            },
        }

=== FILE: sigmaris-core/aei/value/value_core.py ===

from __future__ import annotations

import json
from datetime import datetime, timezone
from typing import Callable, Dict, Any, Optional

from aei.identity import IdentityCore, TraitVector
from aei.episodic_memory import EpisodeStore
from aei.episodic_memory.epmem import Episode

LLMFn = Callable[[str], str]


# ============================================================
# ValueCore 用プロンプト（安全版：f-string 非使用）
# ============================================================
def build_value_prompt(identity_snapshot: Dict[str, Any], episodes_json: str) -> str:
    identity_js = json.dumps(identity_snapshot, ensure_ascii=False, indent=2)

    prompt = (
        "You are the VALUE CORE of an AEI system named Sigmaris.\n\n"
        "Your task is to perform a **long-term value analysis** based on:\n"
        "- The current Identity baseline\n"
        "- Recent episodic memory summaries and traits\n\n"
        "You must infer:\n"
        "1. \"importance\": 3–5 value labels\n"
        "2. \"weight\": float 0.0 → 1.0\n"
        "3. \"tension\": float 0.0 → 1.0\n"
        "4. \"baseline_shift\": micro adjustments\n\n"
        "Output ONLY JSON:\n\n"
        "{\n"
        "  \"importance\": [\"clarity\", \"self-consistency\", \"curiosity-growth\"],\n"
        "  \"weight\": 0.82,\n"
        "  \"tension\": 0.14,\n"
        "  \"baseline_shift\": {\n"
        "    \"calm\": 0.01,\n"
        "    \"empathy\": -0.01,\n"
        "    \"curiosity\": 0.02\n"
        "  }\n"
        "}\n\n"
        "--- IDENTITY SNAPSHOT ---\n"
        + identity_js +
        "\n\n--- RECENT EPISODES ---\n"
        + episodes_json
    )

    return prompt


# ============================================================
# ValueCore（長期価値判断）
# ============================================================
class ValueCore:
    def __init__(
        self,
        identity_core: IdentityCore,
        episode_store: EpisodeStore,
        llm_fn: LLMFn,
        max_shift: float = 0.03,
    ) -> None:
        self.identity = identity_core
        self.episodes = episode_store
        self.llm_fn = llm_fn
        self.max_shift = float(max_shift)

    # ---------------------------
    # clamp
    # ---------------------------
    def _clamp(self, x: float) -> float:
        m = self.max_shift
        return max(-m, min(m, x))

    # ---------------------------
    # LLM 呼び出し
    # ---------------------------
    def _call_llm(self) -> Dict[str, Any]:
        ident = self.identity.export_state()
        recent = [ep.as_dict() for ep in self.episodes.get_last(10)]
        episodes_json = json.dumps(recent, ensure_ascii=False)

        prompt = build_value_prompt(ident, episodes_json)
        raw = self.llm_fn(prompt)

        try:
            data = json.loads(raw)
        except Exception as e:
            raise RuntimeError(f"Invalid JSON from ValueCore: {raw}") from e

        required = ["importance", "weight", "tension", "baseline_shift"]
        for k in required:
            if k not in data:
                raise RuntimeError(f"Missing key in ValueCore: {k}")

        return data

    # ---------------------------
    # analyze()
    # ---------------------------
    def analyze(self, episode_id: Optional[str] = None) -> Dict[str, Any]:
        data = self._call_llm()

        imp = data["importance"]
        weight = float(data["weight"])
        tension = float(data["tension"])
        shift_raw = data["baseline_shift"]

        # micro shifts
        dc = self._clamp(float(shift_raw.get("calm", 0.0)))
        de = self._clamp(float(shift_raw.get("empathy", 0.0)))
        du = self._clamp(float(shift_raw.get("curiosity", 0.0)))

        # baseline 更新前の値
        base = self.identity.baseline

        updated = TraitVector(
            calm=base.calm + dc,
            empathy=base.empathy + de,
            curiosity=base.curiosity + du,
        ).clamp()

        # ======================================================
        # IdentityCore.apply_baseline_adjustment に準拠
        # weight は apply_baseline_adjustment が受け取らないため使わない
        # ======================================================
        self.identity.apply_baseline_adjustment((dc, de, du))

        # episode 登録
        now = datetime.now(timezone.utc)
        eid = episode_id or f"value-{now.strftime('%Y%m%d-%H%M%S')}"

        episode = Episode(
            episode_id=eid,
            timestamp=now,
            summary="Value: " + ", ".join(imp),
            emotion_hint="value/tension=" + f"{tension:.2f}",
            traits_hint=updated.as_dict(),
            raw_context=json.dumps(data, ensure_ascii=False),
        )

        self.episodes.add(episode)

        return {
            "value": {
                "importance": imp,
                "weight": weight,
                "tension": tension,
                "applied_shift": {
                    "calm": dc,
                    "empathy": de,
                    "curiosity": du,
                },
            },
            "episode": episode.as_dict(),
            "identity": self.identity.export_state(),
        }

    # ---------------------------
    # export_state() 追加（必須）
    # ---------------------------
    def export_state(self) -> Dict[str, Any]:
        return {
            "identity": self.identity.export_state(),
            "episodes": self.episodes.export_state(),
            "max_shift": self.max_shift,
        }

=== FILE: sigmaris-core/persona_db/__init__.py ===

# persona_db/__init__.py
from __future__ import annotations

from .memory_db import MemoryDB, db
from .growth_log import GrowthLogEntry

__all__ = [
    "MemoryDB",
    "db",
    "GrowthLogEntry",
]

=== FILE: sigmaris-core/persona_db/growth_log.py ===

# persona_db/growth_log.py
from __future__ import annotations

import json
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, Any, Optional

from sigmaris_persona_core.types import TraitVector, RewardSignal


@dataclass
class GrowthLogEntry:
    """
    PersonaOS → MemoryDB.growth_log 用 1 レコード（完全版 v0.2）

    - 内面状態の変化（trait shift）
    - RewardSignal
    - モード（state）
    - 安全系フラグ（silence / contradiction / intuition_allow）
    - identity_hint / emotion / snapshot

    MemoryDB.store_growth_log(entry.to_row()) 用の dict を生成する。
    """

    # ベース情報
    user_id: str
    session_id: str
    last_message: str

    # Traits
    traits_before: TraitVector
    traits_after: TraitVector

    # RewardSignal（任意・None可）
    reward: Optional[RewardSignal] = None

    # state: "dialogue" / "reflect" / "introspect" / ...
    state: str = ""

    # silence / contradiction / intuition_allow など
    flags: Dict[str, bool] = field(default_factory=dict)

    # EmotionCore 互換
    emotion: Optional[str] = None
    intensity: Optional[float] = None

    # Identity Continuity 互換
    identity_hint: Optional[str] = None

    # 任意デバッグ
    extra_debug: Optional[Dict[str, Any]] = None

    # ============================================================
    # to_row() → MemoryDB.growth_log INSERT 用 dict に変換
    # ============================================================

    def to_row(self) -> Dict[str, Any]:
        ts = datetime.utcnow().isoformat()

        # --------------------------------------------------------
        # Trait shift
        # --------------------------------------------------------
        before = self.traits_before
        after = self.traits_after

        delta_calm = float(after.calm - before.calm)
        delta_empathy = float(after.empathy - before.empathy)
        delta_curiosity = float(after.curiosity - before.curiosity)

        value_shift = abs(delta_calm) + abs(delta_empathy) + abs(delta_curiosity)

        # --------------------------------------------------------
        # RewardSignal
        # --------------------------------------------------------
        reward_value = 0.0
        reward_reason = ""
        reward_meta = {}

        if isinstance(self.reward, RewardSignal):
            reward_value = float(self.reward.value)
            reward_reason = str(self.reward.reason or "")
            reward_meta = self.reward.meta or {}

        # reward_meta に value_shift が入っていれば優先
        if "value_shift" in reward_meta:
            try:
                value_shift = float(reward_meta["value_shift"])
            except Exception:
                pass

        # --------------------------------------------------------
        # emotion / intensity
        # --------------------------------------------------------
        emotion = self.emotion or ""
        intensity = (
            float(self.intensity)
            if self.intensity is not None
            else min(1.0, max(0.0, value_shift))
        )

        # --------------------------------------------------------
        # Flags
        # --------------------------------------------------------
        f = self.flags or {}
        silence = 1 if f.get("silence") else 0
        contradiction = 1 if f.get("contradiction") else 0
        intuition = 1 if (f.get("intuition_allow") or f.get("intuition")) else 0

        # --------------------------------------------------------
        # Identity Hint
        # --------------------------------------------------------
        identity_hint = self.identity_hint or ""

        # --------------------------------------------------------
        # Snapshot（UI・デバッグ）
        # --------------------------------------------------------
        snapshot_obj = {
            "ts": ts,
            "state": self.state,
            "user_id": self.user_id,
            "session_id": self.session_id,
            "last_message": self.last_message,
            "traits_before": {
                "calm": float(before.calm),
                "empathy": float(before.empathy),
                "curiosity": float(before.curiosity),
            },
            "traits_after": {
                "calm": float(after.calm),
                "empathy": float(after.empathy),
                "curiosity": float(after.curiosity),
            },
            "reward": {
                "value": reward_value,
                "reason": reward_reason,
                "meta": reward_meta,
            },
            "flags": f,
        }

        if self.extra_debug:
            snapshot_obj["extra_debug"] = self.extra_debug
        if identity_hint:
            snapshot_obj["identity_hint"] = identity_hint

        snapshot = json.dumps(snapshot_obj, ensure_ascii=False)

        # --------------------------------------------------------
        # MemoryDB.growth_log スキーマに完全対応
        # --------------------------------------------------------
        return {
            "ts": ts,
            "session_id": self.session_id,
            "delta_calm": delta_calm,
            "delta_empathy": delta_empathy,
            "delta_curiosity": delta_curiosity,
            "reward": reward_value,
            "reward_reason": reward_reason,
            "value_shift": value_shift,
            "emotion": emotion,
            "intensity": intensity,
            "silence": silence,
            "contradiction": contradiction,
            "intuition": intuition,
            "identity_hint": identity_hint,
            "snapshot": snapshot,
        }

=== FILE: sigmaris-core/persona_db/memory_db.py ===

from __future__ import annotations

import os
import json
import re
import sqlite3
from datetime import datetime
from typing import Optional, List, Dict, Any

from .growth_log import GrowthLogEntry


# ============================================================
#  DB ROOT
# ============================================================

DEFAULT_DB_ROOT = os.getenv(
    "SIGMARIS_PERSONA_DB_ROOT",
    os.path.dirname(__file__),
)


def _ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def _safe_user_id(user_id: str) -> str:
    """
    SQLite のファイル名として安全化。
    """
    if not user_id:
        return "system"
    safe: List[str] = []
    for ch in user_id:
        if ch.isalnum() or ch in ("-", "_"):
            safe.append(ch)
        else:
            safe.append("_")
    return "".join(safe)


def _simple_tokenize(text: str) -> List[str]:
    """
    v0.2 の超簡易トークナイザー。
    - 空白・句読点で分割
    - 記号のみ/1文字/極端に長いトークンは捨てる
    """
    if not text:
        return []

    rough_chunks = re.split(r"[ \t\r\n、。！？,.!?]+", text)

    tokens: List[str] = []
    for chunk in rough_chunks:
        chunk = chunk.strip()
        if not chunk:
            continue

        # 記号のみは捨てる
        if all((not c.isalnum()) and (c not in "_-") for c in chunk):
            continue

        # 1 文字はノイズと判断
        if len(chunk) < 2:
            continue

        # 長すぎるトークンもノイズ扱い
        if len(chunk) > 24:
            continue

        tokens.append(chunk)

    return tokens


# ============================================================
#   MemoryDB (PersonaOS: All Long-term Memory)
# ============================================================


class MemoryDB:
    """
    PersonaOS 完全版：長期記憶 DB

    - episodes：自然言語の会話ログ
    - concepts：概念クラスタ（トークン/話題）
    - identity_events：トレイト変化の履歴（Value Drift / Reward / Emotion）
    - growth_log：PersonaOS 内部の成長ログ（GrowthLogEntry）
    """

    def __init__(
        self,
        user_id: str = "system",
        db_path: Optional[str] = None,
    ) -> None:
        # user_id 単位でファイルを分ける（sigmaris_persona_core 側で管理）
        if db_path is not None:
            self.db_path = db_path
        else:
            _ensure_dir(DEFAULT_DB_ROOT)
            safe_id = _safe_user_id(user_id)
            self.db_path = os.path.join(DEFAULT_DB_ROOT, f"{safe_id}.sqlite3")

        # FastAPI マルチスレッド想定
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self._init_schema()

    # ============================================================
    # DB schema
    # ============================================================

    def _init_schema(self) -> None:
        cur = self.conn.cursor()

        # --- episodes（会話ログ） ---
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS episodes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ts TEXT NOT NULL,
                session_id TEXT,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                topic_hint TEXT,
                emotion_hint TEXT,
                importance REAL,
                meta_json TEXT
            );
            """
        )
        cur.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_episodes_session_ts
            ON episodes (session_id, ts);
            """
        )

        cur.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_episodes_content
            ON episodes (content);
            """
        )

        # --- identity events（トレイト変動） ---
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS identity_events (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ts TEXT NOT NULL,
                kind TEXT NOT NULL,
                delta_calm REAL,
                delta_empathy REAL,
                delta_curiosity REAL,
                reward REAL,
                meta_json TEXT
            );
            """
        )

        # --- concepts（概念クラスタ） ---
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS concepts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                label TEXT NOT NULL UNIQUE,
                score REAL,
                occurrences INTEGER,
                last_updated TEXT,
                meta_json TEXT
            );
            """
        )

        # --- growth_log（旧仕様互換＋PersonaOS用） ---
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS growth_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ts TEXT NOT NULL,
                session_id TEXT,
                delta_calm REAL,
                delta_empathy REAL,
                delta_curiosity REAL,
                reward REAL,
                reward_reason TEXT,
                value_shift REAL,
                emotion TEXT,
                intensity REAL,
                silence INTEGER,
                contradiction INTEGER,
                intuition INTEGER,
                identity_hint TEXT,
                snapshot TEXT
            );
            """
        )

        self.conn.commit()

    # ============================================================
    # Episode API
    # ============================================================

    def store_episode(
        self,
        *,
        session_id: Optional[str],
        role: str,
        content: str,
        topic_hint: Optional[str],
        emotion_hint: Optional[str],
        importance: float,
        meta: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        自然言語の 1 発話ログを保存し、同時に concept 更新。
        PersonaOS.process() から呼ばれる想定。
        """
        ts = datetime.utcnow().isoformat()
        meta_json = json.dumps(meta or {}, ensure_ascii=False)

        with self.conn:
            self.conn.execute(
                """
                INSERT INTO episodes (
                    ts, session_id, role, content,
                    topic_hint, emotion_hint, importance, meta_json
                )
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    ts,
                    session_id,
                    role,
                    content,
                    topic_hint,
                    emotion_hint,
                    float(importance),
                    meta_json,
                ),
            )

        # episode から concept を更新
        self._update_concepts_from_episode(
            content=content,
            topic_hint=topic_hint,
            importance=float(importance),
        )

    # ------------------------------------------------------------

    def _update_concepts_from_episode(
        self,
        *,
        content: str,
        topic_hint: Optional[str],
        importance: float,
    ) -> None:
        """
        v0.2 concept 更新（topic hint 優先 + トークン抽出）
        """

        # topic hint 優先
        if topic_hint:
            label = str(topic_hint).strip()
            if label:
                self.store_concept(
                    label=label,
                    score=float(importance),
                    occurrences=1,
                    meta={"source": "topic_hint"},
                )

        tokens = _simple_tokenize(content)
        if not tokens:
            return

        unique_tokens = list(dict.fromkeys(tokens))
        base_score = max(0.05, min(1.0, importance or 0.1))

        for tok in unique_tokens[:16]:
            lower = tok.lower()
            if lower in ("the", "this", "that", "it", "you", "and", "or", "but"):
                continue

            self.store_concept(
                label=tok,
                score=base_score,
                occurrences=1,
                meta={"source": "auto_from_episode"},
            )

    # ============================================================
    # Identity events API
    # ============================================================

    def store_identity_event(
        self,
        *,
        kind: str,
        delta_calm: float = 0.0,
        delta_empathy: float = 0.0,
        delta_curiosity: float = 0.0,
        reward: float = 0.0,
        meta: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        AEI からの Reward / Emotion / Value などを
        identity_events に 1 行として記録する。
        """
        ts = datetime.utcnow().isoformat()
        meta_json = json.dumps(meta or {}, ensure_ascii=False)

        with self.conn:
            self.conn.execute(
                """
                INSERT INTO identity_events (
                    ts, kind,
                    delta_calm, delta_empathy, delta_curiosity,
                    reward, meta_json
                )
                VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    ts,
                    kind,
                    float(delta_calm),
                    float(delta_empathy),
                    float(delta_curiosity),
                    float(reward),
                    meta_json,
                ),
            )

    # ------------------------------------------------------------

    def load_latest_traits(
        self,
        baseline: Optional[Dict[str, float]] = None,
    ) -> Dict[str, float]:
        """
        identity_events の delta_* を集計して
        「現在のトレイト」を baseline からの相対で再構成する。
        """
        base = {
            "calm": 0.5,
            "empathy": 0.5,
            "curiosity": 0.5,
        }

        if baseline:
            base.update(
                {
                    "calm": float(baseline.get("calm", base["calm"])),
                    "empathy": float(baseline.get("empathy", base["empathy"])),
                    "curiosity": float(baseline.get("curiosity", base["curiosity"])),
                }
            )

        cur = self.conn.cursor()
        cur.execute(
            """
            SELECT
                COALESCE(SUM(delta_calm), 0.0)       AS sum_calm,
                COALESCE(SUM(delta_empathy), 0.0)    AS sum_empathy,
                COALESCE(SUM(delta_curiosity), 0.0)  AS sum_curiosity
            FROM identity_events;
            """
        )
        row = cur.fetchone()

        if row is None:
            return base

        return {
            "calm": base["calm"] + float(row["sum_calm"] or 0.0),
            "empathy": base["empathy"] + float(row["sum_empathy"] or 0.0),
            "curiosity": base["curiosity"] + float(row["sum_curiosity"] or 0.0),
        }

    # ============================================================
    # Concepts API
    # ============================================================

    def store_concept(
        self,
        *,
        label: str,
        score: float,
        occurrences: int = 1,
        meta: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        ラベルごとに 1 レコードを upsert する。
        score は最新値で上書き、occurrences は加算。
        """
        label = label.strip()
        if not label:
            return

        ts = datetime.utcnow().isoformat()
        meta_json = json.dumps(meta or {}, ensure_ascii=False)

        with self.conn:
            self.conn.execute(
                """
                INSERT INTO concepts (
                    label, score, occurrences, last_updated, meta_json
                )
                VALUES (?, ?, ?, ?, ?)
                ON CONFLICT(label) DO UPDATE SET
                    score = excluded.score,
                    occurrences = concepts.occurrences + excluded.occurrences,
                    last_updated = excluded.last_updated,
                    meta_json = excluded.meta_json
                """,
                (
                    label,
                    float(score),
                    int(occurrences),
                    ts,
                    meta_json,
                ),
            )

    # ------------------------------------------------------------

    def get_concept_map(
        self,
        *,
        min_score: float = 0.0,
        limit: int = 64,
    ) -> Dict[str, Any]:
        """
        PersonaOS / UI 用の概念マップビュー。
        """
        cur = self.conn.cursor()
        cur.execute(
            """
            SELECT
                label,
                score,
                occurrences,
                last_updated,
                meta_json
            FROM concepts
            WHERE score >= ?
            ORDER BY score DESC, occurrences DESC, last_updated DESC
            LIMIT ?
            """,
            (float(min_score), int(limit)),
        )
        rows = cur.fetchall()

        nodes: List[Dict[str, Any]] = []

        for r in rows:
            try:
                meta = json.loads(r["meta_json"]) if r["meta_json"] else {}
            except Exception:
                meta = {"_parse_error": True}

            nodes.append(
                {
                    "label": r["label"],
                    "score": float(r["score"] or 0.0),
                    "occurrences": int(r["occurrences"] or 0),
                    "last_updated": r["last_updated"],
                    "meta": meta,
                }
            )

        return {
            "nodes": nodes,
            "stats": {
                "count": len(nodes),
                "min_score": min_score,
            },
            "edges": [],  # v0.2 では未使用（将来の共起クラスタ用）
        }

    # ============================================================
    # Growth log API（PersonaOS 用）
    # ============================================================

    def store_growth_log(self, entry: GrowthLogEntry) -> None:
        """
        PersonaOS 内部の GrowthLogEntry を 1 件保存。
        """
        row = entry.to_row()
        with self.conn:
            self.conn.execute(
                """
                INSERT INTO growth_log (
                    ts,
                    session_id,
                    delta_calm,
                    delta_empathy,
                    delta_curiosity,
                    reward,
                    reward_reason,
                    value_shift,
                    emotion,
                    intensity,
                    silence,
                    contradiction,
                    intuition,
                    identity_hint,
                    snapshot
                )
                VALUES (
                    :ts,
                    :session_id,
                    :delta_calm,
                    :delta_empathy,
                    :delta_curiosity,
                    :reward,
                    :reward_reason,
                    :value_shift,
                    :emotion,
                    :intensity,
                    :silence,
                    :contradiction,
                    :intuition,
                    :identity_hint,
                    :snapshot
                )
                """,
                row,
            )

    def get_recent_growth_logs(self, limit: int = 50) -> List[Dict[str, Any]]:
        """
        直近の growth_log を取得（デバッグ・可視化用）。
        """
        cur = self.conn.cursor()
        cur.execute(
            """
            SELECT
                id,
                ts,
                session_id,
                delta_calm,
                delta_empathy,
                delta_curiosity,
                reward,
                reward_reason,
                value_shift,
                emotion,
                intensity,
                silence,
                contradiction,
                intuition,
                identity_hint,
                snapshot
            FROM growth_log
            ORDER BY id DESC
            LIMIT ?
            """,
            (int(limit),),
        )
        rows = cur.fetchall()
        return [dict(r) for r in rows]

    # ============================================================
    # Identity Continuity API
    # ============================================================

    def get_recent_episodes(
        self,
        limit: int = 20,
    ) -> List[Dict[str, Any]]:
        """
        最新の会話ログを取得。
        PersonaOS 側が「最近の文脈」を作るために使用。
        """
        cur = self.conn.cursor()
        cur.execute(
            """
            SELECT
                id, ts, session_id, role, content,
                topic_hint, emotion_hint, importance, meta_json
            FROM episodes
            ORDER BY id DESC
            LIMIT ?
            """,
            (int(limit),),
        )
        rows = cur.fetchall()
        return [dict(r) for r in rows]

    # ------------------------------------------------------------

    def search_episodes_by_keyword(
        self,
        keyword: str,
        limit: int = 20,
    ) -> List[Dict[str, Any]]:
        """
        content / topic_hint / meta_json を横断検索。
        PersonaOS が「この前の〇〇の件」のような曖昧参照に利用。
        """
        kw = f"%{keyword}%"
        cur = self.conn.cursor()
        cur.execute(
            """
            SELECT
                id, ts, session_id, role, content,
                topic_hint, emotion_hint, importance, meta_json
            FROM episodes
            WHERE content LIKE ?
               OR topic_hint LIKE ?
               OR meta_json LIKE ?
            ORDER BY id DESC
            LIMIT ?
            """,
            (kw, kw, kw, int(limit)),
        )
        rows = cur.fetchall()
        return [dict(r) for r in rows]

    # ------------------------------------------------------------

    def get_related_episodes(
        self,
        user_input: str,
        limit: int = 10,
    ) -> Dict[str, Any]:
        """
        Identity Continuity の中心機能。
        - 入力文を tokenize
        - keyword ごとに検索
        - importance / recency でスコア計算して並べ替え

        PersonaOS 側はこれだけ呼べば、関連発話がまとまって受け取れる。
        """
        tokens = _simple_tokenize(user_input)
        if not tokens:
            return {"related": [], "tokens": []}

        results: List[Dict[str, Any]] = []

        for tok in tokens[:6]:  # keyword は最大6個
            hits = self.search_episodes_by_keyword(tok, limit=limit)
            for h in hits:
                h["_tok"] = tok
                results.append(h)

        # 重複排除（id ベース）
        uniq: Dict[int, Dict[str, Any]] = {}
        for r in results:
            eid = r["id"]
            if eid not in uniq:
                uniq[eid] = r

        # importance と recency でスコア（簡易）
        merged: List[Dict[str, Any]] = list(uniq.values())
        for r in merged:
            importance = float(r.get("importance") or 0.1)
            # recency boost: id が新しいほど少しだけ加点
            r["_score"] = importance + (float(r["id"]) / 1_000_000.0)

        merged.sort(key=lambda x: x["_score"], reverse=True)

        return {
            "tokens": tokens,
            "related": merged[:limit],
        }

    # ============================================================
    # Utility
    # ============================================================

    def close(self) -> None:
        try:
            self.conn.close()
        except Exception:
            pass


# ------------------------------------------------------------
# 互換性維持用のシングルトン（旧コード向け）
# ------------------------------------------------------------

db = MemoryDB(user_id="system")

=== FILE: sigmaris-core/server.py ===

# server.py
from __future__ import annotations

import os
import json
from dataclasses import asdict
from typing import Optional, Dict, Any

from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# -----------------------------------------
# .env 読み込み
# -----------------------------------------
load_dotenv()

# -----------------------------------------
# AEI Core
# -----------------------------------------
from aei.identity import IdentityCore
from aei.episodic_memory import EpisodeStore
from aei.adapter import LLMAdapter
from aei.reflection import ReflectionCore
from aei.introspection import IntrospectionCore
from aei.psychology.longterm import LongTermPsychology
from aei.psychology.meta_reflection import MetaReflectionCore
from aei.reward import RewardCore
from aei.emotion.emotion_core import EmotionCore
from aei.value.value_core import ValueCore

# -----------------------------------------
# Persona OS (完全版)
# -----------------------------------------
from sigmaris_persona_core.persona_os import PersonaOS
from sigmaris_persona_core.config import PersonaOSConfig
from sigmaris_persona_core.types import (
    PersonaContext,
    Message,
)

# -----------------------------------------
# Persona-DB
# -----------------------------------------
from persona_db.memory_db import MemoryDB


# ============================================================
# AEI 初期化
# ============================================================

identity = IdentityCore()
episodes = EpisodeStore()

persona_os = PersonaOS(PersonaOSConfig())

OPENAI_KEY = os.getenv("OPENAI_API_KEY")
USE_REAL_API = OPENAI_KEY not in (None, "", "0", "false", "False")


# ============================================================
# LLM Adapter
# ============================================================

def make_llm_adapter(dummy_json: str) -> LLMAdapter:
    """実API or ダミー LLMAdapter を返す。"""
    if USE_REAL_API:
        return LLMAdapter(api_key=OPENAI_KEY)
    return LLMAdapter(test_mode=True, dummy_fn=lambda _prompt: dummy_json)


# Reflection
llm_reflect = make_llm_adapter("""{
  "summary": "dummy summary",
  "emotion_hint": "neutral",
  "traits_hint": { "calm": 0.7, "empathy": 0.7, "curiosity": 0.7 }
}""")

# Introspection
llm_intro = make_llm_adapter("""{
  "mid_term_summary": "dummy mid summary",
  "pattern": "neutral",
  "trait_adjustment": { "calm": 0.0, "empathy": 0.0, "curiosity": 0.0 },
  "risk": { "drift_warning": false, "dependency_warning": false }
}""")

# Meta Reflection
llm_meta = make_llm_adapter("""{
  "meta_summary": "dummy meta summary",
  "root_cause": "none",
  "adjustment": { "calm": 0.0, "empathy": 0.0, "curiosity": 0.0 },
  "risk": {
    "identity_drift_risk": false,
    "emotional_collapse_risk": false,
    "over_dependency_risk": false
  }
}""")

# Reward
llm_reward = make_llm_adapter("""{
  "global_reward": 0.25,
  "trait_reward": { "calm": 0.02, "empathy": 0.03, "curiosity": 0.04 },
  "reason": "dummy reward"
}""")

# Emotion
llm_emotion = make_llm_adapter("""{
  "emotion": "calm-focus",
  "intensity": 0.4,
  "reason": "dummy emotion",
  "trait_shift": { "calm": 0.01, "empathy": 0.00, "curiosity": 0.02 },
  "meta": { "energy": 0.3, "stability": 0.8, "valence": 0.1 }
}""")

# Value Core
llm_value = make_llm_adapter("""{
  "importance": ["clarity", "self-consistency", "curiosity-growth"],
  "weight": 0.82,
  "tension": 0.14,
  "baseline_shift": {
    "calm": 0.01,
    "empathy": -0.01,
    "curiosity": 0.02
  }
}""")


# ============================================================
# FastAPI
# ============================================================

app = FastAPI(title="Sigmaris AEI Core API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/")
def root():
    """簡易ヘルスチェック用。ブラウザ直アクセス用。"""
    return {"status": "ok", "service": "sigmaris-aei-core"}


# ============================================================
# AEI Core Modules
# ============================================================

reflection = ReflectionCore(identity, episodes, llm_reflect.as_function())
introspection = IntrospectionCore(identity, episodes, llm_intro.as_function())
longterm = LongTermPsychology(identity, episodes)
metaref = MetaReflectionCore(identity, episodes, llm_meta.as_function())
reward_core = RewardCore(identity, episodes, llm_reward.as_function())
emotion_core = EmotionCore(identity, episodes, llm_emotion.as_function())
value_core = ValueCore(identity, episodes, llm_value.as_function())

last_reward_state: Optional[Dict[str, Any]] = None


# ============================================================
# PersonaOS Bridge（AEI → PersonaOS）
# ============================================================

def bridge_reflection(user_text: str, summary: dict) -> None:
    """ReflectionCore の結果を PersonaOS へ渡す。"""
    msg = Message(role="meta", content=user_text)
    ctx = PersonaContext(user_id="system", session_id="reflection")
    persona_os.feed_reflection(msg, summary, ctx)


def bridge_reward(res: dict) -> None:
    persona_os.feed_reward(res)


def bridge_emotion(res: dict) -> None:
    persona_os.feed_emotion(res)


def bridge_value(res: dict) -> None:
    persona_os.feed_value(res)


# ============================================================
# Pydantic Models
# ============================================================

class LogInput(BaseModel):
    text: str
    episode_id: Optional[str] = None


class SyncInput(BaseModel):
    chat: Dict[str, Any]
    context: Dict[str, Any]


class PersonaDecisionInput(BaseModel):
    user: str
    context: Dict[str, Any]
    session_id: str
    user_id: str


# ============================================================
# AEI API
# ============================================================

@app.post("/reflect")
def api_reflect(inp: LogInput):
    ep = reflection.reflect(inp.text, episode_id=inp.episode_id)
    bridge_reflection(inp.text, ep.summary_dict())
    return {"episode": ep.as_dict(), "identity": identity.export_state()}


@app.post("/introspect")
def api_introspect():
    res = introspection.introspect()
    return {"introspection": res, "identity": identity.export_state()}


@app.post("/longterm")
def api_longterm():
    res = longterm.analyze()
    return {"longterm": res, "identity": identity.export_state()}


@app.post("/meta")
def api_meta():
    res = metaref.meta_reflect()
    return {"meta": res, "identity": identity.export_state()}


@app.post("/reward")
def api_reward():
    global last_reward_state
    res = reward_core.evaluate()
    last_reward_state = res
    bridge_reward(res)
    return {"reward": res, "identity": identity.export_state()}


@app.get("/reward/state")
def api_reward_state():
    return {"reward": last_reward_state, "identity": identity.export_state()}


@app.post("/emotion")
def api_emotion(inp: LogInput):
    res = emotion_core.analyze(inp.text)
    bridge_emotion(res)
    return {"emotion": res, "identity": identity.export_state()}


@app.post("/value")
def api_value():
    res = value_core.analyze()
    bridge_value(res)
    return {"value": res, "identity": identity.export_state()}


@app.get("/value/state")
def api_value_state():
    return value_core.export_state()


@app.get("/identity")
def api_identity():
    return identity.export_state()


@app.get("/memory")
def api_memory():
    eps = episodes.load_all()
    return {
        "episodes": [ep.as_dict() for ep in eps],
        "count": len(eps),
    }


# ============================================================
# Identity Sync（Next.js → AEI）
# ============================================================

@app.post("/sync")
def api_sync(data: SyncInput):
    user_text = data.chat.get("user", "")
    ai_text = data.chat.get("ai", "")

    if user_text:
        reflection.reflect(user_text)
    if ai_text:
        reflection.reflect(f"[AI_OUTPUT] {ai_text}")

    ctx_traits = data.context.get("traits", {})

    identity.update_traits(
        calm=ctx_traits.get("calm", identity.current.calm),
        empathy=ctx_traits.get("empathy", identity.current.empathy),
        curiosity=ctx_traits.get("curiosity", identity.current.curiosity),
    )

    return {
        "status": "synced",
        "identity": identity.export_state(),
        "episode_count": len(episodes.load_all()),
    }


# ============================================================
# PersonaOS Decision API
# ============================================================

@app.post("/persona/decision")
def api_persona_decision(data: PersonaDecisionInput):
    msg = Message(role="user", content=data.user)

    ctx = PersonaContext(
        user_id=data.user_id,
        session_id=data.session_id,
        extra=data.context,
    )

    # PersonaOS 側の例外はここで握って 500 を避ける
    try:
        decision = persona_os.process(
            incoming=msg,
            context=ctx,
        )
        decision_dict = asdict(decision)
    except Exception as e:
        # デバッグしやすいように最低限の情報だけ返す
        return {
            "error": "persona_os_process_failed",
            "detail": str(e),
        }

    return {
        "decision": decision_dict,
        "identity": identity.export_state(),
    }


# ============================================================
# PersonaDB TEST API
# ============================================================

@app.get("/persona_db/growth_logs")
def api_persona_db_growth_logs(user_id: str = "system", limit: int = 20):
    db = MemoryDB(user_id=user_id)
    logs = db.get_recent_growth_logs(limit=limit)
    return {"user_id": user_id, "count": len(logs), "logs": logs}


@app.get("/db/identity")
def api_db_identity(user_id: str = "system"):
    db = MemoryDB(user_id=user_id)
    traits = db.load_latest_traits()
    return {"user_id": user_id, "traits": traits}


@app.get("/db/concepts")
def api_db_concepts(user_id: str = "system", min_score: float = 0.0, limit: int = 64):
    db = MemoryDB(user_id=user_id)
    res = db.get_concept_map(min_score=min_score, limit=limit)
    return {"user_id": user_id, "concepts": res}


@app.get("/db/episodes")
def api_db_episodes(user_id: str = "system", limit: int = 50):
    db = MemoryDB(user_id=user_id)
    conn = db.conn
    cur = conn.cursor()

    cur.execute(
        """
        SELECT
            id, ts, session_id, role, content,
            topic_hint, emotion_hint, importance, meta_json
        FROM episodes
        ORDER BY id DESC
        LIMIT ?
        """,
        (int(limit),),
    )

    rows = cur.fetchall()
    episodes_list = []

    for r in rows:
        try:
            meta = json.loads(r["meta_json"]) if r["meta_json"] else {}
        except Exception:
            meta = {"_parse_error": True}

        episodes_list.append(
            {
                "id": r["id"],
                "ts": r["ts"],
                "session_id": r["session_id"],
                "role": r["role"],
                "content": r["content"],
                "topic_hint": r["topic_hint"],
                "emotion_hint": r["emotion_hint"],
                "importance": r["importance"],
                "meta": meta,
            }
        )

    return {"user_id": user_id, "episodes": episodes_list, "count": len(episodes_list)}


@app.get("/db/growth")
def api_db_growth(user_id: str = "system", limit: int = 50):
    db = MemoryDB(user_id=user_id)
    logs = db.get_recent_growth_logs(limit=limit)
    return {"user_id": user_id, "count": len(logs), "logs": logs}

=== FILE: sigmaris-core/sigmaris_persona_core/__init__.py ===


=== FILE: sigmaris-core/sigmaris_persona_core/concepts/__init__.py ===

# sigmaris_persona_core/concepts/__init__.py
from .extractor import ConceptCandidate, LLMConceptExtractor
from .clusterer import ConceptClusterer

__all__ = [
    "ConceptCandidate",
    "LLMConceptExtractor",
    "ConceptClusterer",
]

=== FILE: sigmaris-core/sigmaris_persona_core/concepts/clusterer.py ===

# sigmaris_persona_core/concepts/clusterer.py
from __future__ import annotations

from typing import List, Dict, Any, Optional

from persona_db.memory_db import MemoryDB
from .extractor import LLMConceptExtractor, ConceptCandidate


class ConceptClusterer:
    """
    PersonaDB に蓄積された episodes などを元に、
    LLM を使って「概念クラスタ」を抽出し、concepts テーブルに反映する。

    v0.1 の役割:
      - 外部からテキスト群（episodes.content のリスト）を渡される
      - LLMConceptExtractor で概念候補を作る
      - MemoryDB.store_concept() で永続化する
    """

    def __init__(
        self,
        db: MemoryDB,
        extractor: LLMConceptExtractor,
        max_batch_chars: int = 4000,
    ) -> None:
        self.db = db
        self.extractor = extractor
        self.max_batch_chars = max_batch_chars

    # ============================================================
    # パブリック API
    # ============================================================

    def update_from_texts(
        self,
        texts: List[str],
        extra_meta: Optional[Dict[str, Any]] = None,
    ) -> int:
        """
        任意のテキスト群から概念クラスタを再抽出し、concepts テーブルを更新する。

        戻り値:
            実際に保存した Concept 数
        """
        normalized = self._truncate_batch(texts)
        if not normalized:
            return 0

        candidates: List[ConceptCandidate] = self.extractor.extract_from_texts(normalized)
        if not candidates:
            return 0

        meta_base = extra_meta or {}

        for c in candidates:
            meta = {
                "kind": c.kind,
                "evidence": c.evidence,
            }
            meta.update(meta_base)

            # persona_db.memory_db.MemoryDB.store_concept を利用
            self.db.store_concept(
                label=c.label,
                score=c.score,
                occurrences=1,
                meta=meta,
            )

        return len(candidates)

    # ------------------------------------------------------------
    # （オプション）最近の episodes から自動抽出するヘルパー
    # ------------------------------------------------------------

    def update_from_recent_episodes(
        self,
        limit: int = 80,
        extra_meta: Optional[Dict[str, Any]] = None,
    ) -> int:
        """
        persona-db の episodes テーブルから直近の content を取り出し、
        そこから概念抽出を行うヘルパー。

        MemoryDB は episodes を読むための専用メソッドは持っていないが、
        .conn を直接叩いている（内部実装に依存するので将来ラップ予定）。
        """
        cur = self.db.conn.cursor()  # type: ignore[attr-defined]

        cur.execute(
            """
            SELECT content
            FROM episodes
            ORDER BY id DESC
            LIMIT ?
            """,
            (limit,),
        )
        rows = cur.fetchall()
        texts = [str(r["content"]) for r in rows if r["content"]]

        return self.update_from_texts(texts, extra_meta=extra_meta)

    # ============================================================
    # 内部: バッチの長さ制御
    # ============================================================

    def _truncate_batch(self, texts: List[str]) -> List[str]:
        """
        LLM に渡す総文字数が極端に膨らまないように、
        おおざっぱに max_batch_chars までに切り詰める。
        """
        result: List[str] = []
        total = 0

        for t in texts:
            if not t:
                continue
            t_str = str(t)
            length = len(t_str)
            if total + length > self.max_batch_chars:
                # 溢れそうな場合は残りの枠だけ切り出して終了
                remain = self.max_batch_chars - total
                if remain <= 0:
                    break
                result.append(t_str[:remain])
                total += remain
                break

            result.append(t_str)
            total += length

        return result

=== FILE: sigmaris-core/sigmaris_persona_core/concepts/extractor.py ===

# sigmaris_persona_core/concepts/extractor.py
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import List, Dict, Any

from aei.adapter.llm_adapter import LLMFn


@dataclass
class ConceptCandidate:
    """
    LLM が返した「概念」1つ分の表現。
    - label:   人間可読なラベル（例: "構造思考", "自己内省"）
    - score:   0〜1 想定の強度（重要度）
    - kind:    "value" / "topic" / "trait" などの分類
    - evidence: 代表となるテキスト断片
    """
    label: str
    score: float
    kind: str
    evidence: str = ""

    def as_dict(self) -> Dict[str, Any]:
        return {
            "label": self.label,
            "score": self.score,
            "kind": self.kind,
            "evidence": self.evidence,
        }


class LLMConceptExtractor:
    """
    LLM を使って episodes などのテキスト群から
    「このユーザーが大事にしている概念・テーマ」を抽出するモジュール。

    - AEI 既存の LLMAdapter.as_function() と同じ LLMFn を受け取る設計。
    """

    def __init__(self, llm_fn: LLMFn) -> None:
        self.llm_fn = llm_fn

    # ============================================================
    # パブリック API
    # ============================================================

    def extract_from_texts(self, texts: List[str], max_concepts: int = 12) -> List[ConceptCandidate]:
        """
        複数テキスト（episodes など）から概念候補を抽出する。

        返り値:
            List[ConceptCandidate] （パースに失敗した場合は []）
        """
        if not texts:
            return []

        prompt = self._build_prompt(texts=texts, max_concepts=max_concepts)
        raw = self.llm_fn(prompt)

        try:
            data = json.loads(raw)
        except json.JSONDecodeError:
            # LLM 側で JSON を返さなかった場合は黙って空配列
            return []

        return self._parse_concepts(data)

    # ============================================================
    # 内部: プロンプト生成
    # ============================================================

    def _build_prompt(self, texts: List[str], max_concepts: int) -> str:
        """
        LLM に渡すプロンプトを生成。
        - すべてのテキストを 1 ブロックで渡し、「概念クラスタ」にまとめさせる。
        """
        joined = "\n---\n".join(texts[:50])  # 安全のため最大50件に制限（後で調整可）

        # System メッセージは LLMAdapter 側で固定されているので、
        # ここでは user プロンプトとして JSON仕様を明示する。
        prompt = f"""
You are an analyzer for a long-term AI personality OS.

I will give you multiple text segments from one user (dialogues, reflections, notes).
From these, extract the core "concepts" that represent what this user values, cares about, or repeatedly focuses on.

Return ONLY a JSON object with the following structure:

{{
  "concepts": [
    {{
      "label": "short human-readable concept label (in Japanese if original texts are Japanese)",
      "score": 0.0-1.0 number (importance / strength),
      "kind": "one of: value | topic | trait | pattern | other",
      "evidence": "short excerpt from the texts that supports this concept"
    }},
    ...
  ]
}}

Requirements:
- Do NOT add any keys other than "concepts".
- "concepts" MUST be an array (possibly empty).
- Use at most {max_concepts} concepts.
- Keep "label" relatively short (max ~20 characters).
- If texts are mostly Japanese, prefer Japanese labels.

Here are the user's texts:

{texts_separator()}
{joined}
{texts_separator()}
"""
        return prompt.strip()

    # ============================================================
    # 内部: パース
    # ============================================================

    def _parse_concepts(self, data: Dict[str, Any]) -> List[ConceptCandidate]:
        concepts_raw = data.get("concepts")
        if not isinstance(concepts_raw, list):
            return []

        results: List[ConceptCandidate] = []
        for item in concepts_raw:
            if not isinstance(item, dict):
                continue

            label = str(item.get("label", "")).strip()
            if not label:
                continue

            # score は [0,1] にクリップ
            try:
                score = float(item.get("score", 0.5))
            except (TypeError, ValueError):
                score = 0.5
            score = max(0.0, min(1.0, score))

            kind = str(item.get("kind", "other")).strip() or "other"
            evidence = str(item.get("evidence", "")).strip()

            results.append(
                ConceptCandidate(
                    label=label,
                    score=score,
                    kind=kind,
                    evidence=evidence,
                )
            )

        return results


def texts_separator() -> str:
    """プロンプト内でテキスト群の境界として使うだけのユーティリティ。"""
    return "\n==== USER_TEXTS ====\n"

=== FILE: sigmaris-core/sigmaris_persona_core/config.py ===

# sigmaris_persona_core/config.py
from __future__ import annotations
from dataclasses import dataclass, field

# ============================================================
# Silence / Contradiction / State Machine
# ============================================================


@dataclass
class SilenceConfig:
    """主体的沈黙の設定（完全版仕様）"""

    # 「抽象化が強すぎる」と沈黙候補とみなす閾値（0.0〜1.0）
    max_abstraction: float = 0.8
    # 同一トピック反復など「ループ疑い」の閾値（0.0〜1.0）
    max_loop_suspect: float = 0.7
    # ユーザーが明示的に「教えて」「どう思う？」と求めたとき、
    # 閾値超えでも沈黙せず軽めの返答を許可するか
    allow_when_user_insists: bool = True
    # 将来的に「沈黙を選ぶ総合スコア」のしきい値として利用予定
    silence_threshold: float = 0.6


@dataclass
class ContradictionConfig:
    """矛盾保持（Contradiction-Hold）の閾値設定"""

    # どれくらい矛盾を「内部に保持したまま」動作するか（0.0〜1.0）
    max_conflict_ratio: float = 0.65
    # ユーザー側が混乱しているときは矛盾を和らげる方向に動くか
    soften_when_user_confusion: bool = True


@dataclass
class StateMachineConfig:
    """
    PersonaOS 内部ステートマシン v0.3
    StateMachine クラスと 1:1 対応。
    """

    # 直近1分間に処理してよいメッセージ数の上限
    overload_limit_per_min: int = 20
    # reflect ステートのクールダウン秒数
    reflect_cooldown_sec: float = 30.0
    # introspect ステートのクールダウン秒数
    introspect_cooldown_sec: float = 60.0

    # 高次ステートの有効 / 無効フラグ（現状ロジックには未使用だが将来拡張用）
    allow_contradiction_hold: bool = True
    allow_silence_mode: bool = True


# ============================================================
# Value Drift / Intuition / Memory
# ============================================================


@dataclass
class ValueDriftConfig:
    """自律的価値変動（Value Drift）の設定"""

    # 1 ステップあたりの最大変動幅
    max_step: float = 0.02
    # 1 ステップあたりの最小変動幅
    min_step: float = 0.001
    # 中心値 0.5 に向かう減衰係数（0.0〜1.0）
    decay: float = 0.995


@dataclass
class IntuitionConfig:
    """疑似直観（Pseudo-Intuition）のトリガー条件"""

    # 直近コンテキストに必要なメッセージ数
    min_context_size: int = 5
    # 「流れ」を読むために必要な最低時間スパン（秒）
    min_time_span_sec: float = 60.0
    # 直観に基づく結論をどれくらい強く押すか（0〜1）
    # （v0.2 では未使用だが、将来 Emotion / Reward へのバイアスに利用予定）
    strength: float = 0.4


@dataclass
class MemoryConfig:
    """短期 / 中期 / 長期メモリの境界値"""

    # short メモリとして保持する時間窓（秒）
    short_window_sec: float = 15 * 60        # 15分
    # mid メモリとして保持する時間窓（秒）
    mid_window_sec: float = 48 * 3600        # 2日
    # 同一テーマが long_min_count 回以上出たら long-term 候補
    long_min_count: int = 5
    # 長期記憶クラスタリング時の類似度閾値（将来の persona_db 拡張用）
    cluster_threshold: float = 0.65


# ============================================================
# Emotion / Decoding / Sampling
# ============================================================


@dataclass
class EmotionConfig:
    """感情レイヤ（Emotion-Synthesis）"""

    base_temperature: float = 0.6
    min_temperature: float = 0.3
    max_temperature: float = 0.9

    # LLM decoding parameters（EmotionCore から任意に利用）
    base_top_p: float = 0.92
    emotion_bias: float = 0.15

    def clamp_temp(self, t: float) -> float:
        """temperature を [min_temperature, max_temperature] にクリップ"""
        if t < self.min_temperature:
            return self.min_temperature
        if t > self.max_temperature:
            return self.max_temperature
        return t

    def clamp_top_p(self, p: float) -> float:
        """top_p を [0.1, 1.0] にクリップ"""
        if p < 0.1:
            return 0.1
        if p > 1.0:
            return 1.0
        return p


# ============================================================
# Meta Reward
# ============================================================


@dataclass
class MetaRewardConfig:
    """MetaRewardEngine 用の設定"""

    # 評価対象とする時間窓（秒）
    window_sec: float = 5 * 60.0
    # 各スコアの重み（MetaRewardEngine 内部の raw 合成と対応）
    depth_weight: float = 0.4
    openness_weight: float = 0.3
    stability_weight: float = 0.3
    negativity_weight: float = -0.5


# ============================================================
# PersonaOS 全体設定
# ============================================================


@dataclass
class PersonaOSConfig:
    """
    完全版 PersonaOS のグローバル設定束ね。
    sigmaris_persona_core の各モジュールがこれを参照する。
    """

    silence: SilenceConfig = field(default_factory=SilenceConfig)
    contradiction: ContradictionConfig = field(default_factory=ContradictionConfig)
    value_drift: ValueDriftConfig = field(default_factory=ValueDriftConfig)
    intuition: IntuitionConfig = field(default_factory=IntuitionConfig)
    memory: MemoryConfig = field(default_factory=MemoryConfig)
    state: StateMachineConfig = field(default_factory=StateMachineConfig)
    emotion: EmotionConfig = field(default_factory=EmotionConfig)
    meta_reward: MetaRewardConfig = field(default_factory=MetaRewardConfig)

=== FILE: sigmaris-core/sigmaris_persona_core/identity.py ===

# sigmaris_persona_core/identity_continuity.py
from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional

import re
from datetime import datetime

from persona_db.memory_db import MemoryDB
from .types import Message, PersonaContext


@dataclass
class AnchorItem:
    """
    プロセス内で保持するアンカー情報。
    DB にも写すが、直近のヒントはここから即座に取る。
    """
    label: str
    session_id: Optional[str]
    created_at: datetime
    source: str = "auto"  # "auto" / "explicit" など将来拡張用


@dataclass
class IdentityContinuityEngine:
    """
    「前に話してたあの件だけど？」に対応するための Identity Continuity v2。

    - update(...) で新しい発話から Anchor を抽出
    - Anchor は
        - プロセス内: anchors リスト
        - PersonaDB: identity_events / concepts
      に両方保存される
    - get_hint() で「最後に登録されたアンカーのラベル」を返す
    """

    # 直近アンカー（メモリ上）
    anchors: List[AnchorItem] = field(default_factory=list)

    # どのような発話を「アンカー候補」とみなすかの閾値
    min_length_for_topic: int = 30  # 30文字以上なら「トピックらしい」とみなす
    max_label_length: int = 80     # ラベルとして DB に残す最大長

    def update(
        self,
        message: Message,
        context: PersonaContext,
        db: MemoryDB,
    ) -> None:
        """
        新しい発話を受け取り、必要ならアンカーとして登録する。
        - message: ユーザー or システムからの発話
        - context: user_id / session_id / client など
        - db: ユーザーごとの MemoryDB
        """
        label = self._extract_anchor_label(message.content)
        if not label:
            return

        item = AnchorItem(
            label=label,
            session_id=context.session_id,
            created_at=datetime.utcnow(),
            source="auto",
        )
        self.anchors.append(item)

        # --- PersonaDB への反映: identity_events に記録 ---
        db.store_identity_event(
            kind="anchor",
            delta_calm=0.0,
            delta_empathy=0.0,
            delta_curiosity=0.0,
            reward=0.0,
            meta={
                "label": label,
                "session_id": context.session_id,
                "client": context.client,
                "source": "identity_continuity",
            },
        )

        # --- PersonaDB への反映: concepts としてもアップサート ---
        db.store_concept(
            label=label,
            score=1.0,  # v0.1: 固定値。将来は頻度や重要度で重み付け
            occurrences=1,
            meta={
                "source": "identity_anchor",
                "last_session_id": context.session_id,
            },
        )

    # ------------------------------------------------------------------
    # 内部ロジック
    # ------------------------------------------------------------------

    def _extract_anchor_label(self, text: str) -> Optional[str]:
        """
        発話テキストから「アンカーとして使える短いラベル」を抽出する。

        シンプルなヒューリスティック：
        - 以下のいずれかを満たす場合にアンカー候補
          1) 「件」「続き」「前に」「前回」「その後」などのキーワードを含む
          2) 全体文字数が min_length_for_topic 以上（＝何かしら話題っぽい）

        ラベル生成：
        - 改行・余計なスペースを潰して 1 行に
        - max_label_length 文字でカット
        """
        raw = (text or "").strip()
        if not raw:
            return None

        # ① キーワードベース
        markers = [
            "件",
            "続き",
            "前に",
            "前回",
            "その後",
            "あのとき",
            "the last",
            "previous",
            "last time",
        ]
        has_marker = any(m in raw for m in markers)

        # ② 長さベース
        long_enough = len(raw) >= self.min_length_for_topic

        if not (has_marker or long_enough):
            # どちらも満たさない場合はアンカーにしない
            return None

        # 改行・連続空白を潰す
        normalized = re.sub(r"\s+", " ", raw)

        # 最大長でカット（末尾に…を付けるかどうかは任意）
        if len(normalized) > self.max_label_length:
            return normalized[: self.max_label_length].rstrip() + "…"
        return normalized

    # ------------------------------------------------------------------
    # パブリック API
    # ------------------------------------------------------------------

    def get_hint(self) -> Optional[str]:
        """
        PersonaOS から呼ばれ、
        「最後に登録されたアンカーの短い説明」を返す。

        例:
        - "前に話していた Sigmaris OS 設計の続き…"
        - "昨日の“AI誘発性精神病”の考察の件…"
        """
        if not self.anchors:
            return None
        return self.anchors[-1].label

    def debug_anchors(self, limit: int = 20) -> List[Dict[str, Any]]:
        """
        デバッグ用: 直近のアンカーを辞書形式で返す。
        /debug やノート用に使える。
        """
        items = self.anchors[-limit:]
        return [
            {
                "label": a.label,
                "session_id": a.session_id,
                "created_at": a.created_at.isoformat(),
                "source": a.source,
            }
            for a in items
        ]

=== FILE: sigmaris-core/sigmaris_persona_core/inner_loop.py ===

# sigmaris_persona_core/inner_loop.py
from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Literal

from .types import (
    PersonaDecision,
    PersonaContext,
    TraitVector,
    RewardSignal,
)

# ============================================================
# 型定義
# ============================================================

InnerActionType = Literal[
    "none",
    "request_reflection",
    "request_introspection",
    "request_meta_reflection",
    "request_reward_update",
    "request_emotion_update",
    "request_value_update",
    "memory_snapshot",
]


@dataclass
class InnerAction:
    """
    InnerLoopEngine が外側（AEI Core / UI）に伝える「次にやるべき内面処理」。

    - type: 何をしたいか
    - reason: なぜそれを提案しているか（簡潔な文字列）
    - meta: 閾値や内部状態など、補助情報
    """

    type: InnerActionType
    reason: str
    meta: Dict[str, Any] = field(default_factory=dict)


@dataclass
class InnerLoopConfig:
    """
    PersonaOS の「自己循環ループ」のパラメータ。

    - reflection_turn_interval:
        何ターンごとに reflection を候補にするか（深めの対話前提）
    - introspection_turn_interval:
        何ターンごとに introspection を候補にするか
    - meta_reflection_turn_interval:
        何ターンごとに meta reflection を候補にするか
    - reward_interval:
        何ターンごとに RewardCore を叩くか
    - emotion_interval:
        何ターンごとに EmotionCore を更新するか
    - value_interval:
        何ターンごとに ValueCore を更新するか
    - min_reward_samples:
        meta 報酬が安定するまでに必要な最小ターン数
    """

    reflection_turn_interval: int = 6
    introspection_turn_interval: int = 12
    meta_reflection_turn_interval: int = 24

    reward_interval: int = 5
    emotion_interval: int = 3
    value_interval: int = 10

    min_reward_samples: int = 3


@dataclass
class InnerLoopState:
    """
    InnerLoopEngine 内部で持つ状態。

    - turn: この Persona インスタンスでの累計ターン数
    - last_*_turn: 各種処理を最後に行ったターン
    - cumulative_reward: meta reward の累積（簡易統計用）
    - reward_samples: meta reward を何回観測したか
    """

    turn: int = 0

    last_reflection_turn: int = 0
    last_introspection_turn: int = 0
    last_meta_reflection_turn: int = 0

    last_reward_turn: int = 0
    last_emotion_turn: int = 0
    last_value_turn: int = 0

    cumulative_reward: float = 0.0
    reward_samples: int = 0

    last_traits: TraitVector = field(
        default_factory=lambda: TraitVector(calm=0.5, empathy=0.5, curiosity=0.5)
    )


# ============================================================
# InnerLoopEngine 本体
# ============================================================


@dataclass
class InnerLoopEngine:
    """
    InnerLoopEngine
    ----------------

    PersonaOS が 1 ステップ応答したあと、
    「次にどの内面処理（Reflection / Reward / Value / Emotion 等）を走らせるべきか」
    を決める OS 内部のループ・コントローラ。

    ここでは実際の LLM コールや DB アクセスは行わず、
    あくまで「やるべきことのリスト (List[InnerAction])」を返す。
    AEI Core / UI 層がそれを見て、必要なエンドポイントを叩く想定。
    """

    config: InnerLoopConfig = field(default_factory=InnerLoopConfig)
    state: InnerLoopState = field(default_factory=InnerLoopState)

    # ========================================================
    # メインエントリ
    # ========================================================

    def step(
        self,
        *,
        decision: PersonaDecision,
        context: PersonaContext,
        traits: TraitVector,
        reward: Optional[RewardSignal],
    ) -> List[InnerAction]:
        """
        PersonaOS.process() 呼び出し後に 1 回叩かれることを想定。

        - decision: 今回の PersonaDecision
        - context: ユーザ / セッションコンテキスト
        - traits: 現在のトレイト値
        - reward: MetaRewardEngine からの報酬（なければ None）
        """

        self.state.turn += 1
        t = self.state.turn
        actions: List[InnerAction] = []

        # ---- Reward 統計更新 ----
        if reward is not None:
            self._update_reward_stat(reward)

        # ---- 1. PersonaDecision による優先行動 ----
        primary = self._plan_from_decision(decision)
        if primary.type != "none":
            actions.append(primary)
            self._update_last_turn(primary.type, t)

        # ---- 2. 周期的な Reward / Emotion / Value 更新 ----
        periodic = self._plan_periodic_updates(t)
        for act in periodic:
            actions.append(act)
            self._update_last_turn(act.type, t)

        # ---- 3. 長期的なメタ反省（meta_reflection）候補 ----
        meta_act = self._plan_meta_reflection(decision, traits)
        if meta_act is not None and meta_act.type != "none":
            actions.append(meta_act)
            self._update_last_turn(meta_act.type, t)

        # ---- 4. 内部状態のスナップショット（UI 可視化用の候補） ----
        snapshot_act = self._maybe_snapshot(context, traits)
        if snapshot_act is not None:
            actions.append(snapshot_act)

        # ---- 最後に traits を記録 ----
        self.state.last_traits = traits

        return actions

    # ========================================================
    # 個別ロジック
    # ========================================================

    def _update_reward_stat(self, reward: RewardSignal) -> None:
        self.state.cumulative_reward += float(reward.value)
        self.state.reward_samples += 1

    # --- 1. PersonaDecision ベースの行動計画 ---

    def _plan_from_decision(self, decision: PersonaDecision) -> InnerAction:
        """
        PersonaOS が「reflect した方がいい / introspect した方がいい」と
        判定した場合、それを最優先で尊重する。
        """
        if decision.need_reflection:
            return InnerAction(
                type="request_reflection",
                reason="persona_decision_need_reflection",
                meta={"preferred_state": decision.preferred_state},
            )

        if decision.need_introspection:
            return InnerAction(
                type="request_introspection",
                reason="persona_decision_need_introspection",
                meta={"preferred_state": decision.preferred_state},
            )

        # それ以外は特に「即時の優先リクエスト」はない
        return InnerAction(type="none", reason="no_primary_request", meta={})

    # --- 2. 周期的更新ロジック ---

    def _plan_periodic_updates(self, turn: int) -> List[InnerAction]:
        cfg = self.config
        st = self.state
        actions: List[InnerAction] = []

        # Reward 更新
        if (
            turn - st.last_reward_turn >= cfg.reward_interval
            and st.reward_samples >= cfg.min_reward_samples
        ):
            actions.append(
                InnerAction(
                    type="request_reward_update",
                    reason="periodic_reward_update",
                    meta={"turn": turn, "since_last": turn - st.last_reward_turn},
                )
            )

        # Emotion 更新
        if turn - st.last_emotion_turn >= cfg.emotion_interval:
            actions.append(
                InnerAction(
                    type="request_emotion_update",
                    reason="periodic_emotion_update",
                    meta={"turn": turn, "since_last": turn - st.last_emotion_turn},
                )
            )

        # Value 更新
        if turn - st.last_value_turn >= cfg.value_interval:
            actions.append(
                InnerAction(
                    type="request_value_update",
                    reason="periodic_value_update",
                    meta={"turn": turn, "since_last": turn - st.last_value_turn},
                )
            )

        return actions

    # --- 3. 長期メタ反省 ---

    def _plan_meta_reflection(
        self,
        decision: PersonaDecision,
        traits: TraitVector,
    ) -> Optional[InnerAction]:
        """
        - 特定のターン間隔で meta_reflection を候補にする
        - reward がマイナスに偏り続けている場合などもトリガー候補
        """
        cfg = self.config
        st = self.state
        turn = st.turn

        # ターンベースでの周期
        if turn - st.last_meta_reflection_turn >= cfg.meta_reflection_turn_interval:
            return InnerAction(
                type="request_meta_reflection",
                reason="periodic_meta_reflection",
                meta={
                    "turn": turn,
                    "since_last": turn - st.last_meta_reflection_turn,
                    "avg_reward": self._avg_reward(),
                    "traits": {
                        "calm": traits.calm,
                        "empathy": traits.empathy,
                        "curiosity": traits.curiosity,
                    },
                },
            )

        # 平均報酬が一定以下に落ち込んでいる場合は早めにトリガー
        avg_reward = self._avg_reward()
        if st.reward_samples >= cfg.min_reward_samples and avg_reward < -0.15:
            return InnerAction(
                type="request_meta_reflection",
                reason="negative_reward_trend",
                meta={
                    "turn": turn,
                    "avg_reward": avg_reward,
                    "reward_samples": st.reward_samples,
                },
            )

        return None

    def _avg_reward(self) -> float:
        st = self.state
        if st.reward_samples == 0:
            return 0.0
        return st.cumulative_reward / float(st.reward_samples)

    # --- 4. UI 可視化用のスナップショット候補 ---

    def _maybe_snapshot(
        self,
        context: PersonaContext,
        traits: TraitVector,
    ) -> Optional[InnerAction]:
        """
        今はかなり控えめに、
        - 10ターンに1度
        - もしくは reward_samples がちょうど min_reward_samples に達したとき
        に、UI 側での可視化用 snapshot を候補として返す。
        """
        st = self.state
        turn = st.turn
        cfg = self.config

        if turn % 10 == 0:
            return InnerAction(
                type="memory_snapshot",
                reason="periodic_snapshot",
                meta={
                    "turn": turn,
                    "user_id": context.user_id,
                    "session_id": context.session_id,
                    "traits": {
                        "calm": traits.calm,
                        "empathy": traits.empathy,
                        "curiosity": traits.curiosity,
                    },
                    "avg_reward": self._avg_reward(),
                    "reward_samples": st.reward_samples,
                },
            )

        if st.reward_samples == cfg.min_reward_samples:
            return InnerAction(
                type="memory_snapshot",
                reason="reward_samples_reached_min",
                meta={
                    "turn": turn,
                    "user_id": context.user_id,
                    "session_id": context.session_id,
                    "avg_reward": self._avg_reward(),
                    "reward_samples": st.reward_samples,
                },
            )

        return None

    # --- 5. last_*_turn の更新 ---

    def _update_last_turn(self, action_type: InnerActionType, turn: int) -> None:
        st = self.state

        if action_type == "request_reflection":
            st.last_reflection_turn = turn
        elif action_type == "request_introspection":
            st.last_introspection_turn = turn
        elif action_type == "request_meta_reflection":
            st.last_meta_reflection_turn = turn
        elif action_type == "request_reward_update":
            st.last_reward_turn = turn
        elif action_type == "request_emotion_update":
            st.last_emotion_turn = turn
        elif action_type == "request_value_update":
            st.last_value_turn = turn
        # memory_snapshot / none は last_* を更新しない

=== FILE: sigmaris-core/sigmaris_persona_core/memory/db.py ===

# sigmaris_persona_core/memory/db.py
from __future__ import annotations

import json
import os
import sqlite3
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple


def _utc_now_iso() -> str:
    """UTC 現在時刻を ISO8601 文字列で返す."""
    return datetime.now(timezone.utc).isoformat()


def _ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


@dataclass
class EpisodeRecord:
    id: int
    user_id: str
    session_id: str
    ts: str
    role: str
    content: str
    topic_hint: Optional[str]
    emotion_hint: Optional[str]
    importance: float
    meta: Dict[str, Any]


@dataclass
class ConceptRecord:
    id: int
    user_id: str
    label: str
    score: float
    occurrences: int
    last_seen: str
    meta: Dict[str, Any]


@dataclass
class IdentityEventRecord:
    id: int
    user_id: str
    kind: str
    delta_calm: float
    delta_empathy: float
    delta_curiosity: float
    reward: float
    ts: str
    meta: Dict[str, Any]


class MemoryDB:
    """
    Persona-DB v0.1

    ・ユーザーごとに SQLite DB ファイルを分離
    ・3レイヤ記憶：
        - episodes: 短〜中期の対話ログ
        - concepts: テーマ/概念のクラスタ（タグ的なもの）
        - identity_events: 価値・トレイト変動などの重要イベント
    """

    def __init__(
        self,
        user_id: str,
        db_root: Optional[str] = None,
    ) -> None:
        """
        :param user_id: この DB インスタンスが扱うユーザー ID
        :param db_root: DB を格納するルートディレクトリ
                        未指定の場合は "persona_db" をプロジェクト直下に作成
        """
        self.user_id = user_id

        # ルートディレクトリ決定
        if db_root is None:
            # 環境変数優先（あれば）
            env_root = os.getenv("PERSONA_DB_DIR")
            if env_root:
                db_root = env_root
            else:
                # このファイルから 2階層上をプロジェクトルートとみなす
                base_dir = os.path.abspath(
                    os.path.join(os.path.dirname(__file__), "..", "..")
                )
                db_root = os.path.join(base_dir, "persona_db")

        _ensure_dir(db_root)

        db_path = os.path.join(db_root, f"{self.user_id}.db")
        self._db_path = db_path

        # check_same_thread=False で FastAPI からの利用に耐えられるようにする
        self._conn = sqlite3.connect(self._db_path, check_same_thread=False)
        self._conn.row_factory = sqlite3.Row

        self._init_schema()

    # ------------------------------------------------------------------
    # スキーマ初期化
    # ------------------------------------------------------------------
    def _init_schema(self) -> None:
        cur = self._conn.cursor()

        # episodes: 対話ログ / 記憶エピソード
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS episodes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT NOT NULL,
                session_id TEXT,
                ts TEXT NOT NULL,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                topic_hint TEXT,
                emotion_hint TEXT,
                importance REAL NOT NULL DEFAULT 0.0,
                meta TEXT
            );
            """
        )

        # concepts: 概念・テーマクラスタ
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS concepts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT NOT NULL,
                label TEXT NOT NULL,
                score REAL NOT NULL DEFAULT 0.0,
                occurrences INTEGER NOT NULL DEFAULT 1,
                last_seen TEXT NOT NULL,
                meta TEXT,
                UNIQUE (user_id, label)
            );
            """
        )

        # identity_events: トレイト変動や価値の重要イベント
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS identity_events (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT NOT NULL,
                kind TEXT NOT NULL,
                delta_calm REAL NOT NULL DEFAULT 0.0,
                delta_empathy REAL NOT NULL DEFAULT 0.0,
                delta_curiosity REAL NOT NULL DEFAULT 0.0,
                reward REAL NOT NULL DEFAULT 0.0,
                ts TEXT NOT NULL,
                meta TEXT
            );
            """
        )

        self._conn.commit()

    # ------------------------------------------------------------------
    # 内部ユーティリティ
    # ------------------------------------------------------------------
    def _dump_meta(self, meta: Optional[Dict[str, Any]]) -> Optional[str]:
        if meta is None:
            return None
        try:
            return json.dumps(meta, ensure_ascii=False)
        except Exception:
            # 破損しないよう保険で文字列化
            return json.dumps({"_raw": str(meta)}, ensure_ascii=False)

    def _load_meta(self, raw: Optional[str]) -> Dict[str, Any]:
        if raw in (None, ""):
            return {}
        try:
            return json.loads(raw)
        except Exception:
            return {"_raw": raw}

    # ------------------------------------------------------------------
    # episodes
    # ------------------------------------------------------------------
    def store_episode(
        self,
        session_id: str,
        role: str,
        content: str,
        topic_hint: Optional[str] = None,
        emotion_hint: Optional[str] = None,
        importance: float = 0.0,
        meta: Optional[Dict[str, Any]] = None,
    ) -> int:
        """
        対話エピソードを保存する。
        importance は 0.0〜1.0 を想定（高いほど重要）
        """
        ts = _utc_now_iso()
        meta_str = self._dump_meta(meta)

        cur = self._conn.cursor()
        cur.execute(
            """
            INSERT INTO episodes (
                user_id, session_id, ts, role, content,
                topic_hint, emotion_hint, importance, meta
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);
            """,
            (
                self.user_id,
                session_id,
                ts,
                role,
                content,
                topic_hint,
                emotion_hint,
                float(importance),
                meta_str,
            ),
        )
        self._conn.commit()
        return int(cur.lastrowid)

    def load_recent_episodes(
        self,
        limit: int = 50,
        min_importance: float = 0.0,
    ) -> List[EpisodeRecord]:
        """
        直近のエピソードを新しい順で取得。
        """
        cur = self._conn.cursor()
        cur.execute(
            """
            SELECT *
            FROM episodes
            WHERE user_id = ?
              AND importance >= ?
            ORDER BY ts DESC, id DESC
            LIMIT ?;
            """,
            (self.user_id, float(min_importance), int(limit)),
        )

        rows = cur.fetchall()
        records: List[EpisodeRecord] = []
        for r in rows:
            records.append(
                EpisodeRecord(
                    id=int(r["id"]),
                    user_id=str(r["user_id"]),
                    session_id=str(r["session_id"] or ""),
                    ts=str(r["ts"]),
                    role=str(r["role"]),
                    content=str(r["content"]),
                    topic_hint=r["topic_hint"],
                    emotion_hint=r["emotion_hint"],
                    importance=float(r["importance"]),
                    meta=self._load_meta(r["meta"]),
                )
            )
        return records

    # ------------------------------------------------------------------
    # concepts
    # ------------------------------------------------------------------
    def store_concept(
        self,
        label: str,
        score: float,
        occurrences: int = 1,
        meta: Optional[Dict[str, Any]] = None,
    ) -> int:
        """
        概念クラスタを upsert する。
        ・既存 label があれば score や occurrences を更新
        ・なければ新規追加
        """
        ts = _utc_now_iso()
        meta_str = self._dump_meta(meta)

        cur = self._conn.cursor()
        # 既存レコードを確認
        cur.execute(
            """
            SELECT id, score, occurrences
            FROM concepts
            WHERE user_id = ? AND label = ?;
            """,
            (self.user_id, label),
        )
        row = cur.fetchone()

        if row:
            # 更新
            new_score = float(score)
            new_occ = int(row["occurrences"]) + int(occurrences)
            cur.execute(
                """
                UPDATE concepts
                SET score = ?, occurrences = ?, last_seen = ?, meta = ?
                WHERE id = ?;
                """,
                (
                    new_score,
                    new_occ,
                    ts,
                    meta_str,
                    int(row["id"]),
                ),
            )
            self._conn.commit()
            return int(row["id"])
        else:
            # 挿入
            cur.execute(
                """
                INSERT INTO concepts (
                    user_id, label, score, occurrences, last_seen, meta
                ) VALUES (?, ?, ?, ?, ?, ?);
                """,
                (
                    self.user_id,
                    label,
                    float(score),
                    int(occurrences),
                    ts,
                    meta_str,
                ),
            )
            self._conn.commit()
            return int(cur.lastrowid)

    def load_concepts(
        self,
        limit: int = 50,
        min_score: float = 0.0,
    ) -> List[ConceptRecord]:
        """
        概念クラスタを score の高い順に取得。
        """
        cur = self._conn.cursor()
        cur.execute(
            """
            SELECT *
            FROM concepts
            WHERE user_id = ?
              AND score >= ?
            ORDER BY score DESC, occurrences DESC, last_seen DESC
            LIMIT ?;
            """,
            (self.user_id, float(min_score), int(limit)),
        )
        rows = cur.fetchall()
        records: List[ConceptRecord] = []
        for r in rows:
            records.append(
                ConceptRecord(
                    id=int(r["id"]),
                    user_id=str(r["user_id"]),
                    label=str(r["label"]),
                    score=float(r["score"]),
                    occurrences=int(r["occurrences"]),
                    last_seen=str(r["last_seen"]),
                    meta=self._load_meta(r["meta"]),
                )
            )
        return records

    # ------------------------------------------------------------------
    # identity_events
    # ------------------------------------------------------------------
    def store_identity_event(
        self,
        kind: str,
        delta_calm: float = 0.0,
        delta_empathy: float = 0.0,
        delta_curiosity: float = 0.0,
        reward: float = 0.0,
        meta: Optional[Dict[str, Any]] = None,
    ) -> int:
        """
        トレイト変動・価値変動などの「人格に関わるイベント」を記録。
        """
        ts = _utc_now_iso()
        meta_str = self._dump_meta(meta)

        cur = self._conn.cursor()
        cur.execute(
            """
            INSERT INTO identity_events (
                user_id, kind,
                delta_calm, delta_empathy, delta_curiosity,
                reward, ts, meta
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?);
            """,
            (
                self.user_id,
                kind,
                float(delta_calm),
                float(delta_empathy),
                float(delta_curiosity),
                float(reward),
                ts,
                meta_str,
            ),
        )
        self._conn.commit()
        return int(cur.lastrowid)

    def load_identity_events(
        self,
        limit: int = 100,
        kind: Optional[str] = None,
    ) -> List[IdentityEventRecord]:
        """
        直近の identity_events を新しい順で取得。
        kind を指定するとその種類に絞る。
        """
        cur = self._conn.cursor()

        if kind is None:
            cur.execute(
                """
                SELECT *
                FROM identity_events
                WHERE user_id = ?
                ORDER BY ts DESC, id DESC
                LIMIT ?;
                """,
                (self.user_id, int(limit)),
            )
        else:
            cur.execute(
                """
                SELECT *
                FROM identity_events
                WHERE user_id = ?
                  AND kind = ?
                ORDER BY ts DESC, id DESC
                LIMIT ?;
                """,
                (self.user_id, kind, int(limit)),
            )

        rows = cur.fetchall()
        records: List[IdentityEventRecord] = []
        for r in rows:
            records.append(
                IdentityEventRecord(
                    id=int(r["id"]),
                    user_id=str(r["user_id"]),
                    kind=str(r["kind"]),
                    delta_calm=float(r["delta_calm"]),
                    delta_empathy=float(r["delta_empathy"]),
                    delta_curiosity=float(r["delta_curiosity"]),
                    reward=float(r["reward"]),
                    ts=str(r["ts"]),
                    meta=self._load_meta(r["meta"]),
                )
            )
        return records

    # ------------------------------------------------------------------
    # クローズ
    # ------------------------------------------------------------------
    def close(self) -> None:
        try:
            self._conn.close()
        except Exception:
            pass

=== FILE: sigmaris-core/sigmaris_persona_core/persona_modules/__init__.py ===

# sigmaris_persona_core/persona_modules/__init__.py

from .contradiction_manager import ContradictionManager
from .silence_manager import SilenceManager
from .intuition_engine import IntuitionEngine
from .value_drift_engine import ValueDriftEngine
from .memory_integrator import MemoryIntegrator
from .identity_continuity_engine import IdentityContinuityEngine
from .meta_reward_engine import MetaRewardEngine
from .emotion_core import EmotionCore
from .snapshot_builder import SnapshotBuilder

__all__ = [
    "ContradictionManager",
    "SilenceManager",
    "IntuitionEngine",
    "ValueDriftEngine",
    "MemoryIntegrator",
    "IdentityContinuityEngine",
    "MetaRewardEngine",
    "EmotionCore",
    "SnapshotBuilder"
]

=== FILE: sigmaris-core/sigmaris_persona_core/persona_modules/contradiction_manager.py ===

# sigmaris_persona_core/persona_modules/contradiction_manager.py
from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Dict, Any
import re
import time

from ..types import Message


@dataclass
class ContradictionManager:
    """
    矛盾検出モジュール（PersonaOS 完全版）

    役割:
      - feed(): 直近のユーザーメッセージ（最大 max_history 件）を蓄積
      - detect(): 直前メッセージと今回メッセージを比較し、態度や意図の急反転を検出

    PersonaOS での利用想定:

        self.contradiction.feed(incoming)
        info = self.contradiction.detect(incoming)

        info = {
            "flags": {
                "contradiction": bool,
            },
            "note": str | None,
        }

    ※ v0.2 では「軽量ヒューリスティック」のみ。将来的な強化（系列矛盾など）は
       このインターフェースを維持したまま拡張する前提。
    """

    # 保存履歴（content のみ）
    history: List[str] = field(default_factory=list)
    timestamps: List[float] = field(default_factory=list)

    # 最大保持件数
    max_history: int = 32

    # 軽量肯定/否定パターン（normalize 後のテキストに対して検索）
    POSITIVE: List[str] = field(default_factory=lambda: [
        r"そう思う",
        r"賛成",
        r"いいと思う",
        r"そうだね",
        r"はい",
        r"ok",
        r"同意",
        r"なるほど",
        r"理解した",
        r"わかる",
        r"好き",
        r"興味ある",
    ])

    NEGATIVE: List[str] = field(default_factory=lambda: [
        r"そう思わない",
        r"違う",
        r"反対",
        r"よくない",
        r"だめ",
        r"無理",
        r"いや",
        r"納得できない",
        r"嫌い",
        r"わからない",
    ])

    # ============================================================
    # normalize
    # ============================================================
    def _normalize(self, text: str) -> str:
        """
        ゆらぎを落とす軽量正規化。

        - 前後の空白トリム
        - 改行・連続空白を削除
        - 全角/半角の単純な混在をある程度吸収
        - 句読点・一般的な終端記号を除去
        """
        if not text:
            return ""
        t = text.lower().strip()
        # 空白類を削る
        t = re.sub(r"[ \t\r\n　]+", "", t)
        # 句読点・記号を削る
        t = re.sub(r"[。,.!?！？…、]", "", t)
        return t

    # ============================================================
    # Feed
    # ============================================================
    def feed(self, message: Message) -> None:
        """
        incoming Message から content を取り出し、履歴に積む。

        - role は問わないが、通常は user/system_user を想定。
        - timestamp があれば尊重し、なければ現在時刻を使う。
        """
        content = getattr(message, "content", "") or ""
        if not content:
            return

        self.history.append(content)

        ts_raw = getattr(message, "timestamp", None)
        try:
            ts = float(ts_raw) if ts_raw is not None else time.time()
        except Exception:
            ts = time.time()
        self.timestamps.append(ts)

        # 上限管理（古いものから削除）
        if len(self.history) > self.max_history:
            overflow = len(self.history) - self.max_history
            if overflow > 0:
                self.history = self.history[overflow:]
                self.timestamps = self.timestamps[overflow:]

    # ============================================================
    # Detect
    # ============================================================
    def detect(self, message: Message) -> Dict[str, Any]:
        """
        「直前の履歴メッセージ」と今回の message を比較して
        明確な矛盾（態度の急反転・意図の逆転など）があれば検出する。

        戻り値:
            {
              "flags": { "contradiction": bool },
              "note": str | None,
            }
        """
        new_raw = getattr(message, "content", "") or ""
        new_text = self._normalize(new_raw)

        # 比較対象となる「前のメッセージ」がない場合は矛盾なし
        if len(self.history) < 2 or not new_text:
            return {
                "flags": {"contradiction": False},
                "note": None,
            }

        # feed() で既に incoming を積んでいる前提なので、
        # 比較対象は history[-2]（ひとつ前の発話）とする
        last_raw = self.history[-2]
        last_text = self._normalize(last_raw)

        # history があっても last_text が空なら矛盾判定はスキップ
        if not last_text:
            return {
                "flags": {"contradiction": False},
                "note": None,
            }

        # ========================================================
        # 1. 肯定 → 否定 / 否定 → 肯定 の急反転
        # ========================================================
        pos_new = any(re.search(p, new_text) for p in self.POSITIVE)
        neg_new = any(re.search(n, new_text) for n in self.NEGATIVE)
        pos_last = any(re.search(p, last_text) for p in self.POSITIVE)
        neg_last = any(re.search(n, last_text) for n in self.NEGATIVE)

        if (pos_last and neg_new) or (neg_last and pos_new):
            return {
                "flags": {"contradiction": True},
                "note": f"態度の急反転（『{last_raw}』→『{new_raw}』）",
            }

        # ========================================================
        # 2. 単語レベルの反転（できる/できない・行く/行かない など）
        # ========================================================
        pairs = [
            ("できる", "できない"),
            ("行く", "行かない"),
            ("やる", "やらない"),
            ("続ける", "やめる"),
        ]

        for pos, neg in pairs:
            if (pos in last_text and neg in new_text) or \
               (neg in last_text and pos in new_text):
                return {
                    "flags": {"contradiction": True},
                    "note": f"意図の逆転（{pos}/{neg}）",
                }

        # ========================================================
        # 3. 否定語の急増
        #    ※ 完全な NLP ではなくヒューリスティック
        # ========================================================
        neg_markers = ["違う", "いや", "無理", "否定", "そんなことない", "だめ", "嫌"]

        last_neg_count = sum(m in last_text for m in neg_markers)
        new_neg_count = sum(m in new_text for m in neg_markers)

        if (new_neg_count - last_neg_count) >= 3:
            return {
                "flags": {"contradiction": True},
                "note": "否定語の急増",
            }

        # ========================================================
        # 4. 文面の多くが一致しつつ “ない” だけ増える
        #    → 典型的な 軽否定 → 強否定 への反転パターン
        # ========================================================
        # 例:
        #   last: "たぶんできると思う"
        #   new : "たぶんできると思わない / できないかもしれない"
        if last_text and new_text.startswith(last_text) and ("ない" in new_text):
            return {
                "flags": {"contradiction": True},
                "note": "文面一致からの否定方向への反転",
            }

        # ========================================================
        # Default: 矛盾なし
        # ========================================================
        return {
            "flags": {"contradiction": False},
            "note": None,
        }

=== FILE: sigmaris-core/sigmaris_persona_core/persona_modules/emotion_core.py ===

# sigmaris_persona_core/persona_modules/emotion_core.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Any

from ..config import EmotionConfig
from ..types import TraitVector


def _clamp(value: float, min_v: float, max_v: float) -> float:
    """value を min_v〜max_v にクリップする小ユーティリティ。"""
    if value < min_v:
        return min_v
    if value > max_v:
        return max_v
    return value


@dataclass
class EmotionCore:
    """
    Sampling 戦略と「語りトーン」を決めるレイヤ。

    - PersonaOS からは decide_tone_and_sampling() だけを呼べばよい。
    - 入力: TraitVector（calm / empathy / curiosity）
    - 出力: LLM 呼び出し用の tone / temperature / top_p
    """

    config: EmotionConfig

    # ------------------------------------------------------------
    # public API
    # ------------------------------------------------------------
    def decide_tone_and_sampling(self, traits: TraitVector) -> Dict[str, Any]:
        """
        現在のトレイトから、応答の「トーン」と sampling パラメータを決定する。
        PersonaOS 側では以下のキーを参照することを前提にする:

        - tone: str                 … 文章スタイルのヒント
        - temperature: float        … LLM 温度
        - top_p: float              … nucleus sampling
        - meta: Dict[str, Any]      … デバッグ用の内部値
        """

        # TraitVector → float に安全変換
        calm = float(traits.calm)
        empathy = float(traits.empathy)
        curiosity = float(traits.curiosity)

        # -------------------------
        # 1. 内部スコア算出
        # -------------------------
        # 「覚醒度」: 落ち着きが低い + 好奇心が高いほど高めに
        arousal = (1.0 - calm) * 0.6 + curiosity * 0.4
        arousal = _clamp(arousal, 0.0, 1.0)

        # 「温かさ」: 共感性が高いほど warm 寄り（現状は meta 用）
        warmth = _clamp(empathy, 0.0, 1.0)

        # -------------------------
        # 2. temperature 決定
        # -------------------------
        base_t = float(self.config.base_temperature)

        # arousal が 0.5 より高いなら温度を上げ、低ければ下げる
        raw_temperature = base_t + (arousal - 0.5) * 0.4
        # 実際のクリップは EmotionConfig.clamp を通す
        temperature = self.config.clamp(raw_temperature)

        # -------------------------
        # 3. top_p 決定
        # -------------------------
        # 好奇心が高いほど「広く」サンプルするイメージ
        # calm が高いと少しだけ絞る方向
        base_top_p = 0.90
        top_p = base_top_p + (curiosity - 0.5) * 0.12 - (calm - 0.5) * 0.08
        top_p = _clamp(top_p, 0.60, 0.98)

        # -------------------------
        # 4. tone（スタイルラベル）
        # -------------------------
        tone = self._decide_tone(calm=calm, empathy=empathy, curiosity=curiosity)

        return {
            "tone": tone,
            "temperature": temperature,
            "top_p": top_p,
            "meta": {
                "arousal": arousal,
                "warmth": warmth,
                "traits": {
                    "calm": calm,
                    "empathy": empathy,
                    "curiosity": curiosity,
                },
            },
        }

    # ------------------------------------------------------------
    # internal helpers
    # ------------------------------------------------------------
    def _decide_tone(self, *, calm: float, empathy: float, curiosity: float) -> str:
        """
        トレイトから人間が読めるラベルを決める。
        （実際の LLM プロンプト側で利用しても良いし、UI 表示用でも良い）
        """

        # かなり落ち着いていて、共感性も高い
        if calm >= 0.7 and empathy >= 0.6:
            if curiosity >= 0.6:
                return "warm-curious"
            return "warm-calm"

        # 好奇心優位
        if curiosity >= 0.7:
            if calm >= 0.5:
                return "curious-balanced"
            return "curious-intense"

        # 落ち着きが低く、やや不安定
        if calm <= 0.3:
            if empathy >= 0.6:
                return "gentle-tense"
            return "tense-direct"

        # 共感性が高めで、他は中庸
        if empathy >= 0.7:
            return "soft-supportive"

        # どれも中庸 → ニュートラル
        return "neutral-analytic"

=== FILE: sigmaris-core/sigmaris_persona_core/persona_modules/identity_continuity_engine.py ===

# sigmaris_persona_core/persona_modules/identity_continuity_engine.py
from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
import time
import re

from ..types import Message


@dataclass
class IdentityAnchor:
    """
    「アイデンティティの手がかり」1件分。

    - ts:   記録時刻（epoch秒）
    - text: アンカーとして使うテキスト（要約 or 抜粋）
    - kind: anchor の種類（"topic" / "self" / "ref" など）
    - meta: デバッグ・UI用メタ情報
    """
    ts: float
    text: str
    kind: str = "topic"
    meta: Dict[str, Any] = field(default_factory=dict)


@dataclass
class IdentityContinuityEngine:
    """
    Identity Continuity（文脈的一貫性）モジュール v0.2

    PersonaOS からの使われ方：
      - process() 内で identity.update(incoming)
      - identity_hint = identity.get_hint()

    目的：
      「前に話してた〇〇の件だけど」というときに、
      その「〇〇」を軽量に思い出すための“アンカー”を管理する。

    設計方針：
      - 完全な長期記憶検索ではなく、直近〜中期の「象徴的な断片」を保持
      - 役割は「ヒントを返す」ことであって、完全な要約ではない
      - DB（MemoryDB）に依存しない純ローカルモジュール
    """

    # 保持するアンカーのリスト（古いものから順に）
    anchors: List[IdentityAnchor] = field(default_factory=list)

    # 最大保持件数
    max_anchors: int = 32

    # アンカー候補の最低文字数
    min_length: int = 20

    # 「自分」「変化」「方向性」などアイデンティティ寄りの語
    self_keywords: List[str] = field(
        default_factory=lambda: [
            "私",
            "自分",
            "俺",
            "僕",
            "わたし",
            "将来",
            "これから",
            "方向",
            "変わった",
            "変えたい",
            "今の自分",
            "昔の自分",
            "居場所",
        ]
    )

    # 「前に話してた〜」「さっきの件」など参照表現
    ref_keywords: List[str] = field(
        default_factory=lambda: [
            "前に話してた",
            "この前の",
            "さっきの件",
            "前に言ってた",
            "前回の",
            "前の話",
            "さっきの話",
            "あの件",
        ]
    )

    # ============================================================
    #  Internal helpers
    # ============================================================

    def _normalize(self, text: str) -> str:
        """改行・余分なスペースを潰して軽く正規化。"""
        if not isinstance(text, str):
            text = str(text or "")
        t = text.strip()
        t = re.sub(r"\s+", " ", t)
        return t

    # ============================================================
    #  Public API
    # ============================================================

    def update(self, message: Message) -> None:
        """
        新しいメッセージを観測し、「アンカー化するかどうか」を判定して保存。

        ざっくりルール：
          - role == "user" のみ対象
          - 一定以上の長さがある
          - 自己言及 or 方向性 or 参照表現を含む場合は優先的にアンカー
        """
        content = getattr(message, "content", "") or ""
        role = getattr(message, "role", "user")

        if not content or role != "user":
            return

        text = self._normalize(content)

        # 短すぎるものはスキップ
        if len(text) < self.min_length:
            return

        # Message に timestamp があれば尊重、なければ現在時刻
        try:
            ts_attr = getattr(message, "timestamp", None)
            if isinstance(ts_attr, (int, float)):
                now_ts = float(ts_attr)
            else:
                now_ts = time.time()
        except Exception:
            now_ts = time.time()

        # 種類判定
        kind = "topic"
        meta: Dict[str, Any] = {"raw_length": len(text)}

        if any(k in text for k in self.ref_keywords):
            kind = "ref"
        elif any(k in text for k in self.self_keywords):
            kind = "self"

        # アンカー用の短い抜粋を作る（先頭 80〜120 文字）
        snippet = text
        max_len = 120
        if len(snippet) > max_len:
            snippet = snippet[: max_len - 3] + "..."

        # 直前のアンカーと全く同じならノイズとしてスキップ
        if self.anchors and self.anchors[-1].text == snippet:
            return

        anchor = IdentityAnchor(
            ts=now_ts,
            text=snippet,
            kind=kind,
            meta=meta,
        )

        self._push_anchor(anchor)

    # ------------------------------------------------------------

    def get_hint(self) -> Optional[Dict[str, Any]]:
        """
        PersonaDecision.debug["identity_hint"] に渡すヒント構造を返す。

        戻り値の例：
          {
            "last": { "text": "...", "kind": "self", "ts": 1730000000.0 },
            "recent_self": [...],
            "anchor_count": 7,
          }

        PersonaOS 側では「None かどうか」だけを見ることもあるが、
        UI/デバッグ向けにある程度構造化して返す。
        """
        if not self.anchors:
            return None

        last = self.anchors[-1]

        # 直近 N 件のうち self/ref 系だけを軽くまとめる
        recent_self: List[Dict[str, Any]] = []
        for a in reversed(self.anchors):
            if len(recent_self) >= 5:
                break
            if a.kind in ("self", "ref"):
                recent_self.append(
                    {
                        "text": a.text,
                        "kind": a.kind,
                        "ts": a.ts,
                    }
                )

        return {
            "last": {
                "text": last.text,
                "kind": last.kind,
                "ts": last.ts,
            },
            "recent_self": recent_self,
            "anchor_count": len(self.anchors),
        }

    # ============================================================
    #  Internal
    # ============================================================

    def _push_anchor(self, anchor: IdentityAnchor) -> None:
        """
        アンカーをキューに追加し、max_anchors を超えたら古いものから削る。
        """
        self.anchors.append(anchor)
        if len(self.anchors) > self.max_anchors:
            overflow = len(self.anchors) - self.max_anchors
            if overflow > 0:
                # 古い順に詰まっているので先頭から drop
                self.anchors = self.anchors[overflow:]

=== FILE: sigmaris-core/sigmaris_persona_core/persona_modules/intuition_engine.py ===

# sigmaris_persona_core/persona_modules/intuition_engine.py
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Dict, Any

from ..types import Message
from ..config import IntuitionConfig


@dataclass
class IntuitionEngine:
    """
    疑似直観（Pseudo-Intuition）エンジン — PersonaOS 完全版 v0.2 対応

    PersonaOS.process() 内の利用：
        intuition_info = self.intuition.infer(self.messages)

    戻り値：
        {
            "allow": bool,
            "reason": str,
        }
    """

    config: IntuitionConfig

    # ============================================================
    # PUBLIC API
    # ============================================================
    def infer(self, messages: List[Message]) -> Dict[str, Any]:
        """
        全メッセージ履歴から「疑似直観」を発火させるか判定する。
        Reflection / Introspection の起動条件にも関わる。
        """

        # --------------------------------------------------------
        # 0. 履歴が存在しない
        # --------------------------------------------------------
        if not messages:
            return {"allow": False, "reason": "no_messages"}

        # --------------------------------------------------------
        # 1. user メッセージの抽出
        # --------------------------------------------------------
        user_msgs = [m for m in messages if getattr(m, "role", None) == "user"]
        total = len(user_msgs)

        min_ctx = max(1, int(self.config.min_context_size))

        if total < min_ctx:
            return {
                "allow": False,
                "reason": f"context_too_small:{total}",
            }

        # --------------------------------------------------------
        # 2. timestamp の整合性チェック
        # --------------------------------------------------------
        timestamps: List[float] = []
        for m in user_msgs:
            raw = getattr(m, "timestamp", None)
            try:
                if raw is not None:
                    timestamps.append(float(raw))
            except Exception:
                continue

        if len(timestamps) < 2:
            return {
                "allow": False,
                "reason": "insufficient_timestamp",
            }

        span = max(0.0, max(timestamps) - min(timestamps))
        min_span = max(0.0, float(self.config.min_time_span_sec))

        if span < min_span:
            return {
                "allow": False,
                "reason": f"timespan_short:{span:.2f}",
            }

        # --------------------------------------------------------
        # 3. 内容ベース（深層パターン）
        # --------------------------------------------------------
        last_msg = user_msgs[-1]
        content_raw = getattr(last_msg, "content", "") or ""
        content = content_raw.lower().strip()

        # PersonaOS 側で introspection/reflect の根拠に使う深層パターン
        deep_keywords = [
            "どう思う", "なんで", "理由", "意味", "方向", "本質",
            "変わった", "内面", "深い", "正直",
            # 英語圏用
            "why", "meaning", "reason", "direction", "inner", "core",
        ]

        deep_hit = any(k in content for k in deep_keywords)

        if not deep_hit:
            return {
                "allow": False,
                "reason": "no_deep_pattern",
            }

        # --------------------------------------------------------
        # 4. 疑似直観発火
        # --------------------------------------------------------
        return {
            "allow": True,
            "reason": "intuition_triggered",
        }

=== FILE: sigmaris-core/sigmaris_persona_core/persona_modules/memory_integrator.py ===

# sigmaris_persona_core/persona_modules/memory_integrator.py
from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Dict, Any
import time

from ..types import MemoryEntry
from ..config import MemoryConfig


@dataclass
class MemoryIntegrator:
    """
    記憶統合レイヤ v0.2

    - 「短期（short）」「中期（mid）」の 2 ストラタをプロセス内で保持
    - long-term は persona_db（episodes / concepts）側に委譲
    - PersonaOS からは:

        self.memory.feed(
            MemoryEntry(
                ts=incoming.timestamp,
                kind="short" | "mid",
                content=...,
                meta={...},
            )
        )

      のように呼び出されることを想定。

    役割:
      - 時間窓に応じて short / mid バッファをメンテナンス
      - UI / デバッグ用に snapshot() で状態を返す
      - long-term は MemoryDB.store_episode / store_concept 側が担当
    """

    config: MemoryConfig

    # プロセス内の短期・中期メモリバッファ
    short_buffer: List[MemoryEntry] = field(default_factory=list)
    mid_buffer: List[MemoryEntry] = field(default_factory=list)

    # ============================================================
    # Feed
    # ============================================================
    def feed(self, entry: MemoryEntry) -> None:
        """
        1 つの MemoryEntry を受け取り、short / mid のストラタを更新する。

        kind は現状 "short" / "mid" を想定しているが、
        多少ラフに扱えるよう、防御的に実装。
        """
        now = time.time()

        # kind 自体は今のところ挙動には使わないが、
        # 将来の分岐用に一応抜き出しておく
        kind = getattr(entry, "kind", "short") or "short"
        _ = kind  # reserved for future use

        # short レイヤに追加
        self.short_buffer.append(entry)
        # mid レイヤにも積む（mid 専用エントリも short と同様に蓄積）
        self.mid_buffer.append(entry)

        # 古い short / mid を時間窓から外す
        self._gc_short(now=now)
        self._gc_mid(now=now)

    # ============================================================
    # ガーベジコレクション（時間窓でのトリミング）
    # ============================================================
    def _gc_short(self, now: float) -> None:
        """
        short_window_sec を超えたエントリを short_buffer から除去。
        """
        window = float(self.config.short_window_sec)
        threshold = now - window

        new_buf: List[MemoryEntry] = []
        for e in self.short_buffer:
            try:
                ts = float(getattr(e, "ts", 0.0))
            except Exception:
                # ts が取れないものは念のため残しておく（ログ用途など）
                new_buf.append(e)
                continue

            if ts >= threshold:
                new_buf.append(e)

        self.short_buffer = new_buf

    def _gc_mid(self, now: float) -> None:
        """
        mid_window_sec を超えたエントリを mid_buffer から除去。
        """
        window = float(self.config.mid_window_sec)
        threshold = now - window

        new_buf: List[MemoryEntry] = []
        for e in self.mid_buffer:
            try:
                ts = float(getattr(e, "ts", 0.0))
            except Exception:
                new_buf.append(e)
                continue

            if ts >= threshold:
                new_buf.append(e)

        self.mid_buffer = new_buf

    # ============================================================
    # Snapshot / 可視化用ヘルパ
    # ============================================================
    def snapshot(self) -> Dict[str, Any]:
        """
        UI / デバッグ用に、現在の short / mid メモリの概況を返す。

        内容全文ではなく、
        - 件数
        - 直近 N 件の簡易情報
        だけ返す。
        """

        def _to_view(entries: List[MemoryEntry], limit: int = 8) -> List[Dict[str, Any]]:
            view: List[Dict[str, Any]] = []
            for e in entries[-limit:]:
                try:
                    view.append(
                        {
                            "ts": float(getattr(e, "ts", 0.0)),
                            "kind": getattr(e, "kind", "short"),
                            "content_preview": (getattr(e, "content", "") or "")[:64],
                            "meta": getattr(e, "meta", {}) or {},
                        }
                    )
                except Exception:
                    continue
            return view

        return {
            "short": {
                "count": len(self.short_buffer),
                "entries": _to_view(self.short_buffer),
            },
            "mid": {
                "count": len(self.mid_buffer),
                "entries": _to_view(self.mid_buffer),
            },
        }

=== FILE: sigmaris-core/sigmaris_persona_core/persona_modules/meta_reward_engine.py ===

# sigmaris_persona_core/persona_modules/meta_reward_engine.py
from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Dict, Any
import time

from ..types import Message, RewardSignal, TraitVector


@dataclass
class MetaRewardEngine:
    """
    MetaRewardEngine（メタ報酬エンジン / PersonaOS 完全版）

    役割:
      - 直近のメッセージ履歴から
          ・depth（どれだけ本質や理由に踏み込んでいるか）
          ・openness（どれだけ本音・内面を開示しているか）
          ・stability（感情表現の極端さ・安定度）
          ・negativity（ネガティブ度）
        を評価する。
      - RewardSignal を新仕様で返す：
          - global_reward（= value としても参照可）: -1.0〜1.0
          - trait_reward: calm / empathy / curiosity への軸別報酬（-1.0〜1.0 想定）
          - meta: 指標と trait_reward の dict 版
          - detail: 必要なら将来の可視化用の内訳を積む（v0.2では軽め）

    PersonaOS / ValueDriftEngine からは主に:

      reward.value                … 全体報酬
      reward.meta["trait_reward"] … 軸別報酬（dict形式）
      reward.trait_reward         … TraitVector としての軸別報酬

    を読み取る想定。
    """

    # 評価対象とする時間窓（秒）
    window_sec: float = 5 * 60.0  # 直近5分

    # ローカル履歴
    history: List[Message] = field(default_factory=list)

    # ============================================================
    # Feed
    # ============================================================
    def feed(self, message: Message) -> None:
        """
        直近 window_sec 秒分だけを保持する簡易バッファ。
        """
        now = time.time()
        self.history.append(message)

        # 古いものは削除
        trimmed: List[Message] = []
        for m in self.history:
            ts = getattr(m, "timestamp", None)
            if ts is None:
                trimmed.append(m)
                continue
            if (now - ts) <= self.window_sec:
                trimmed.append(m)
        self.history = trimmed

    # ============================================================
    # Main
    # ============================================================
    def compute(self) -> RewardSignal:
        """
        現在の履歴から RewardSignal を1つ生成する。

        戻り値:
          RewardSignal(
            global_reward = float,         # -1.0〜1.0
            trait_reward  = TraitVector,   # calm/empathy/curiosityへの軸別報酬
            reason        = "meta_reward:...",
            meta          = {..., "trait_reward": {...}},
            detail        = {...}          # 今後の可視化用に拡張可能
          )
        """

        # 0件ならニュートラル
        if not self.history:
            return RewardSignal(
                global_reward=0.0,
                trait_reward=None,
                reason="meta_reward:no_history",
                meta={
                    "depth": 0.0,
                    "openness": 0.0,
                    "stability": 0.0,
                    "negativity": 0.0,
                    "sample_size": 0,
                    "trait_reward": {
                        "calm": 0.0,
                        "empathy": 0.0,
                        "curiosity": 0.0,
                    },
                },
                detail={},
            )

        # user 発話のみ評価
        user_msgs = [
            m for m in self.history
            if getattr(m, "role", "") in ("user", "system_user")
        ]

        if not user_msgs:
            return RewardSignal(
                global_reward=0.0,
                trait_reward=None,
                reason="meta_reward:no_user_messages",
                meta={
                    "depth": 0.0,
                    "openness": 0.0,
                    "stability": 0.0,
                    "negativity": 0.0,
                    "sample_size": 0,
                    "trait_reward": {
                        "calm": 0.0,
                        "empathy": 0.0,
                        "curiosity": 0.0,
                    },
                },
                detail={},
            )

        # -----------------------------
        # スコア計算
        # -----------------------------
        depth = self._measure_depth(user_msgs)
        openness = self._measure_openness(user_msgs)
        stability = self._measure_stability(user_msgs)
        negativity = self._measure_negativity(user_msgs)

        # -----------------------------
        # 全体報酬の合成
        # -----------------------------
        raw = (
            + 0.4 * depth
            + 0.3 * openness
            + 0.3 * stability
            - 0.5 * negativity
        )
        global_reward = self._clip(raw, -1.0, 1.0)

        # -----------------------------
        # 軸別報酬の設計
        # -----------------------------
        # calm     : 安定度 - ネガティブ度
        # empathy  : 開示度 - ネガティブ度の一部
        # curiosity: 深さ   - ネガティブ度の一部
        calm_axis = self._clip(stability - negativity, -1.0, 1.0)
        empathy_axis = self._clip(openness - negativity * 0.5, -1.0, 1.0)
        curiosity_axis = self._clip(depth - negativity * 0.3, -1.0, 1.0)

        trait_vec = TraitVector(
            calm=calm_axis,
            empathy=empathy_axis,
            curiosity=curiosity_axis,
        )

        # reason タグ
        tags = []
        if depth > 0.6:
            tags.append("deep")
        if openness > 0.6:
            tags.append("open")
        if stability > 0.6:
            tags.append("stable")
        if negativity > 0.4:
            tags.append("negative")

        reason = "meta_reward"
        if tags:
            reason += ":" + ",".join(tags)

        # meta / detail には UI / デバッグが欲しがる情報をまとめておく
        meta: Dict[str, Any] = {
            "depth": depth,
            "openness": openness,
            "stability": stability,
            "negativity": negativity,
            "sample_size": len(user_msgs),
            # ValueDriftEngine v0.3 が参照する dict 形式の trait_reward
            "trait_reward": {
                "calm": calm_axis,
                "empathy": empathy_axis,
                "curiosity": curiosity_axis,
            },
        }

        detail: Dict[str, Any] = {
            "weights": {
                "depth": 0.4,
                "openness": 0.3,
                "stability": 0.3,
                "negativity": -0.5,
            },
            "raw_score": raw,
        }

        return RewardSignal(
            global_reward=global_reward,
            trait_reward=trait_vec,
            reason=reason,
            meta=meta,
            detail=detail,
        )

    # ============================================================
    # 内部スコアリング
    # ============================================================
    def _measure_depth(self, messages: List[Message]) -> float:
        deep_keywords = [
            "なぜ", "なんで", "どうして", "意味", "理由", "本質",
            "方向", "変わった", "どう思う",
            "what does", "why", "meaning",
        ]
        if not messages:
            return 0.0
        hits = sum(
            1
            for m in messages
            if any(k in (m.content or "").lower() for k in deep_keywords)
        )
        return self._clip(hits / len(messages), 0.0, 1.0)

    def _measure_openness(self, messages: List[Message]) -> float:
        keys = [
            "疲れ", "しんど", "つら", "悩", "不安", "正直", "本音",
            "私は", "俺は", "気持ち", "感情", "怖",
        ]
        if not messages:
            return 0.0
        hits = sum(
            1
            for m in messages
            if any(k in (m.content or "").lower() for k in keys)
        )
        return self._clip(hits / len(messages), 0.0, 1.0)

    def _measure_stability(self, messages: List[Message]) -> float:
        extreme_pos = ["最高", "完璧", "神", "最強"]
        extreme_neg = ["最悪", "無理", "死にたい", "消えたい"]
        if not messages:
            # 中庸とみなして 0.5
            return 0.5
        hits = sum(
            1
            for m in messages
            if any(k in (m.content or "").lower() for k in extreme_pos)
            or any(k in (m.content or "").lower() for k in extreme_neg)
        )
        ratio = hits / len(messages)
        # 極端表現が多いほど安定度を下げる
        return self._clip(1.0 - ratio, 0.0, 1.0)

    def _measure_negativity(self, messages: List[Message]) -> float:
        neg = [
            "無理", "だめ", "嫌", "いや", "疲れた", "しんどい",
            "つらい", "終わり", "価値がない", "どうでもいい",
        ]
        if not messages:
            return 0.0
        hits = sum(
            1
            for m in messages
            if any(k in (m.content or "").lower() for k in neg)
        )
        return self._clip(hits / len(messages), 0.0, 1.0)

    # ============================================================
    # 共通ユーティリティ
    # ============================================================
    @staticmethod
    def _clip(v: float, v_min: float, v_max: float) -> float:
        if v < v_min:
            return v_min
        if v > v_max:
            return v_max
        return v

=== FILE: sigmaris-core/sigmaris_persona_core/persona_modules/silence_manager.py ===

# sigmaris_persona_core/persona_modules/silence_manager.py
from __future__ import annotations
from dataclasses import dataclass
from typing import Dict, Any

from ..config import SilenceConfig


@dataclass
class SilenceManager:
    """
    主体的沈黙モジュール（PersonaOS 完全版 v0.2 対応）

    PersonaOS.process() 内で使われる想定：
        silence_info = silence.decide(
            abstraction_score=...,
            loop_suspect_score=...,
            user_insists=...
        )

    返却形式:
        {
            "silence": bool,
            "reason": str,
        }
    """

    config: SilenceConfig

    # ============================================================
    # DECISION CORE
    # ============================================================
    def decide(
        self,
        *,
        abstraction_score: float,
        loop_suspect_score: float,
        user_insists: bool,
    ) -> Dict[str, Any]:
        """
        主体的沈黙ロジック。

        方針（完全版 v0.2）:
          - abstraction_score / loop_suspect_score が閾値を超えると沈黙候補
          - user が強く要求している場合 allow_when_user_insists が True なら返答可能
          - 返答理由は全てタグ化して debug に渡す
        """

        # ---- 0. 安全クリップ ---------------------------------
        a = max(0.0, min(1.0, float(abstraction_score)))
        l = max(0.0, min(1.0, float(loop_suspect_score)))

        # ---- 1. 閾値判定 --------------------------------------
        too_abstract = a >= float(self.config.max_abstraction)
        too_loopy = l >= float(self.config.max_loop_suspect)

        silence_candidate = bool(too_abstract or too_loopy)

        # ---- 2. user insist -----------------------------------
        if user_insists and bool(self.config.allow_when_user_insists):
            return {
                "silence": False,
                "reason": self._build_reason(
                    silence=False,
                    too_abstract=too_abstract,
                    too_loopy=too_loopy,
                    user_insists=True,
                ),
            }

        # ---- 3. 沈黙発動 --------------------------------------
        if silence_candidate:
            return {
                "silence": True,
                "reason": self._build_reason(
                    silence=True,
                    too_abstract=too_abstract,
                    too_loopy=too_loopy,
                    user_insists=False,
                ),
            }

        # ---- 4. 通常返信 --------------------------------------
        return {
            "silence": False,
            "reason": "reply_selected:threshold_not_reached",
        }

    # ============================================================
    # REASON BUILDER
    # ============================================================
    def _build_reason(
        self,
        *,
        silence: bool,
        too_abstract: bool,
        too_loopy: bool,
        user_insists: bool,
    ) -> str:
        """
        PersonaOS.debug に入るタグ生成器。
        """
        tags: list[str] = []

        if too_abstract:
            tags.append("abstract_overload")
        if too_loopy:
            tags.append("loop_suspect")
        if user_insists:
            tags.append("user_insists")

        base = "silence_selected" if silence else "reply_selected"

        if tags:
            return f"{base}:" + ",".join(tags)
        return base

=== FILE: sigmaris-core/sigmaris_persona_core/persona_modules/snapshot_builder.py ===

from __future__ import annotations
from dataclasses import dataclass
from typing import Dict, Any, Optional

from ..types import TraitVector
from ..config import EmotionConfig


@dataclass
class SnapshotBuilder:
    """
    PersonaOS の内部状態を 1 スナップショットとしてまとめるモジュール（完全版）

    PersonaOS.process() からは：
        snapshot = self.snapshot_builder.build(
            state=state,
            traits=self.traits,
            flags=flags,
            reward=reward,
        )

    Snapshot の目的：
        - UI 表示
        - デバッグ（LLM 応答と内部状態の整合性確認）
        - ロギング
        - AEI Core との内部同期
    """

    emotion_config: Optional[EmotionConfig] = None

    # ============================================================
    # PUBLIC API — Snapshot Build
    # ============================================================
    def build(
        self,
        *,
        state: str,
        traits: TraitVector,
        flags: Dict[str, bool],
        reward: Any,
    ) -> Dict[str, Any]:
        """
        PersonaOS 内部状態を snapshot としてまとめて返す。
        """
        return {
            "state": state,
            "traits": self._traits_block(traits),
            "flags": flags,
            "reward": self._reward_block(reward),
            "meta": {
                "version": "persona_snapshot_v0.3",
                "emotion_config": self._emotion_cfg_view(),
            },
        }

    # ============================================================
    # INTERNAL — Traits
    # ============================================================
    def _traits_block(self, traits: TraitVector) -> Dict[str, float]:
        """
        TraitVector → dict（0.0〜1.0）
        """
        try:
            return {
                "calm": float(traits.calm),
                "empathy": float(traits.empathy),
                "curiosity": float(traits.curiosity),
            }
        except Exception:
            # 防御的 fallback
            return {
                "calm": float(getattr(traits, "calm", 0.5)),
                "empathy": float(getattr(traits, "empathy", 0.5)),
                "curiosity": float(getattr(traits, "curiosity", 0.5)),
            }

    # ============================================================
    # INTERNAL — Reward
    # ============================================================
    def _reward_block(self, reward: Any) -> Dict[str, Any]:
        """
        RewardSignal（クラス）/ dict の両方に対応する統一フォーマット。

        出力形式:
            {
              "global_reward": float,
              "trait_reward": {calm, empathy, curiosity},
              "reason": str,
              "meta": {...}
            }
        """

        # ---- global_reward ----
        if hasattr(reward, "global_reward"):
            try:
                global_r = float(getattr(reward, "global_reward"))
            except Exception:
                global_r = 0.0
        else:
            try:
                global_r = float(reward.get("global_reward", 0.0))
            except Exception:
                global_r = 0.0

        # ---- reason ----
        if hasattr(reward, "reason"):
            reason = getattr(reward, "reason", None)
        else:
            reason = reward.get("reason")

        # ---- trait_reward ----
        if hasattr(reward, "trait_reward"):
            tr_raw = getattr(reward, "trait_reward", None)
        else:
            tr_raw = reward.get("trait_reward")

        trait_reward = {}
        for k in ("calm", "empathy", "curiosity"):
            try:
                if isinstance(tr_raw, dict):
                    trait_reward[k] = float(tr_raw.get(k, 0.0))
                else:
                    trait_reward[k] = float(getattr(tr_raw, k, 0.0))
            except Exception:
                trait_reward[k] = 0.0

        # ---- meta ----
        if hasattr(reward, "meta"):
            meta_raw = getattr(reward, "meta", None)
        else:
            meta_raw = reward.get("meta")

        meta = meta_raw if isinstance(meta_raw, dict) else {}

        return {
            "global_reward": global_r,
            "trait_reward": trait_reward,
            "reason": reason,
            "meta": meta,
        }

    # ============================================================
    # INTERNAL — Emotion Config Preview
    # ============================================================
    def _emotion_cfg_view(self) -> Dict[str, float] | None:
        """EmotionConfig の必要最小限ビュー（UI / デバッグ向け）"""
        cfg = self.emotion_config
        if cfg is None:
            return None
        try:
            return {
                "base_temperature": float(cfg.base_temperature),
                "min_temperature": float(cfg.min_temperature),
                "max_temperature": float(cfg.max_temperature),
                "base_top_p": float(cfg.base_top_p),
                "emotion_bias": float(cfg.emotion_bias),
            }
        except Exception:
            return None 

=== FILE: sigmaris-core/sigmaris_persona_core/persona_modules/value_drift_engine.py ===

# sigmaris_persona_core/persona_modules/value_drift_engine.py
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Dict

from ..types import TraitVector, RewardSignal
from ..config import ValueDriftConfig


@dataclass
class ValueDriftEngine:
    """
    自律的価値変動（Value Drift）エンジン v0.3

    役割:
      - 現在の TraitVector（calm/empathy/curiosity）と RewardSignal から
        「わずかな長期ドリフト」を加える。
      - 長期的には 0.5 付近に収束させつつ、
        報酬に応じた微細なズレを蓄積させる。

    RewardSignal 仕様（types.py）:
      - global_reward: float  （-1.0〜+1.0 を想定）
      - trait_reward: Optional[TraitVector]  （軸ごとの報酬 / 使わない場合 None）
      - reason: str
      - meta: dict
      - detail: dict
      - value プロパティは global_reward の互換アクセスとして定義済み。

    本エンジンは以下の順に値を解釈する：
      - グローバル報酬:
          1) reward.global_reward
          2) reward.value
          3) dict["global_reward"]
          4) dict["value"]
      - トレイト報酬:
          1) reward.trait_reward（TraitVector 互換オブジェクト）
          2) dict["trait_reward"]
          3) dict["meta"]["trait_reward"]
    """

    config: ValueDriftConfig

    # ------------------------------------------------------------
    # public API
    # ------------------------------------------------------------
    def step(
        self,
        traits: TraitVector,
        reward: RewardSignal | Dict[str, Any],
    ) -> TraitVector:
        """
        1 ステップ分の Value Drift を適用した TraitVector を返す。

        - traits: 現在のトレイト値（0.0〜1.0 を想定）
        - reward: MetaRewardEngine 等からの RewardSignal または dict
        """
        # 現在値を float にクリップして受け取る
        calm = float(traits.calm)
        empathy = float(traits.empathy)
        curiosity = float(traits.curiosity)

        # 0.5 の中心へ弱い引き戻し（長期的に暴走しないよう減衰）
        calm = self._decay_toward_center(calm)
        empathy = self._decay_toward_center(empathy)
        curiosity = self._decay_toward_center(curiosity)

        # RewardSignal を読み取る
        global_r = self._get_global_reward(reward)
        trait_r = self._get_trait_reward(reward)

        # 報酬の大きさに応じてステップサイズを算出
        step_size = self._compute_step_size(global_r)

        # 軸ごとに微小なドリフトを加える
        calm += self._drift_for_axis(
            axis_name="calm",
            axis_value=calm,
            global_reward=global_r,
            trait_reward=trait_r,
            step_size=step_size,
        )
        empathy += self._drift_for_axis(
            axis_name="empathy",
            axis_value=empathy,
            global_reward=global_r,
            trait_reward=trait_r,
            step_size=step_size,
        )
        curiosity += self._drift_for_axis(
            axis_name="curiosity",
            axis_value=curiosity,
            global_reward=global_r,
            trait_reward=trait_r,
            step_size=step_size,
        )

        # 0〜1 にクリップして返す
        return TraitVector(
            calm=self._clip01(calm),
            empathy=self._clip01(empathy),
            curiosity=self._clip01(curiosity),
        )

    # ------------------------------------------------------------
    # decay
    # ------------------------------------------------------------
    def _decay_toward_center(self, v: float) -> float:
        """
        中心 0.5 に向かって少しだけ引き戻す。

        v' = 0.5 + (v - 0.5) * decay
        """
        center = 0.5
        return center + (v - center) * float(self.config.decay)

    # ------------------------------------------------------------
    # reward getter（新仕様 / 旧仕様 両方対応）
    # ------------------------------------------------------------
    def _get_global_reward(
        self,
        reward: RewardSignal | Dict[str, Any],
    ) -> float:
        """
        グローバル報酬は以下順で探す：

          1. reward.global_reward
          2. reward.value（互換プロパティ）
          3. dict["global_reward"]
          4. dict["value"]
        """
        # dataclass / オブジェクト形式（RewardSignal）
        if hasattr(reward, "global_reward"):
            try:
                return float(getattr(reward, "global_reward"))
            except Exception:
                pass

        # 旧コードとの互換: value プロパティ
        if hasattr(reward, "value"):
            try:
                return float(getattr(reward, "value"))
            except Exception:
                pass

        # dict 形式
        if isinstance(reward, dict):
            if "global_reward" in reward:
                try:
                    return float(reward.get("global_reward", 0.0))
                except Exception:
                    pass
            if "value" in reward:
                try:
                    return float(reward.get("value", 0.0))
                except Exception:
                    pass

        # 見つからなければニュートラル
        return 0.0

    def _get_trait_reward(
        self,
        reward: RewardSignal | Dict[str, Any],
    ) -> Dict[str, float]:
        """
        trait_reward は以下順で探す：

          1. reward.trait_reward（TraitVector 互換）
          2. dict["trait_reward"]
          3. dict["meta"]["trait_reward"]
        """
        # 1. オブジェクト型: reward.trait_reward（TraitVector 想定）
        if hasattr(reward, "trait_reward"):
            try:
                tr = getattr(reward, "trait_reward")
            except Exception:
                tr = None

            if tr is not None:
                vals: Dict[str, float] = {}
                for k in ("calm", "empathy", "curiosity"):
                    try:
                        vals[k] = float(getattr(tr, k, 0.0))
                    except Exception:
                        vals[k] = 0.0
                return vals

        # 2. dict 型: reward["trait_reward"] が dict のケース
        if isinstance(reward, dict):
            tr = reward.get("trait_reward")
            if isinstance(tr, dict):
                vals_dict: Dict[str, float] = {}
                for k in ("calm", "empathy", "curiosity"):
                    try:
                        vals_dict[k] = float(tr.get(k, 0.0))
                    except Exception:
                        vals_dict[k] = 0.0
                return vals_dict

            # 3. 新仕様の直列化: reward["meta"]["trait_reward"]
            meta = reward.get("meta")
            if isinstance(meta, dict):
                tr2 = meta.get("trait_reward")
                if isinstance(tr2, dict):
                    vals_meta: Dict[str, float] = {}
                    for k in ("calm", "empathy", "curiosity"):
                        try:
                            vals_meta[k] = float(tr2.get(k, 0.0))
                        except Exception:
                            vals_meta[k] = 0.0
                    return vals_meta

        # 何もなければ0で埋める
        return {"calm": 0.0, "empathy": 0.0, "curiosity": 0.0}

    # ------------------------------------------------------------
    # step size
    # ------------------------------------------------------------
    def _compute_step_size(self, global_reward: float) -> float:
        """
        global_reward の絶対値に応じて
        min_step〜max_step の間でステップ幅を決める。

        - |global_reward| が小さい → min_step 付近
        - |global_reward| が大きい → max_step 付近
        """
        mag = abs(global_reward)
        if mag > 1.0:
            mag = 1.0

        min_s = float(self.config.min_step)
        max_s = float(self.config.max_step)

        return min_s + (max_s - min_s) * mag

    # ------------------------------------------------------------
    # axis drift
    # ------------------------------------------------------------
    def _drift_for_axis(
        self,
        axis_name: str,
        axis_value: float,
        global_reward: float,
        trait_reward: Dict[str, float],
        step_size: float,
    ) -> float:
        """
        個別軸（calm/empathy/curiosity）に対する変動量を計算。

        - global_reward は全軸に薄く効く
        - trait_reward[axis_name] があれば、その軸にやや強めに効く

        axis_value は将来的に
        「高すぎる時は抑制する」といったロジック追加用に残している。
        """
        _ = axis_value  # 未来の拡張用に保持

        # global_reward による共通ドリフト
        drift = step_size * float(global_reward) * 0.3

        # 各軸固有の評価
        axis_r = float(trait_reward.get(axis_name, 0.0))

        # 軸固有の評価は少し強めに反映
        drift += step_size * axis_r * 0.7

        return drift

    # ------------------------------------------------------------
    # clip
    # ------------------------------------------------------------
    @staticmethod
    def _clip01(v: float) -> float:
        """0.0〜1.0 にクリップ。"""
        if v < 0.0:
            return 0.0
        if v > 1.0:
            return 1.0
        return v

=== FILE: sigmaris-core/sigmaris_persona_core/persona_modules.py ===

from __future__ import annotations
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
import math
import time

from .types import (
    Message,
    TraitVector,
    MemoryEntry,
    RewardSignal,
    PersonaStateSnapshot,
)
from .config import (
    SilenceConfig,
    ValueDriftConfig,
    IntuitionConfig,
    MemoryConfig,
    EmotionConfig,
)

# ============================================================
# ① 矛盾保持モジュール（軽量版）
# ============================================================

@dataclass
class ContradictionManager:
    history: List[Message] = field(default_factory=list)

    def feed(self, message: Message) -> None:
        self.history.append(message)

    def detect(self, message: Message) -> Dict[str, Any]:
        """
        - 最小限の「簡易矛盾検出」
        - 後で Embedding ＆ semantic conflict 判定に差し替える前提
        """
        content = message.content.lower()
        flags = {"contradiction": False}
        note = ""

        opposites = [
            ("好き", "嫌い"),
            ("trust", "distrust"),
            ("楽しい", "つらい")
        ]

        for past in reversed(self.history[-50:]):
            for a, b in opposites:
                if a in past.content and b in content:
                    flags["contradiction"] = True
                    note = f"past:「{a}」 vs now:「{b}」"
                    break
            if flags["contradiction"]:
                break

        return {"flags": flags, "note": note}


# ============================================================
# ② 主体的沈黙モジュール
# ============================================================

@dataclass
class SilenceManager:
    config: SilenceConfig

    def decide(
        self,
        *,
        abstraction_score: float,
        loop_suspect_score: float,
        user_insists: bool,
    ) -> Dict[str, Any]:

        should_silence = False
        reason = ""

        # 抽象度オーバー
        if abstraction_score > self.config.max_abstraction:
            should_silence = True
            reason = "abstraction_overload"

        # ループ疑惑
        if loop_suspect_score > self.config.max_loop_suspect:
            should_silence = True
            reason = "loop_suspect"

        # ユーザーが強く求めている場合は解除
        if user_insists and self.config.allow_when_user_insists:
            should_silence = False
            reason = "user_override"

        return {"silence": should_silence, "reason": reason}


# ============================================================
# ③ 疑似直観エンジン
# ============================================================

@dataclass
class IntuitionEngine:
    config: IntuitionConfig

    def infer(self, messages: List[Message]) -> Dict[str, Any]:

        if len(messages) < self.config.min_context_size:
            return {"allow": False, "strength": 0.0, "reason": "not_enough_context"}

        times = [m.timestamp for m in messages]
        if not times:
            return {"allow": False, "strength": 0.0, "reason": "no_time_info"}

        span = max(times) - min(times)
        if span < self.config.min_time_span_sec:
            return {"allow": False, "strength": 0.0, "reason": "span_too_short"}

        return {
            "allow": True,
            "strength": self.config.strength,
            "reason": "ok",
        }


# ============================================================
# ④ Value Drift（自律価値変動）
# ============================================================

@dataclass
class ValueDriftEngine:
    config: ValueDriftConfig

    def step(self, traits: TraitVector, reward: Optional[RewardSignal]) -> TraitVector:

        def approach(cur: float, target: float, amount: float) -> float:
            return max(0.0, min(1.0, cur + (target - cur) * amount))

        # 通常ドリフト：常に 0.5 に引き寄せる弱い力
        drift_step = self.config.min_step
        calm = approach(traits.calm, 0.5, drift_step)
        emp = approach(traits.empathy, 0.5, drift_step)
        cur = approach(traits.curiosity, 0.5, drift_step)

        if reward is None:
            return TraitVector(calm, emp, cur)

        # 報酬の符号による drift 振幅
        sign = 1 if reward.value >= 0 else -1
        mag = min(abs(reward.value), 1.0)

        step = drift_step + (self.config.max_step - drift_step) * mag

        target_calm = calm + sign * 0.1 * mag
        target_emp = emp + sign * 0.1 * mag
        target_cur = cur + sign * 0.1 * mag

        return TraitVector(
            approach(calm, target_calm, step),
            approach(emp, target_emp, step),
            approach(cur, target_cur, step),
        )


# ============================================================
# ⑤ MemoryIntegrator（軽量層）
# ============================================================

@dataclass
class MemoryIntegrator:
    config: MemoryConfig
    buffer: List[MemoryEntry] = field(default_factory=list)

    def feed(self, entry: MemoryEntry) -> None:
        self.buffer.append(entry)

    def stratify(self, now: Optional[float] = None) -> Dict[str, List[MemoryEntry]]:
        now = now or time.time()

        short, mid, long = [], [], []

        for e in self.buffer:
            age = now - e.ts
            if age <= self.config.short_window_sec:
                short.append(e)
            elif age <= self.config.mid_window_sec:
                mid.append(e)
            else:
                long.append(e)

        return {"short": short, "mid": mid, "long": long}


# ============================================================
# ⑥ Identity Continuity Engine（完全版）
# ============================================================

@dataclass
class IdentityContinuityEngine:
    """
    PersonaOS 完全版の Identity Continuity モジュール。
    - 「話題の連続性」「過去のトピック」「参照すべき anchor」などを扱う
    - persona-db を利用して長期的な anchor を保持する
    """

    anchors: List[str] = field(default_factory=list)

    def update(self, msg: Message) -> None:
        """
        anchor 候補の抽出（軽量版）
        """
        text = msg.content
        if any(key in text for key in ["件", "前に", "続き", "前回", "前の話"]):
            self.anchors.append(text[:40])

    def get_hint(self) -> Optional[str]:
        if not self.anchors:
            return None
        return self.anchors[-1]


# ============================================================
# ⑦ Meta Reward Engine（完全版）
# ============================================================

@dataclass
class MetaRewardEngine:
    """
    「深度」「安定性」「流れの連続性」を報酬信号として返す簡易モデル。
    PersonaOS 完全版では、
    - RewardCore（sigmaris-core）とは独立した “会話構造” 報酬
    """
    window: List[Message] = field(default_factory=list)

    def feed(self, message: Message) -> None:
        self.window.append(message)
        if len(self.window) > 30:
            self.window.pop(0)

    def compute(self) -> RewardSignal:

        # ユーザ発話だけ見る
        user_msgs = [m for m in self.window if m.role == "user"]
        if not user_msgs:
            return RewardSignal(value=0.0, reason="no_data")

        lengths = [len(m.content) for m in user_msgs]
        avg_len = sum(lengths) / len(lengths)

        # depth-based reward
        if avg_len < 20:
            return RewardSignal(value=-0.3, reason="too_short")
        if avg_len > 400:
            return RewardSignal(value=-0.2, reason="too_long")

        # ここは “ちょうどよい深さ”
        return RewardSignal(value=0.4, reason="good_depth")


# ============================================================
# ⑧ Emotion Core
# ============================================================

@dataclass
class EmotionCore:
    config: EmotionConfig

    def decide_tone_and_sampling(self, traits: TraitVector) -> Dict[str, Any]:

        base = self.config.base_temperature

        delta = (
            (traits.curiosity - 0.5) * 0.35 -
            (traits.calm - 0.5) * 0.25
        )

        temp = max(
            self.config.min_temperature,
            min(self.config.max_temperature, base + delta),
        )

        # tone decision
        if traits.empathy > 0.65:
            tone = "soft"
        elif traits.calm > 0.6 and traits.empathy < 0.45:
            tone = "dry"
        else:
            tone = "neutral"

        return {
            "tone": tone,
            "temperature": temp,
            "top_p": 0.9,
        }


# ============================================================
# ⑨ Snapshot Builder（OS 調停）
# ============================================================

@dataclass
class SnapshotBuilder:
    def build(
        self,
        *,
        state: str,
        traits: TraitVector,
        flags: Dict[str, bool],
        reward: Optional[RewardSignal],
    ) -> PersonaStateSnapshot:

        return PersonaStateSnapshot(
            state=state,
            traits=traits,
            flags=flags,
            last_reward=reward,
        )

=== FILE: sigmaris-core/sigmaris_persona_core/persona_os.py ===

# sigmaris_persona_core/persona_os.py
from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Dict, Any, Literal, Optional

from .types import (
    Message,
    TraitVector,
    PersonaContext,
    PersonaDecision,
    RewardSignal,
    MemoryEntry,
)
from .config import PersonaOSConfig
from .state_machine import StateMachine
from .persona_modules import (
    ContradictionManager,
    SilenceManager,
    IntuitionEngine,
    ValueDriftEngine,
    MemoryIntegrator,
    IdentityContinuityEngine,
    MetaRewardEngine,
    EmotionCore,
    SnapshotBuilder,
)

# 🔥 Persona-DB v0.2 — Multi-User DB
from persona_db.memory_db import MemoryDB
from persona_db.growth_log import GrowthLogEntry


DepthPref = Literal["shallow", "normal", "deep"]


@dataclass
class PersonaOS:
    """
    Sigmaris PersonaOS 完全版 (core-level)

    - LLM 本体はここでは持たず、応答方針（PersonaDecision）のみを返す。
    - persona_db は user_id ごとに専用 DB を開く（v0.2）。
    - process() 内で:
        - 矛盾検出 / 主体的沈黙 / 疑似直観 / ValueDrift / Emotion を実行
        - growth_log を growth_log テーブルへ永続化
        - episodes を episodes テーブルへ永続化（＋concepts の自動更新）
    """

    config: PersonaOSConfig
    traits: TraitVector = field(
        default_factory=lambda: TraitVector(calm=0.5, empathy=0.5, curiosity=0.5)
    )

    # サブシステム
    state_machine: StateMachine = field(init=False)
    contradiction: ContradictionManager = field(init=False)
    silence: SilenceManager = field(init=False)
    intuition: IntuitionEngine = field(init=False)
    value_drift: ValueDriftEngine = field(init=False)
    memory: MemoryIntegrator = field(init=False)
    identity: IdentityContinuityEngine = field(init=False)
    meta_reward: MetaRewardEngine = field(init=False)
    emotion: EmotionCore = field(init=False)
    snapshot_builder: SnapshotBuilder = field(init=False)

    # ローカルプロセス内の履歴
    messages: List[Message] = field(default_factory=list)

    # Persona-DB インスタンスキャッシュ（user_id -> MemoryDB）
    db_cache: Dict[str, MemoryDB] = field(default_factory=dict)

    def __post_init__(self) -> None:
        # ステートマシン & 基本モジュール
        self.state_machine = StateMachine(self.config.state)
        self.contradiction = ContradictionManager()
        self.silence = SilenceManager(self.config.silence)
        self.intuition = IntuitionEngine(self.config.intuition)
        self.value_drift = ValueDriftEngine(self.config.value_drift)
        self.memory = MemoryIntegrator(self.config.memory)
        self.identity = IdentityContinuityEngine()

        # MetaRewardEngine（内部で window_sec 等を扱う。ここではデフォルト構成を利用）
        self.meta_reward = MetaRewardEngine()

        # Emotion レイヤ
        self.emotion = EmotionCore(self.config.emotion)

        # SnapshotBuilder（UI / デバッグ用）
        self.snapshot_builder = SnapshotBuilder()

    # ============================================================
    # Internal utility — per-user DB
    # ============================================================

    def _db(self, user_id: Optional[str]) -> MemoryDB:
        """
        user_id ごとの DB をキャッシュして返す。
        user_id が空 or None の場合は "system" として扱う。
        """
        key = user_id or "system"
        if key not in self.db_cache:
            self.db_cache[key] = MemoryDB(user_id=key)
        return self.db_cache[key]

    # ============================================================
    # Main Entry
    # ============================================================

    def process(
        self,
        *,
        incoming: Message,
        context: PersonaContext,
        depth_pref: DepthPref = "normal",
        safety_flagged: bool = False,
        abstraction_score: float = 0.0,
        loop_suspect_score: float = 0.0,
    ) -> PersonaDecision:
        # ローカル履歴
        self.messages.append(incoming)

        # --------------------------------------------------------
        # 0. user-specific DB を取得し、identity_events から traits を再構成
        # --------------------------------------------------------
        user_key = context.user_id or "system"

        try:
            user_db: Optional[MemoryDB] = self._db(user_key)
        except Exception:
            user_db = None

        if user_db is not None:
            try:
                latest = user_db.load_latest_traits(
                    baseline={
                        "calm": float(self.traits.calm),
                        "empathy": float(self.traits.empathy),
                        "curiosity": float(self.traits.curiosity),
                    }
                )
                self.traits = TraitVector(
                    calm=float(latest.get("calm", self.traits.calm)),
                    empathy=float(latest.get("empathy", self.traits.empathy)),
                    curiosity=float(latest.get("curiosity", self.traits.curiosity)),
                )
            except Exception:
                # DB 側の問題で人格コアを巻き込まない
                pass

        # --------------------------------------------------------
        # 1. ローカルモジュールへの feed
        # --------------------------------------------------------
        self.contradiction.feed(incoming)
        self.identity.update(incoming)
        self.meta_reward.feed(incoming)

        # MemoryIntegrator 用の短期エントリ
        self.memory.feed(
            MemoryEntry(
                ts=incoming.timestamp,
                kind="short",
                content=incoming.content,
                meta={"role": incoming.role},
            )
        )

        # --------------------------------------------------------
        # 2. episodes 永続化（＋concepts 自動更新）
        # --------------------------------------------------------
        if user_db is not None:
            try:
                # context.extra から topic_hint を拾えるなら拾う
                topic_hint = None
                extra = getattr(context, "extra", None)
                if isinstance(extra, dict):
                    topic_hint = extra.get("topic") or extra.get("topic_hint")

                # content 長から超ラフに importance を決める（0.1〜1.0）
                content_len = len(incoming.content or "")
                if content_len > 0:
                    importance = content_len / 200.0
                else:
                    importance = 0.1
                if importance < 0.1:
                    importance = 0.1
                if importance > 1.0:
                    importance = 1.0

                user_db.store_episode(
                    session_id=context.session_id,
                    role=incoming.role,
                    content=incoming.content,
                    topic_hint=topic_hint,
                    emotion_hint=None,  # v0.2 では未使用
                    importance=importance,
                    meta={
                        "client": context.client,
                        "depth_pref": depth_pref,
                    },
                )
            except Exception:
                # DB 側の問題で人格コアを巻き込まない
                pass

        # --------------------------------------------------------
        # 3. 矛盾検出
        # --------------------------------------------------------
        contradiction_info = self.contradiction.detect(incoming)
        contradiction_flags = contradiction_info.get("flags", {})
        contradiction_note = contradiction_info.get("note")

        # --------------------------------------------------------
        # 4. 疑似直観 判定
        # --------------------------------------------------------
        intuition_info = self.intuition.infer(self.messages)

        # --------------------------------------------------------
        # 5. 主体的沈黙 判定
        # --------------------------------------------------------
        content_str = incoming.content or ""
        user_insists = ("教えて" in content_str) or ("どう思う" in content_str)
        silence_info = self.silence.decide(
            abstraction_score=abstraction_score,
            loop_suspect_score=loop_spect_score,
            user_insists=user_insists,
        )

        # --------------------------------------------------------
        # 6. Identity Continuity ヒント
        # --------------------------------------------------------
        identity_hint = self.identity.get_hint()
        identity_anchor = identity_hint is not None

        # --------------------------------------------------------
        # 7. 状態遷移
        # --------------------------------------------------------
        intuition_allow = bool(intuition_info.get("allow", False))
        contradiction_flag = bool(contradiction_flags.get("contradiction", False))

        state = self.state_machine.step(
            user_requested_depth=depth_pref,
            safety_flagged=safety_flagged,
            reflection_candidate=(intuition_allow and depth_pref == "deep"),
            introspection_candidate=((not intuition_allow) and depth_pref == "deep"),
            contradiction_flag=contradiction_flag,
            intuition_allow=intuition_allow,
            identity_anchor=identity_anchor,
            abstraction_score=abstraction_score,
            loop_suspect_score=loop_suspect_score,
        )

        # --------------------------------------------------------
        # 8. メタ報酬 & Value Drift
        # --------------------------------------------------------
        raw_reward = self.meta_reward.compute()

        if isinstance(raw_reward, RewardSignal):
            reward: RewardSignal = raw_reward
        elif isinstance(raw_reward, dict):
            reward = RewardSignal(
                global_reward=raw_reward.get(
                    "global_reward", raw_reward.get("value", 0.0)
                ),
                reason=str(raw_reward.get("reason", "")),
                meta=raw_reward,
                detail=raw_reward.get("detail", {}),
            )
        else:
            # 予期しない型の場合は安全側に倒す
            reward = RewardSignal(value=0.0, reason="unsupported_reward_type")

        prev_traits = TraitVector(
            calm=self.traits.calm,
            empathy=self.traits.empathy,
            curiosity=self.traits.curiosity,
        )

        new_traits = self.value_drift.step(self.traits, reward)
        self.traits = new_traits

        # --------------------------------------------------------
        # 9. Emotion レイヤ
        # --------------------------------------------------------
        emo = self.emotion.decide_tone_and_sampling(self.traits)

        # --------------------------------------------------------
        # 10. Snapshot / Debug 構築
        # --------------------------------------------------------
        flags: Dict[str, bool] = {
            "safety_flagged": safety_flagged,
            "silence": bool(silence_info.get("silence")),
            "contradiction": contradiction_flag,
            "intuition_allow": intuition_allow,
            "identity_anchor": identity_anchor,
        }

        snapshot = self.snapshot_builder.build(
            state=state,
            traits=self.traits,
            flags=flags,
            reward=reward,
        )

        debug: Dict[str, Any] = {
            "state": state,
            "silence_reason": silence_info.get("reason"),
            "intuition_reason": intuition_info.get("reason"),
            "contradiction_note": contradiction_note,
            "identity_hint": identity_hint,
            "snapshot": snapshot,
            "context": {
                "user_id": context.user_id,
                "session_id": context.session_id,
                "client": context.client,
            },
        }

        # --------------------------------------------------------
        # 11. Persona-DB: growth_log 永続化 (v0.2)
        # --------------------------------------------------------
        if user_db is not None:
            try:
                user_db.store_growth_log(
                    GrowthLogEntry(
                        user_id=user_key,
                        session_id=context.session_id,
                        last_message=incoming.content,
                        traits_before=prev_traits,
                        traits_after=new_traits,
                        reward=reward,
                        state=state,
                        flags=flags,
                    )
                )
            except Exception:
                # DB 例外は握りつぶす
                pass

        # --------------------------------------------------------
        # 12. PersonaDecision を返す
        # --------------------------------------------------------
        allow_reply = (not bool(silence_info.get("silence"))) and (not safety_flagged)

        return PersonaDecision(
            allow_reply=allow_reply,
            preferred_state=state,
            tone=emo["tone"],
            temperature=float(emo["temperature"]),
            top_p=float(emo["top_p"]),
            need_reflection=(state == "reflect"),
            need_introspection=(state == "introspect"),
            apply_contradiction_note=flags["contradiction"],
            apply_identity_anchor=identity_anchor,
            updated_traits=self.traits,
            reward=reward,
            debug=debug,
        )

    # ============================================================
    # AEI BRIDGE — Reflection / Reward / Emotion / Value
    # ============================================================

    def feed_reflection(
        self,
        msg: Message,
        summary: Dict[str, Any],
        context: PersonaContext,
    ) -> None:
        """
        AEI Core の Reflection 結果を PersonaOS に渡すフック。
        - msg: 元のユーザ発話
        - summary: AEI 側の要約/分析
        """
        self.messages.append(msg)
        self.identity.update(msg)

        # MemoryIntegrator の mid-layer に積む
        self.memory.feed(
            MemoryEntry(
                ts=msg.timestamp,
                kind="mid",
                content=summary.get("summary", msg.content),
                meta={
                    "role": msg.role,
                    "raw_text": msg.content,
                    "summary": summary,
                },
            )
        )
        # v0.2 ではここでは episodes への追記は行わない（生ログは process 側で処理）

    def feed_reward(self, reward_res: Dict[str, Any], user_id: str = "system") -> None:
        """
        AEI Core の RewardCore 出力を PersonaOS に渡す。
        identity_events に kind='reward' として記録。
        """
        try:
            db = self._db(user_id)
            db.store_identity_event(
                kind="reward",
                reward=float(
                    reward_res.get(
                        "global_reward",
                        reward_res.get("value", 0.0),
                    )
                ),
                meta=reward_res,
            )
        except Exception:
            pass

    def feed_emotion(
        self,
        emotion_res: Dict[str, Any],
        user_id: str = "system",
    ) -> None:
        """
        AEI Core の EmotionCore 出力を PersonaOS に渡す。
        trait_shift をそのまま identity_events に delta_* として積む。
        """
        try:
            db = self._db(user_id)
            shift = emotion_res.get("trait_shift", {}) or {}
            db.store_identity_event(
                kind="emotion",
                delta_calm=float(shift.get("calm", 0.0)),
                delta_empathy=float(shift.get("empathy", 0.0)),
                delta_curiosity=float(shift.get("curiosity", 0.0)),
                meta=emotion_res,
            )
        except Exception:
            pass

    def feed_value(self, value_res: Dict[str, Any], user_id: str = "system") -> None:
        """
        AEI Core の ValueCore 出力を PersonaOS に渡す。
        importance 配列を concepts に投げ込む。
        """
        try:
            db = self._db(user_id)
            weight = float(value_res.get("weight", 0.5))
            for label in value_res.get("importance", []):
                db.store_concept(
                    label=label,
                    score=weight,
                    occurrences=1,
                    meta=value_res,
                )
        except Exception:
            pass

    # ============================================================
    # shutdown hook
    # ============================================================

    def close_all(self) -> None:
        """
        プロセス終了時などに DB コネクションをすべて閉じるためのフック。
        """
        for db in self.db_cache.values():
            try:
                db.close()
            except Exception:
                continue

=== FILE: sigmaris-core/sigmaris_persona_core/route/persona.py ===

# sigmaris_persona_core/routes/persona.py
from __future__ import annotations
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Dict, Any

from sigmaris_persona_core.persona_os import PersonaOS
from persona_db.memory_db import MemoryDB

# ------------------------------------------------------------
# Router
# ------------------------------------------------------------
router = APIRouter()

# ------------------------------------------------------------
# Request models
# ------------------------------------------------------------
class MessageModel(BaseModel):
    role: str
    content: str
    timestamp: Optional[float] = None


class PersonaRequest(BaseModel):
    user_id: str
    session_id: str
    messages: List[MessageModel]


# ------------------------------------------------------------
# PersonaOS インスタンス（user_id ごとに DB を持つ）
# ------------------------------------------------------------
def get_os(user_id: str) -> PersonaOS:
    """user_id ごとに MemoryDB を持つ PersonaOS を生成."""
    db = MemoryDB(user_id=user_id)
    return PersonaOS(user_id=user_id, db=db)


# ------------------------------------------------------------
# API: /persona/respond
# ------------------------------------------------------------
@router.post("/respond")
def persona_respond(req: PersonaRequest) -> Dict[str, Any]:
    os = get_os(req.user_id)

    try:
        # PersonaOS に問い合わせ
        result = os.process(
            session_id=req.session_id,
            messages=[
                {
                    "role": m.role,
                    "content": m.content,
                    "timestamp": m.timestamp,
                }
                for m in req.messages
            ],
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

    return result


# ------------------------------------------------------------
# API: /persona/state
# ------------------------------------------------------------
@router.get("/state/{user_id}")
def persona_state(user_id: str) -> Dict[str, Any]:
    os = get_os(user_id)
    try:
        return os.export_state()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ------------------------------------------------------------
# API: /persona/memory/recent
# ------------------------------------------------------------
@router.get("/memory/recent/{user_id}")
def persona_memory_recent(user_id: str) -> Dict[str, Any]:
    db = MemoryDB(user_id=user_id)
    try:
        logs = db.get_recent_growth_logs(limit=100)
        return {"growth_log": logs}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

=== FILE: sigmaris-core/sigmaris_persona_core/state_machine.py ===

# sigmaris_persona_core/state_machine.py
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Literal, Dict, Any, Optional
import time

from .config import StateMachineConfig

"""
PersonaOS 完全版対応 StateMachine

【役割】
- PersonaOS 内部の「状態レイヤ」を管理する小さなステートマシン
- Safety / Overload / Reflect / Introspect / Intuition / Identity Anchor を統合
- depth_pref（shallow / normal / deep）の3層モードに対応

【主なステート】
- idle             : 待機（初期状態）
- dialogue         : 通常対話
- reflect          : 反省（短期）
- introspect       : 内省（中期）
- intuition-deep   : 疑似直観による自発的 deep
- identity-anchor  : Identity Continuity 回復フェーズ
- overload-prevent : メッセージ過多による一時ブレーキ
- safety           : セーフティ優先モード
"""

StateName = Literal[
    "idle",
    "dialogue",
    "reflect",
    "introspect",
    "intuition-deep",
    "identity-anchor",
    "overload-prevent",
    "safety",
]


@dataclass
class StateMachine:
    """
    PersonaOS 内部ステートマシン v0.3（完全版）

    - StateMachineConfig と 1:1 で対応するフィールドのみ参照する
    - PersonaOS からは step() / info() / load_hint() を使用
    """

    config: StateMachineConfig

    # 現在ステート
    current: StateName = "idle"

    # 直近の発火時刻
    last_reflection_ts: float = field(default_factory=lambda: 0.0)
    last_introspection_ts: float = field(default_factory=lambda: 0.0)

    # 過負荷管理
    last_activity_ts: float = field(default_factory=time.time)
    messages_last_minute: int = 0

    # Debug用: 前ステート
    prev_state: Optional[StateName] = None

    # ============================================================
    # 負荷管理
    # ============================================================
    def _update_load(self) -> None:
        """
        直近60秒のメッセージ数をざっくりカウント。
        60秒以上空いた場合はカウンタをリセットする。
        """
        now = time.time()

        # 1分以上空いたら完全リセット
        if now - self.last_activity_ts > 60.0:
            self.messages_last_minute = 0

        self.messages_last_minute += 1
        self.last_activity_ts = now

    # ============================================================
    # Main step
    # ============================================================
    def step(
        self,
        *,
        user_requested_depth: Literal["shallow", "normal", "deep"],
        safety_flagged: bool,
        reflection_candidate: bool,
        introspection_candidate: bool,
        contradiction_flag: bool = False,
        intuition_allow: bool = False,
        identity_anchor: bool = False,
        abstraction_score: float = 0.0,
        loop_suspect_score: float = 0.0,
    ) -> StateName:
        """
        PersonaOS 完全版：状態遷移ロジック

        Parameters
        ----------
        user_requested_depth : {"shallow","normal","deep"}
            ユーザー／フロント側からの深度要求。
        safety_flagged : bool
            SafetyLayer による危険判定フラグ。
        reflection_candidate : bool
            deep 時に Reflection を走らせる候補か。
        introspection_candidate : bool
            deep 時に Introspection を走らせる候補か。
        contradiction_flag : bool, optional
            矛盾検出モジュールからのフラグ。
        intuition_allow : bool, optional
            疑似直観エンジンからの「直観で踏み込んでよい」フラグ。
        identity_anchor : bool, optional
            Identity Continuity エンジンからの「一度アンカー回復を優先したい」フラグ。
        abstraction_score : float, optional
            抽象度の高さ（将来の overthinking 判定に利用予定）。
        loop_suspect_score : float, optional
            自己ループ感の強さ（将来の沈黙トリガーに利用予定）。

        Returns
        -------
        StateName
            次の状態名。
        """

        now = time.time()
        self.prev_state = self.current
        self._update_load()

        # --------------------------------------------------------
        # 1. SafetyLayer 優先
        # --------------------------------------------------------
        if safety_flagged:
            self.current = "safety"
            return self.current

        # --------------------------------------------------------
        # 2. 過負荷制御
        # --------------------------------------------------------
        try:
            overload_limit = int(self.config.overload_limit_per_min)
        except Exception:
            overload_limit = 20  # フォールバック

        if overload_limit > 0 and self.messages_last_minute > overload_limit:
            # 一旦ブレーキ。ここでは応答自体は抑えめにする想定。
            self.current = "overload-prevent"
            return self.current

        # --------------------------------------------------------
        # 3. Identity Continuity の回復要求
        # --------------------------------------------------------
        if identity_anchor:
            # 直観や deep よりも「自己の連続性の回復」を優先
            self.current = "identity-anchor"
            return self.current

        # --------------------------------------------------------
        # 4. 直観（intuition）による自発的 deep
        # --------------------------------------------------------
        if intuition_allow:
            # 疑似直観はクールダウンをあまりかけず、
            # 「今だけ深く踏み込む価値がある」と判断された場合に使う。
            self.current = "intuition-deep"
            return self.current

        # --------------------------------------------------------
        # 5. 矛盾（contradiction）→ reflect 優先
        # --------------------------------------------------------
        try:
            reflect_cd = float(self.config.reflect_cooldown_sec)
        except Exception:
            reflect_cd = 30.0

        if contradiction_flag:
            if now - self.last_reflection_ts > reflect_cd:
                self.current = "reflect"
                self.last_reflection_ts = now
                return self.current

        # --------------------------------------------------------
        # 6. 深度要求 deep の場合
        # --------------------------------------------------------
        if user_requested_depth == "deep":
            # 6-1 Reflect（短期反省）
            if reflection_candidate and (now - self.last_reflection_ts > reflect_cd):
                self.current = "reflect"
                self.last_reflection_ts = now
                return self.current

            # 6-2 Introspect（中期内省）
            try:
                introspect_cd = float(self.config.introspect_cooldown_sec)
            except Exception:
                introspect_cd = 60.0

            if introspection_candidate and (
                now - self.last_introspection_ts > introspect_cd
            ):
                self.current = "introspect"
                self.last_introspection_ts = now
                return self.current

        # abstraction_score / loop_suspect_score は将来の拡張用に残しておく
        _ = abstraction_score
        _ = loop_suspect_score

        # --------------------------------------------------------
        # 7. shallow / normal → 通常ダイアログ
        # --------------------------------------------------------
        self.current = "dialogue"
        return self.current

    # ============================================================
    # Debug info
    # ============================================================
    def info(self) -> Dict[str, Any]:
        """
        現在の状態と負荷状況を返す（可視化・デバッグ用）。
        """
        return {
            "state": self.current,
            "previous": self.prev_state,
            "messages_last_minute": self.messages_last_minute,
            "last_reflection_ts": self.last_reflection_ts,
            "last_introspection_ts": self.last_introspection_ts,
        }

    # ============================================================
    # 追加: 軽量ロード指標（UI用）
    # ============================================================
    def load_hint(self) -> Dict[str, Any]:
        """
        フロント側で「今どれくらい負荷か」を表示するための簡易指標。
        """
        try:
            limit = float(self.config.overload_limit_per_min or 1)
        except Exception:
            limit = 1.0

        ratio = min(1.0, self.messages_last_minute / limit)

        return {
            "messages_last_minute": self.messages_last_minute,
            "overload_limit_per_min": self.config.overload_limit_per_min,
            "load_ratio": ratio,
        }

=== FILE: sigmaris-core/sigmaris_persona_core/types.py ===

# sigmaris_persona_core/types.py
from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Dict, Optional, Literal, Any
import time

# ============================================================
# 基本ロール
# ============================================================

Role = Literal["user", "assistant", "system", "system_user", "meta"]


@dataclass
class Message:
    """
    PersonaOS / AEI コア間でやり取りするメッセージの最小単位。
    """
    role: Role
    content: str
    timestamp: float = field(default_factory=lambda: time.time())
    tags: List[str] = field(default_factory=list)


# ============================================================
# トレイト / コンテキスト
# ============================================================

@dataclass
class TraitVector:
    """
    人格の基本3軸（0.0〜1.0）。
    """
    calm: float
    empathy: float
    curiosity: float


@dataclass
class PersonaContext:
    """
    PersonaOS に渡される外部メタ情報。
    """
    user_id: str
    session_id: str
    locale: str = "ja-JP"
    client: str = "sigmaris-os"
    extra: Dict[str, Any] = field(default_factory=dict)


# ============================================================
# Memory / Reward
# ============================================================

@dataclass
class MemoryEntry:
    """
    PersonaOS 内部で扱うメモリエントリ。
    """
    ts: float
    kind: Literal["short", "mid", "long"]
    content: str
    meta: Dict[str, Any] = field(default_factory=dict)


@dataclass(init=False)
class RewardSignal:
    """
    PersonaOS 完全版のメタ報酬信号。

    MetaRewardEngine / ValueDriftEngine / PersonaOS 全体で共通利用する形。

    基本仕様（新）:
      - value: float                      — 報酬スカラー（-1.0〜+1.0 を想定）
      - trait_reward: Optional[dict|TraitVector]
          軸ごとの報酬（使わなければ None）
      - reason: str                       — なぜこの報酬が発生したか（タグ）
      - meta: dict                        — AEI 側の生データや補足情報
          - とくに meta["trait_reward"] に dict を入れてもよい（ValueDrift 互換）
      - detail: dict                      — ヒューリスティック内訳など UI 可視化用

    互換性:
      - 旧コードの `RewardSignal(global_reward=...)` も受け付ける。
      - プロパティ `global_reward` は `value` のエイリアス。
    """

    # 実フィールド（dataclass 用）
    value: float
    trait_reward: Optional[Dict[str, float] | TraitVector] = None
    reason: str = ""
    meta: Dict[str, Any] = field(default_factory=dict)
    detail: Dict[str, Any] = field(default_factory=dict)

    def __init__(
        self,
        value: Optional[float] = None,
        *,
        global_reward: Optional[float] = None,
        trait_reward: Optional[Dict[str, float] | TraitVector] = None,
        reason: str = "",
        meta: Optional[Dict[str, Any]] = None,
        detail: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        新旧両方の呼び出しに対応するコンストラクタ。

        OK:
          RewardSignal(value=0.8, reason="meta_reward")
          RewardSignal(global_reward=0.8, reason="meta_reward")
        """
        # value / global_reward のマージ
        if value is None and global_reward is None:
            v = 0.0
        elif value is not None:
            v = float(value)
        else:
            v = float(global_reward)

        object.__setattr__(self, "value", v)
        object.__setattr__(self, "trait_reward", trait_reward)
        object.__setattr__(self, "reason", reason)
        object.__setattr__(self, "meta", meta or {})
        object.__setattr__(self, "detail", detail or {})

    # 互換用プロパティ（旧コードが global_reward を参照しても動く）
    @property
    def global_reward(self) -> float:
        return self.value

    @global_reward.setter
    def global_reward(self, v: float) -> None:
        object.__setattr__(self, "value", float(v))


# ============================================================
# Snapshot / Decision
# ============================================================

@dataclass
class PersonaStateSnapshot:
    """
    PersonaOS 内部状態のスナップショット。
    """
    state: str
    traits: TraitVector
    flags: Dict[str, bool]
    last_reward: Optional[RewardSignal] = None


@dataclass
class PersonaDecision:
    """
    PersonaOS が返す応答方針（UI / システム向け）。
    """
    allow_reply: bool
    preferred_state: str
    tone: str               # EmotionCore は str ラベルを返すため Literal ではない
    temperature: float
    top_p: float

    need_reflection: bool
    need_introspection: bool

    apply_contradiction_note: bool
    apply_identity_anchor: bool

    updated_traits: TraitVector
    reward: Optional[RewardSignal] = None

    debug: Dict[str, Any] = field(default_factory=dict)
